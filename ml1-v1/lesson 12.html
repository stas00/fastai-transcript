<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 12</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826">fast.ai: Intro to Machine Learning 1 (v1) (2018)</a></p>
  <h1>Lesson 12</h1>
  <h2>Outline</h2>
<ul>

<li>NLP : trigrams</li>
<li>Naive Bayes Classifier</li>
<li>Binarized version of NB</li>
<li>NBSVM - combination of probs</li>
<li>Storage efficiency of 1-hot</li>
<li>RossMan store examination</li>
<li>Introduction to embeddings</li>

</ul>


  <h2>Video Timelines and Transcript</h2>

<p><strong>Note: you may want to pay specific attention to the second part of this final lesson, where Jeremy brings up delicate issues on Data Science &amp; Ethics.</strong><br>
<strong>This goes beyond what most courses on DS cover.</strong></p>

<h3>1. <a href="https://youtu.be/5_xFdhfUnvQ?t=2s">00:00:01</a></h3>

<ul style="list-style-type: square;">

<li><b> Final lesson program !</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>I thought what we might do today is to like finish off where we were in 
this Russman noteblock, looking at timeseriesforecasting and structured 
data analysis, and then we might do a little mini review of like 
everything, we've learned because believe it or not. This is the end. Like 
there's nothing more to know about machine learning, rather than everything 
that you're going to learn next semester and for the rest of your life 
there anyway, I got nothing else to teach ya, so I'll, do a little review 
and and then we'll cover like the most Important part of the course which 
is like thinking about like what are the what what of how are ways to think 
about how to use this kind of technology appropriately, and you know 
effectively in a way that's a positive, hopefully a positive impact on 
society. So last time </p>

<h3>2. <a href="https://youtu.be/5_xFdhfUnvQ?t=1m2s">00:01:01</a></h3>

<ul style="list-style-type: square;">

<li><b> Review of Rossmann Kaggle competition with ‘lesson3-rossman.ipynb’</b></li>

<li><b>Using “df.apply(lambda x:…)” and “create_promo2since(x)”</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>We got to the point where we talked a bit about this site. This idea that 
when we were looking at like building this competition months, open derived 
variable, but we actually truncated it down to be no more than 24 months, 
and we talked about the reason. Why being that, we actually wanted to use 
it as a categorical variable, because categorical variables, thanks to 
embeddings, have more flexibility in how the neural net can can use them, 
and so that was kind of where we, where we left off. So, let's like keep 
working through this because what's happening in this notebook is stuff 
which is probably going to apply to most time series data sets that you 
work with right and, as we talked about like, although we used the F dot 
apply here. This is something where it's running a piece of Python code 
over every row and it's that's terrifically slow right, so we only do that 
if we can't find a vectorized, pandas or numpy function, that can do it 
too, the whole column at once, but in this case I couldn't find a way to 
convert a year and a week number into a date without using arbitrary 
Python. Also worth remembering. This idea of a lambda function, anytime, 
you're, trying to apply a function to every row of something or every 
element of a tensor, or something like that. If there isn't a vectorized 
version, already you're going to have to call something like data frame, 
dot apply which will run a function, you pass to every element, so this is 
something like you know.</p>

<p>Kind of this is basically a map in functional 
programming. Since very often, the function that you want to pass to, it is 
something you're just going to use once and then throw it away. It's really 
common to use this lambda approach, so this lambda is creating a function 
just for the purpose of telling DF not apply. What to use right so we 
could. We could also have written this in a different way, which would have 
been to say define create from o2 since on some value return, and then we 
put that in here, okay, so that - and that are the same thing. Okay, so one 
approach is to define the function and then pass it by name or the other is 
to define the function in place using lambda all right, and so, if you're 
not comfortable, creating and using lambdas. You know good thing to 
practice and playing around at the F dot, apply as a good way to good way 
to practice it. Okay, </p>

<h3>3. <a href="https://youtu.be/5_xFdhfUnvQ?t=4m30s">00:04:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Durations function “get_elapsed(fld, pre):” using “zip()”</b></li>

<li><b>Check the notebook for detailed explanations.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>So let's talk about this durations section, which may at first seem a 
little specific, but actually it turns out not to be. What we're going to 
do is we're going to look at three fields: promot state holiday and school 
holiday, and so basically, what we have is a table of for each store for 
each date. That's that store have a promo going on at that date. Is there a 
school holiday in that region of that store of that date? Is there a state 
holiday in that region for that store at that date? Okay, and so this kind 
of thing is, you know, like their events and time. Series with events are 
like very, very common, like if you're looking at oil and gas drilling 
data, you're trying to say like the flow through this pipe. You know here's 
an event representing when it set off some alarm. You know or here's an 
event where the drill got stuck or or whatever right, and so like most 
time, series at some level will tend to represent some events. So the fact 
that an event happened at a time is is interesting itself, but very often a 
time series will also show some something happening before and after the 
event. So, for example, in this case we're doing grocery sales prediction 
if there's a holiday coming up. It's quite likely that sales will be higher 
before and after the holiday and lower during the holiday.</p>

<p>If this is a 
City based store right, because you know you're gon na you've got to stock 
up before you go away to bring things with you, then, when you come back, 
you've got a refill. The fridge, for instance right. So it's alone like we, 
don't necessarily have to do this kind of feature engineering to create 
features specifically about like this is before or after a holiday that the 
neural net, you know the more. We can give the neuron that, like the kind 
of information it needs the less it's going to have to learn it, the less 
that it's going to have to learn it, the more we can do with the data, or 
we already have the more we can do With the you know, the size architecture 
we already have so future engineering, even even with stuff like neural 
nets, is still important because it means that you know we'll be able to 
do. You know, get better results with whatever limited data we have 
whatever limited computation. We have so the basic idea here, therefore, is 
when we have events in our time series is: we want to create two new 
columns for each event? How long is it going to be until the next time this 
event happens, and how long has it been since the last time that event 
happened so, in other words, how long until the next state holiday, how 
long since the previous state holiday? Okay, so that's not something which 
I'm aware of as existing as a library or anything like that, so we, I wrote 
up here by hand right and so importantly, I need to do this by store right. 
So I want to say like because you know for this store: when was this stores 
last promo? So how long has it been since the last time it had a promo how 
long it will be until the next time it has a promo, for instance, all 
right. So here's what I'm going to do! I'm gon na create a little function, 
that's going to take a field name and I'm going to pass it a chive, promo 
and then state holiday and then school holiday, all right! So let's do 
school holiday, for example.</p>

<p>So we'll say, field equals school holiday and 
then we'll say, get elapsed, school holiday, comma after so let me show you 
what that's going to do so. We've got a first of all sort by store and date 
right. So now, when we loop through this, we're going to be looping through 
within a store so store number one January, the first journey, the second 
January third and so forth, and as we look through each store, we're 
basically going to say like is: is this row a School holiday or not, and if 
it is a school holiday, then we'll keep track of this variable called last 
date, which says this is the last date and which, where we saw a school 
holiday, okay and so then we're basically going to append to our result. 
The number of days since the last school holiday - that's the kind of basic 
idea here. So there's a few interesting features. One is the use of zip 
right, so I could actually write this much more simply right, I could say: 
let's go through well, we could basically go through like four row in DF, 
get a rose right and then grab the the fields we want from each row. It 
turns out this is 300 times slower than the version that I have, and 
basically like iterating, through a data frame and extracting specific 
fields out of a row has a lot of overhead. What's much faster is to iterate 
through a numpy array. So if you take a series like the F store and add 
values after it, that grabs a numpy array of that series. Okay, so here are 
three numpy arrays one is the store.</p>

<p>Ids one is whatever field is, in this 
case: that's a school holiday and what is the date so now. What I want to 
want to do is look through the first one of each of those lists and then 
the second one of each of those lists and then the third one of each of 
those lists and like this is a really really common pattern. I need to do 
something like this in basically every notebook I write and the way to do 
it is with zip all right. So zip means look through each of these lists one 
at a time, and then this here is where we can grab that element out of the 
first list, the second list and the third list. Okay, so if you haven't 
played around watch with zip, that's a really important function to 
practice with, like I say, I use it in pretty much every notebook. I write 
all the time you have to look through. You know a bunch of lists at the 
same time. Alright, so we're going to look through every store every school 
holiday at every date. Yes, so is it living through, like all the possible 
combinations of each of those or for one one, one yeah exactly thanks for 
the question, so in this case we basically want to say: let's grab the 
first store, the first school holiday, the first date right so Fast or one 
January, the first school holiday was true or false right, and so, if the, 
if it is a school holiday, I'll keep track of that fact.</p>

<p>By saying the last 
time I saw a school holiday was that day, okay and then append? How long 
has it been since the last school holiday right and if the store ID is 
different to the last door ID I saw, then I've now got to a whole new 
store, in which case I have to basically reset everything. Okay, you pass 
that to her. What will happen to the first points that we don't have a life 
last holiday yeah? So I just said I basically set this to some arbitrary 
starting point. It's going to end up with, like I can't remember, is either 
largest or the smallest possible date, and you know you may need to replace 
this with a missing value afterwards or some you know the zero or or 
whatever you know. The nice thing is, though, thanks to rallies, it's very 
easy for a neural net to kind of cut off extreme values. So in this case I 
didn't do anything special with it. I ended up with these like negative, a 
billion day time stamps, and it still worked. Fine, okay, so we can go 
through, and so the next thing to note is there's a whole bunch of stuff 
that I need to do to both the training set and the test set right. So in 
the previous section I actually kind of added this little loop, where I go 
for each of the training data frame and the test data frame through these 
things right. So I kind of you know each cell I did for each of the data 
frames. I've now got a whole coming up a whole series of cells that I want 
to run, first of all for the training set and then for the test set.</p>

<p>So in 
this case, the way I did that was, I have two different cells here: one 
which set DF to be the training set, one which set to be the test set, and 
so the way I use this is, I run just this cell right and then I run all the 
cells underneath right, so it does it alter the training set and then I 
come back and run just this cell and then run all the cells underneath. 
Okay, so like this, notebook is not designed to be just run from top to 
bottom, but it's designed to be run in this particular way, and I mentioned 
that because like this can be a handy trick. No, like you could, of course, 
put all the stuff underneath in a function that you pass the data frame to 
and call it once with a test set once with the training set bill, I kind of 
like to experiment a bit more interactively. Look at each step as I go, so 
this way is an easy way to kind of run something on two different data 
frames without turning it into a function. Okay, so this is going to. If, 
if I sort by store - and by date, then this is keeping track of the last 
time, something happened, and so this is therefore going to end up telling 
me how many days was it since the last school holiday? Okay. So now, if I 
sort date descending and call the exact same function, then it's going to 
say how long until the next school holiday, okay, so that's a kind of a 
nice little trick for adding these kind of event time as arbitrary event. 
Timers into your time, series models right.</p>

<p>So if you're doing, for 
example, the Ecuadorean groceries competition right now, you know, maybe 
this kind of approach would be useful for various events in that as well. 
Do it for state holiday, do it for promo there we go! Okay, the next thing 
</p>

<h3>4. <a href="https://youtu.be/5_xFdhfUnvQ?t=16m10s">00:16:10</a></h3>

<ul style="list-style-type: square;">

<li><b> Rolling function (or windowing function) for moving-average</b></li>

<li><b>Hint: learn the Pandas API for Time-Series, it’s extremely diverse and powerful</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>That we look at here is rolling functions, so rolling functions is how we 
rolling in pandas is how we, what we call create, what we call windowing 
functions. So let's say I had some data. You know something like this right 
and this is like date, and I don't know this is like sales or whatever. 
What I could do is, I could say, like: okay, let's create a window around 
this point of like seven days, all right, so it'd be like okay. This is a 
seven day window, say right, and so then I could take the average sales in 
that seven-day window. All right, then, I could like do the same thing like 
I don't know over here. Take the average sales over that seven-day window 
all right, and so, if we do that for every point and join up those averages 
you're going to end up with a moving average okay. So the kind of the the 
the more generic version of the moving average is a window function, ie, 
something where you apply to some function to some window of data around 
each point. Now very often that windows that I've shown here are not 
actually what you want. If you're trying to build a predictive model, you 
can't include the future as part of a moving average all right. So quite 
often you actually need a window. That's ends here, so that would be our 
window function right and so pandas lets you create window. Func arbitrary 
window functions using this rolling here. This here says how many time 
steps do I want to apply the function to write.</p>

<p>This here says if I'm at 
the edge, so in other words, if I'm like out here, should you have? Should 
you make that a missing value, because I don't have seven days to average 
over or you know, what's the minimum number of time periods to use? That's 
so here I said one okay and then then optionally, you can also say: do you 
want to say the window at the start of a period or the end of a period or 
the middle of the period? Okay, so, and then within that you can then apply 
whatever function. You like, okay, so here I've got, my weekly buy, store, 
sums; okay, so there's a nice easy way of getting kind of moving averages 
or or whatever else - and you know I should mention in pandas. If you go to 
the time series page on pandas, there's literally like look at just the 
index here time, series functionality is all of this. Is this like there's 
lots because, like where's bikini, who created this, he was originally in 
hedge fund trading? I believe - and you know his work was all about time 
series and so I think, like pandas originally was very focused on time 
series and still you know it's perhaps the strongest part of pandas. So, if 
you're playing like if you're playing around with time-series computations, 
you definitely owe it to yourself to try to learn this entire API and, like 
it, there's a lot of kind of conceptual pieces around like time, stamps and 
date, offsets and resampling and stuff like that.</p>

<p>To kind of get your head 
around, but it's totally worth it because otherwise you'll be writing this 
stuff as loops by hand it's going to take you a lot longer than leveraging 
what pandas already does and of course pandas will do it in you know highly 
optimized C code for you vectorize the C code, whereas your version is 
going to loop in Python. So definitely worth you know. If you're doing 
stuff in x here is learning the the full pandas time series API there's 
about is about as strong as any time series API out there. Okay, so at the 
end of all that, you can see here's those kind of starting point values. I 
mentioned slightly on the extreme side, and so you can see here the 17th of 
September store. One was thirteen days after the last school holiday. The 
16th was 12 11 10, so forth, okay and we're currently in a promotion right 
here. This is one day before a promotion. Here, we've got nine days after 
the last promotion and so forth. Okay, so that's how we can add kind of 
event counters to our time series and probably always a </p>

<h3>5. <a href="https://youtu.be/5_xFdhfUnvQ?t=21m40s">00:21:40</a></h3>

<ul style="list-style-type: square;">

<li><b> Create Features, assign to ‘cat_vars’ and ‘contin_vars’</b></li>

<li><b>‘joined_samp’, ‘do_scale=True’, ‘mapper’,</b></li>

<li><b>‘yl = np.log(y)’ for RMSPE (Root Mean Squared Percent Error)</b></li>

<li><b>Selecting a most recent Validation set in Time-Series, if possible of the exact same length as Test set.</b></li>

<li><b>Then dropping the Validation set with ‘val_idx = [0]’ for final training of the model.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Good idea, when you're doing work with time series so now that we've done 
that you know, we've got lots of columns in our data set, and so we split 
them out into categorical versus continuous columns. We'll talk more about 
that in a moment in the review section, but so these are going to be all 
the things I'm going to create an embedding for okay, and these are all of 
the things that I'm going to feed directly into the into the model. So, for 
example, we've got like competition distance, so that's distance to the 
nearest competitor, maximum temperature, and here we've got day of week 
right so so here we've got maximum temperature. Maybe is like twenty two 
point: one because they use centigrade in Germany. We've got distance. Two 
nearest competitor might be 321 kilometres 0.7 all right and then we've got 
day of week, which might be I don't know. Maybe Saturday is a six okay, so 
these numbers here are going to go straight into , now vector right, the 
vector that we're going to be feeding into our neural net right 22, 1-3, 
21.7. Okay well see in a moment, we'll actually will normalize them, but 
more or less, but this categorical variable we're not. We need to put it 
through an embedding right, so we'll have some embedding matrix right of if 
there are seven days by, I don't know, maybe dimension for embedding okay, 
and so this will look up the sixth row to get back the four items right, 
and so This is going to turn into length four vector, which will then add 
here. Okay, so that's how our continuous and categorical variables they're 
going to work.</p>

<p>So then, all of our categorical variables will turn them 
into pandas categorical variables in the same way that we've done before 
and then we're going to apply the same mappings to the test set right. So 
if Saturday is a six in the training set, this apply. Cats makes sure that 
Saturday is also a six and the test set for the continuous variables make 
sure they're all floats, because pay torch expects everything to be a 
float. So then, this is another little trick that I use both of these cells 
to find something called joined, sent one of them defines them as the whole 
training set. One of them defines them as a random subset all right, and so 
the idea is that I do all of my work on the sample make sure it all works 
well play around with different hyper parameters and architectures, and 
then, unlike okay, I'm very happy with this. I then go back and run this 
line of code to say: okay now make that make the whole dataset be the 
sample and then rerun it okay. So this is a good way again similar to that. 
What I showed you before it lets. You use the same cells in your notebook 
to run first of all on the sample and then go back later and run it on the 
full data set. Okay. So now that we've got that join samp, we can then pass 
it to proc DF, as we've done before, to grab the dependent variable to deal 
with missing values, and in this case we pass one more thing, which is du 
scale, equals true de scale equals true.</p>

<p>We'll subtract the mean and divide 
by the standard deviation, and so the reason for that is that if our first 
layer, you know it's just a matrix model play right. So here's our set of 
weights and our input is like I don't know, it's got something with just 
like 0.001 and then it's got something like which is like 10 to the 6 right 
and then our weight matrix has been initialized to be like random numbers 
between 0 & amp 1 right so got like 0.6 0.1, etc. Then basically, this 
thing here is going to have gradients that are nine orders of magnitude 
bigger than this thing here, which is not going to be good for 
optimization, okay, so by normalizing everything to be mean of 0 standard 
deviation of 1 to start with, then that Means that all of the gradients are 
going to be. You know on the same kind of scale. We didn't have to do that 
in random forests, because in random forests we only cared about the sort 
order. We didn't care about the values at all right, but in that with 
linear models and things that are built out of layers of lynnium, like ie 
neural nets, we care very much about the scale. Okay, so do scale equals 
true normalizes our data for us now, since it normalizes our data for us, 
it returns one extra object, which is a mapper, which is an object that 
contains for each continuous variable.</p>

<p>What was the mean and standard 
deviation? It was normalized with the reason being that we're going to have 
to use the same know mean and standard deviation on the test set correct, 
because we need our test set in our training set to be scaled in the exact 
same way. Otherwise, they're going to have different meanings: okay and so 
these details about making sure that your tests and training set have the 
same categorical, codings, the same missing value replacement and the same 
scaling. Normalization are really important to get right, because if you 
don't get it right, then your test set is: you know not gon na work at all. 
Okay. But if you follow these steps, you know it'll work, fine. We also 
take the log of the dependent variable and that's because, in this capital, 
competition, the evaluation metric was root, mean squared percent error, so 
root mean squared percent. Error means we're being penalized based on the 
ratio between our answer and the correct answer. We don't have a loss 
function in pie, chart called root, mean square percent error. We could 
write 1 but easier is just to take the log of the dependent, because the 
difference between logs is the same as the ratio. Okay. So, by taking the 
log, we kind of get that for free, you'll notice, like the vast majority of 
regression. Competitions on Kaggle use, either root mean squared percent 
error or a root mean squared error of the log as their evaluation metric. 
And that's because in real world problems most of the time we care more 
about ratios than about raw differences right.</p>

<p>So if you're designing your 
own project, it's quite likely that you'll want to think about using log of 
your dependent variable. So then we create a validation set and, as we've 
learned before most of the time, if you've got a problem involving a time 
component, your validation set probably wants to be the most recent time 
period rather than a random subset. Okay. So that's what I do here when I 
finished modeling and I found an architecture and a set of hyper parameters 
and a number of epochs and all that stuff that works really. Well. If I 
want to make my model as good as possible, I'll retrain on the whole thing 
right, including the validation set right now currently at least past AI 
assumes that you do have a validation set. So my kind of hacky workaround 
is to set my validation set to just be one index, which is the first row. 
Okay, and that way, like all the code keeps working, but there's no real 
validation set. So obviously, if you do this, you need to make sure that 
your final training is like the exact same hyperparameters, the exact same 
number of epochs, exactly the same as the thing that worked, because you 
don't actually have a proper validation set now to check against. I have a 
question regarding get elapsed, function which we discussed before so in 
get elapsed, function we are trying to find.</p>

<p>When is the next holiday like? 
When will the next one ready come? How many, how many days away so every 
year the holidays are more or less fixed like there will be holiday on 4th 
of July 21st assemble and there is hardly any change. So can't we just look 
from previous years and just get a list of all the holidays that are going 
to occur this year. Maybe I mean in this case I guess like that's, not sure 
of promo right and some holidays change like Easter. You know so like this 
this this way I get to write one piece of code that works for all of them. 
You know - and it doesn't take very long to run so yeah, so there might be 
ways if you're, if your data set was so big that this took too long. You 
could maybe do it on one year and then kind of somehow copy it, but yeah in 
this case, so those no need to - and I don't you know - I always value my 
time over my computer's time. So I try to keep things as simple as I can. 
Okay, so now we can create our model and </p>

<h3>6. <a href="https://youtu.be/5_xFdhfUnvQ?t=32m30s">00:32:30</a></h3>

<ul style="list-style-type: square;">

<li><b> How to create our Deep Learning algorithm (or model), using ‘ColumnarModelData.from_data_frame()’</b></li>

<li><b>Use the cardinality of each variable to decide how large to make its embeddings.</b></li>

<li><b>Jeremy’s Golden Rule on difference between modern ML and old ML:</b></li>

<li><b>“In old ML, we controlled complexity by reducing the number of parameters.</b></li>

<li><b>In modern ML, we control it by regularization. We are not much concerned about Overfitting because we use increasing Dropout or Weight-Decay to avoid it”</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>So to create our model, we have to create a model data object, as we always 
do with fastai. So a columnar model data object is just a data, a model 
data object that represents a training set, a validation set and an 
optional test set of standard columnar. You know structured data, okay, and 
we just have to tell it which of the variables should we treat as 
categorical, okay and then pass in our data friends. So for each of our 
categorical variables here is the number of categories it has. Okay, so for 
each of our embedding matrices, this tells us the number of rows and that 
embedding matrix, and so then we define what embedding dimensionality we 
want. If you're doing like natural language processing, then the number of 
dimensions you need to capture all the nuance of what a word means and how 
it's used has been found empirically to be about 600. It turns out that 
when you do NLP models with embedding matrices that are that are smaller 
than 600, you don't get as good a results as you do. If you do, if there's 
the size 600 beyond 600, it doesn't seem to improve much. I would say that 
human language is one of the most complex things that we model, so I 
wouldn't expect you to come across many if any categorical variables that 
need embedding matrices with more than 600 dimensions. At the other end, 
you know some things may have pretty simple kind of causality right.</p>

<p>So, 
for example, let's have a look state holiday, you know maybe, if 
something's a holiday, then it's just a case of like ok at stores that are 
in the city. There's some behavior there's doors that are in the country, 
there's some other behavior and that's about it. You know like maybe it's a 
pretty pretty simple relationship so like. Ideally, when you decide what 
imbedding size to use, you would kind of use your knowledge about the 
domain to decide like how complex is the relationship, and so how big 
embedding do I need right in practice. You almost never know that right, 
like you, only know that, because maybe somebody else has previously done 
that research and figured it out like in NLP. So in practice you probably 
need to use some rule of thumb, okay and then having tried to a rule of 
thumb, you could then maybe try a little bit higher and a little bit lower 
and see what helps so, it's kind of experimental. So here's my rule of 
thumb, my rule of thumb, is look at how how many discrete values the 
category has ie the number of rows in the embedding matrix and make the 
dimensionality of the embedding half of that. Alright. So if a day of week, 
which is the second one, eight rows and four colors so here it is there 
right the number of categories divided by two. But then I say: don't go 
more than 50 right, so here you can see for store.</p>

<p>There's a thousand 
stores only have a dimensionality of 50. Why 50? I don't know it seems to 
work okay, so far like you may find you need something a little different 
actually for the ecuadorian groceries competition. You know I haven't 
really tried playing with this, but I think we may need some larger, 
embedding sizes, but it's something to feel a bit princess. Can you pass 
that left social variables, the cardinality size becomes larger and larger 
you're creating more and more like. I think we collide in very much seas on 
you, therefore, massively risking overfitting, which is just using so many 
parameters of the model, can never possibly capture all that variation. 
Less. Your data is actually huge, that's a great question, and so let me 
remind you about my kind of like golden rule, with the difference between 
modern machine learning and old machine learning, an old machine learning. 
We control complexity by reducing the number of parameters in modern 
machine learning. We control complexity by regularization. So short answer 
is not I'm not concerned about overfitting, because the way I avoid 
overfitting is not by reducing the number of parameters, but by increasing 
my dropout or increasing my weight. Okay, okay. Now, having said that, like 
there's no point using more parameters for a particular embedding than I 
need like because regularization like is penalizing a model by giving it 
like more random data or by actually penalizing weights, so we like we'd 
rather not use more than we have To but they're kind of, my general rule of 
thumb for designing an architecture is, to you know, be generous on the 
side of the number of parameters but yeah in this case.</p>

<p>If, after doing 
some work, we kind of felt like you know what the store doesn't. Actually 
seem to be that important, then I might manually go and make change this to 
make it smaller, but you know or if I was really finding there's not enough 
data here, I'm either overfitting or I'm using more regularization 
uncomfortable with again. You know, then you might go back, but I would 
always start with like being generous with parameters and yeah. In this 
case, this model turned out pretty good okay. So now we've got a list of 
tuples containing the number of rows and columns of each bar. Embedding 
matrices, and so when we call get learner to create our neural net. That's 
the first thing we pass in right is how big is each of our embeddings okay, 
and then we tell it how many continuous variables we have. We tell it how 
many activations to create for each layer and we tell it what dropout to 
use page layer, okay and so then we can go ahead and call fit okay. So then 
we fit for awhile and we're </p>

<h3>7. <a href="https://youtu.be/5_xFdhfUnvQ?t=39m20s">00:39:20</a></h3>

<ul style="list-style-type: square;">

<li><b> Checking our submission vs Kaggle Public Leaderboard (not great), then Private Leaderboard (great!).</b></li>

<li><b>Why Kaggle Public LB (LeaderBoard) is NOT a good replacement to your own Validation set.</b></li>

<li><b>What is the relation between Kaggle Public LB and Private LB ?</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Kind of getting something around the the point, one mark all right, so I 
tried running this on the test set and I submitted it to kaggle during the 
week actually last week, and here it is okay. Private score. 107. Public 
score. 103. Okay, so let's have a look and see how that would go. So when I 
was seven private, 103 public's. Let's start on public, which is 103 not 
there out of 3000. I've got to go back a long way here. It is 103. Okay, 
three hundred and fortieth yeah - that's not good! So on the public 
leaderboard three hundred and fortieth: let's try the private leader board, 
which is 107, oh fifth, so, like hopefully, you are now thinking. Oh, there 
are some chemical competitions finishing soon which I entered, and I spent 
a lot of time trying to get good results on the public leaderboard. I 
wonder if that was a good idea and the answer is no. I want write the cavil 
public leaderboard is not meant to be a replacement for your carefully 
developed validation set okay. So, for example, if you're doing the iceburg 
competition right, which ones are ships which ones icebergs, then they've 
actually put something like 4,000 synthetic images into the public, 
leaderboard and money into the private leaderboard okay. So this is one of 
the really good kind of things that tests you out on Kegel is like.</p>

<p>Are you 
creating a good validation set, and are you trusting it right? Because if 
you're trusting your leaderboard feedback more than your validation 
feedback, then you may find yourself in three hundred and fiftieth place 
when you thought you're in fifth right. So in this case, we actually had a 
pretty good validation set right because, as you can see, it's saying like 
somewhere around 0.1 and we actually did get somewhere around 0.1 okay, and 
so in this case the validation set. That's re that publicly the board in 
this competition was entirely useless. Yeah. Can you place the box please? 
So in regards to that, how much does the top of the public leaderboard 
actually correspond to the top of a privately reward? Because in the in the 
churn prediction challenge there's like for people who are just completely 
above everyone else, that's it, it totally depends. You know like if they 
randomly sample the public and private leaderboard, then it should be 
extremely indicative right, but it might not be right. So, in this case, 
okay - let's crushed - oh here - comes so in this case the person who was 
second on the public leader. What did end up winning s? Dnt came seventh 
right, so in fact you can see the little green thing here right. Where else 
this guy jumped 96 places, if we had entered with the neural net we just 
looked at, we would have jumped 250 places, so it yeah, it just depends, 
and so often like you can figure out where the the public leaderboard, like 
sometimes they'll, tell you The public leaderboard was randomly sampled, 
sometimes they'll tell you it's not generally.</p>

<p>You have to figure it out by 
looking at the correlation between your validation set results and the 
public leaderboard results to see how well they're correlated. Sometimes, 
if, like 2 or 3, people are way ahead of everybody else. They may have 
found some kind of leakage or something like that, like that's, often a 
sign that there's some trick. Okay, so that's Russman, and that brings us 
to the end of all of our material. So let's come back after the break and 
do a quick review and then we will talk about ethics and machine level. So 
let's come </p>

<h3>8. <a href="https://youtu.be/5_xFdhfUnvQ?t=44m15s">00:44:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Course review (lessons 1 to 12)</b></li>

<li><b>Two ways to train a model: one by building a tree, one with SGD (Stochastic Gradient Descent)</b></li>

<li><b>Reminder: Tree-building can be combined with Bagging (Random Forests) or Boosting (GBM)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Back in five minutes, so we've learnt two ways to train a model. One is by 
building a tree and one is with SGD, okay, and so the SGD approach is a way 
we can train a model which is a linear model or a stack of linear layers. 
With nonlinearities between them, whereas tree building specifically, will 
give us a tree okay and then tree building, we can combine with bagging to 
create a random forest or with boosting to create a GBM or various other 
slight variations, such as extremely randomize trees. So it's worth like 
reminding ourselves of like what these things do. So, let's, let's look at 
some data, so if we've got some data like so actually, let's look 
specifically, let's look specifically a categorical data right, okay, so 
categorical data. There's a couple of possibilities of what categorical 
data might look like it could be like. Okay. So let's say we got zip code 
like so: we've got line for double O three: here's our zip code right and 
then we've got like sales right and it's like 50 and like nine four one, 
three one sales or twenty, two and so forth. All right! So we've got some 
categorical variable, so there's a couple: </p>

<h3>9. <a href="https://youtu.be/5_xFdhfUnvQ?t=46m15s">00:46:15</a></h3>

<ul style="list-style-type: square;">

<li><b> How to represent Categorical variables with Decision Trees</b></li>

<li><b>One-hot encoding a vector and its relation with embedding</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Of ways we could represent that categorical variable one would be just to 
use the number right and like maybe it wasn't a number to start. You know, 
maybe it wasn't a number at all. Maybe a categorical variable is like San 
Francisco New York, Mumbai and Sydney right, but we can turn it into a 
number just by like arbitrarily deciding to give them numbers right so like 
it ends up in your number. So we could just use that kind of arbitrary 
number. So if, if it turns out that zip codes that are numerically next to 
each other have somewhat similar behavior, then the zip code versus sales 
chart might look something like this, for example right or alternatively, 
if the zip code versus sales. Like sorry, if the two zip codes next to each 
other didn't have any ways similar similar sales behavior, you would expect 
to see something that looked more like this, like kind of just all over the 
place right, okay, so there are kind of two possibilities. So what a random 
forest would do if we had just encoded zip in this way, is it's gon na say 
alright. I need to find my single best split point. Okay, the split point: 
that's going to make the two sides have as smaller standard deviation as 
possible or mathematically equivalently have the lowest root mean squared 
error. So, in this case, it might pick here as our first bit point because 
on this side, there's one average and on the other side there's the other 
average okay and then, for its second split point, it's going to say: okay, 
how do I split this and it's Probably going to say, I would split here 
right because now we've got this average versus this average right and then 
finally, it's going to say: okay, how do we split here and it's going to 
say, okay I'll spit there right so now, I've got that average and Average, 
okay, so you can see that it's able to kind of hone in on the set of spits 
it needs, even though it kind of doesn't greatly top down one at a time 
right. The only reason it wouldn't be able to do this as if like it was 
just such bad luck, that the two halves were kind of always exactly 
balanced right.</p>

<p>But even if that happens, it's not going to be the end of 
the world it'll spit on something else, some other variable and next time 
around. You know it's very unlikely that it's still going to be exactly 
balanced in both parts of the tree, all right. So in practice this works 
just fine. In the second case right, it can do exactly the same thing 
right, it'll say like okay, which is my best first split right, even as 
though there's no relationship between one zip code and its neighboring zip 
code. Numerically, we can still see here if it if it's bits here right, 
there's the average on one side and the average on the other side is 
probably about here. Okay and then, where would it split next, probably 
here all right because here's the average on one side? Here's the average 
on the other side, all right so again can do the same thing right. It's 
going to need more splits because it's going to end up having to kind of 
narrow down on each individual large that code in each individual, small, 
zip code. But it's still going to be fine, okay, so when we're dealing with 
building decision, trees for random forests or gbm's or whatever, we tend 
to encode our variables, just as ordinals okay, on the other hand, if we 
are doing a a neural network or like a simplest Version like a linear 
regression or logistic regression, the best it could do is that right, 
which is no good at all and did over this one.</p>

<p>It's going to mean like 
that. Okay, so an ordinal is not going to be a useful encoding for a linear 
model or something that stacks linear and nonlinear models together. So 
instead what we do is we create a one, hot encoding right so we'll say, 
like you know, he is zero. One! Zero! Zero zero here: zero 100 - he is Oh 
100, one okay, and so with that encoding that can effectively create like a 
little histogram right, where it's going to have a different coefficient 
for each level. Okay, and so that way it can do exactly what it needs to 
do. Can you pass that back? Please, at what point does that become like too 
tedious for your system, or does it not pretty much, never yeah, because 
remember in real life, we don't actually. Actually, we don't actually have 
to create that matrix. Instead, we can, just you know, have the four 
coefficients right and just do an index lookup to grab the second one, 
which is mathematically equivalent to x on the one pot encoding. Okay. So 
so that's no problem. One thing to mention: you know: I know you guys have 
kind of been taught quite a bit of more like analytical solutions to things 
and in analytical solutions to like a linear regression, you get, you can't 
solve something with this amount of collinearity, in other words, Sydnee. 
Something you know something is certainly if it's not Mumbai or New York or 
San Francisco, so in other words, there's a hundred percent collinearity 
between the forth of these classes versus the other three.</p>

<p>And so, if you 
try to solve a linear aggression, analytically, that way the whole thing 
falls. Apart now note with SGD, we have no such problem. Okay, like SGD, 
why would it care right we're just taking one step along the derivative 
here cares a little right because, like in the end, the main problem with 
collinearity is that there's an infinite number of equally good solution 
right, so, in other words, we could increase All of these and decrease this 
or decrease all of these and increase this and they're going to balance out 
right and when there's an infinitely large number of good solutions. It 
means there's a lot of kind of flat spots in the Loess surface and it can 
be harder to optimize all right. So that's a really easy way to get rid of 
all of those flat spots, which is to add a little bit of regularization. So 
if we added a little bit of a little bit of weight decay like one enix, 
seven, even then that basically says these are not all equally good 
anymore. The one which is the best is the one where the parameters are the 
smallest and the most similar to each other and so that'll again move it 
back to being a nice loss function. Yes, could you just clarify that point 
you made about why? What hard coding wouldn't be that sure if we have a one 
hot encoded vector right and we are multiplying it by a set of coefficients 
right, then that's exactly the same thing as simply saying: let's grab the 
thing where the one is right, so in other words, If we had stored this as a 
zero, you know - and this one has a one - and this one is a two right.</p>

<p>Then 
it's exactly the same as just saying: hey look up that thing in the array, 
okay, and so we call that version and imbedding right. So an embedding is a 
model. Clica is a weight matrix. You can multiply by a one-pot encoding and 
it's just a computational shortcut, okay, but it's mathematically the same 
so there's a key difference. So the first you know key difference between 
like solving linear type models. Analytically versus with SGD with SGD, we 
don't have to worry about the arity and stuff, or at least not nearly to 
the same degree, and then the difference between solving a linear or single 
layer or multi-layer model with SGD versus a trinny. A tree is going to be 
like it's going to complain about less things right, so in particular, you 
can just use ordinals as your categorical variables and as we learnt just 
before. We also don't have to worry about normalizing continuous variables 
for a tree, but we do have to worry about it for these SGD change models. 
So then we </p>

<h3>10. <a href="https://youtu.be/5_xFdhfUnvQ?t=55m50s">00:55:50</a></h3>

<ul style="list-style-type: square;">

<li><b> Interpreting Decision Trees, Random Forests in particular, with Feature Importance.</b></li>

<li><b>Use the same techniques to interpret Neural Networks, shuffling Features.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Also learnt a lot about interpreting random forests in particular, and if 
you're interested, you may be interested in trying to use those same 
techniques to interpret neural nets right. So if you want to know which of 
my features are important in a neural net, you could try the same thing. 
Try shuffling each column in turn and see how much it changes your 
accuracy, okay and that's going to be your feature importance for your 
neural net and then, if you really want to have fun recognize, then that 
shuffling that column is just a way of calculating how Sensitive the output 
is to that input which, in other words, is the derivative of the output 
with respect to that input, and so, therefore, maybe you could just ask 
pipe torch to give you the derivatives with respect to the input directly 
and see. If that gives you the same kind of answers, okay, you could do the 
same kind of thing for partial dependence plot you could try. You know 
doing the exact same thing with your neural net, replace everything in the 
column with the same value. Do it for 1960. 1961, 1962 plot that that I 
don't know if anybody who's done these things before, not because it's 
rocket science, but just because I don't know, maybe no one thought of it 
or it's not in our library. I don't know, but if somebody tried it, I think 
you should find it useful. It would make a great blog post, maybe even the 
paper if you wanted to take it a bit further.</p>

<p>So there's a thought that 
something you can do so those most of those interpretation techniques are 
not particularly specific to random forests. Things like the tree 
interpreter certainly are because they're all about like what's inside the 
tree, can you pass? It occur we're applying for interpreter for neuron. 
That's how are we going to make inference out of activations that the path 
follows for example? So how are we going to like in three interpreter we 
are like when looking at we are looking at the parts and their 
contributions of the features. In this case, it will be same with 
activations. I guess the contributions of each activation on their path, 
yeah, baby. No, I haven't thought about it. How can we like make in front 
of the activations? So I'd be careful to say the word inference, because no 
people normally is the word inference specifically domain. The same is like 
a test a test time. Okay prediction you make like make some kind of 
interrogate the model. Yes yeah, I'm not sure we should think about that. 
Actually, Hinton and one of his students just published a paper on how to 
approximate a neural net with a tree for this exact reason, which I haven't 
read the paper. Yet could you pass that so in linear regression and 
traditional statistics, like one of the things that we focused on, was 
statistical significance of like the changes and things like that, and so 
when thinking about a tree interpreter or even like the waterfall chart, 
which I guess Is just a visualization um? I guess where does that fit in 
like </p>

<h3>11. <a href="https://youtu.be/5_xFdhfUnvQ?t=59m">00:59:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Why Jeremy usually doesn’t care about ‘Statistical Significant’ in ML, due to Data volume, but more about ‘Practical Significance’.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Because we can see like oh now, this looks important in the sense that it 
causes large changes. But how do we know that it's like traditionally 
statistically significant or anything about yeah? So, most of the time, I 
don't care about the traditional statistical significance and the reason 
why is that, nowadays, the main driver of statistical significance is data 
volume, not kind of practical importance and nowadays most of the models 
you build will have so much data that, like Every tiny thing will be 
statistically significant, but most of them won't be practically 
significant, so my main focus therefore, is practical significance, which 
is: does the size of this influence impact? Your business? You know 
statistical significance. Only you know like it was much more important 
when we had a lot less data to work with. If you do need to know 
statistical significance, because, for example, you have a very small data 
set because it's like really expensive to label or hard to collect or 
whatever or it's a medical data set for a rare disease. You can always get 
statistical significance by bootstrapping, which is to say that you can 
randomly resample. Your data set a number of times train your model a 
number of times, and you can then see the actual variation in predictions. 
Ok, so that's that's with bootstrapping. You can turn any model into 
something that gives you confidence intervals.</p>

<p>There's a paper by Michael 
Jordan, which has a technique called the bag of little bootstraps, which 
actually kind of takes, takes this a little bit further and well worth 
reading if you're interested, actually positive prints. So you say we don't 
need one hold encoding matrix in if you're doing random forests or if you 
are doing any cleavage models. What will happen if we do that and how bad 
can a model be? If you trigger a one horn, coding yeah, we actually did do 
it. Remember we had that like maximum category size and we did create 
one-hot encodings and the reason why we did it was that then our feature 
importance would tell us the importance of the individual levels and our 
partial dependence plot. We could include the individual levels, so it 
doesn't necessarily make the model worse. It may make it better, but it 
probably won't change it much at all. In this case, it hardly changed it. 
This is something that we have noticed on real data, also that if 
cardinality is higher, let's say 50 levels and if you do one hot encoding, 
the random forest performs very badly yeah yeah, that's right! If the cab 
now that's why we have that in that's. Why, in fast I we have that like 
maximum categorical size, you know because at some point you're one hot 
encoded variables become too sparse right.</p>

<p>So I generally like cut it off 
at six or seven, also because, like when you get past, that it kind of 
becomes less useful, because the feature importance, there's going to be 
too many levels to really look at. So can it not just not look at those 
levels which are not important and just gives those significant features as 
important yeah yeah I mean it's, it's it's it'll be okay. You know it's 
just like once. The cardinality increases to high you're just you're, just 
splitting your data up, you know too much basically, and so in practice, 
your ordinal version is likely to be it's likely to be better okay, so 
yeah, so little, there's no time to you're kind of review everything. But I 
think that's the kind of key concepts and then, of course, remembering that 
you know the embedding matrix that we can use is likely to have more than 
just one coefficient will actually have a dimensionality of a few 
coefficients which isn't going to be useful. For most linear models, but 
once you've got multi-layer models, that's now creating a representation of 
your category, which is kind of quite a lot richer, and you can 
</p>

<h3>12. <a href="https://youtu.be/5_xFdhfUnvQ?t=1h3m10s">01:03:10</a></h3>

<ul style="list-style-type: square;">

<li><b> Jeremy talks about “The most important part in this course: Ethics and Data Science, it matters.”</b></li>

<li><b>How does Machine Learning influence people’s behavior, and the responsibility that comes with it ?</b></li>

<li><b>As a ML practicioner, you should care about the ethics and think about them BEFORE you are involved in one situation.</b></li>

<li><b>BTW, you can end up in jail/prison as a techie doing “his job”.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Do a lot more with it. Let's now talk about the most important bit, we 
started off early in this course talking about how actually a lot of 
machine learning is kind of misplaced people. Focus on predictive accuracy, 
like Amazon, has a collaborative filtering algorithm for recommending books 
and they end up recommending the book which it thinks you're most likely to 
write highly, and so what they end up doing is probably recommending a book 
that you already have or that you Already know about and would have bought 
anyway right, which isn't very valuable. What they should instead have done 
is to figure out like which book can I recommend that would cause you to 
change your behavior right, and so that way we actually maximize our lift 
in sales. Due to recommendations - and so this idea of, like the difference 
between optimizing influencing your actions, were suggest of improving 
predictive accuracy, improving predictive accuracy is a really important 
distinction which, like very rarely discussed in academia or industry, kind 
of crazy enough. It's more discussed in industry. It's particularly ignored 
in most of academia right, so it's a really important idea, which is that 
in the end that our idea, the goal of your model, presumably is to 
influence behavior, okay and so and remember.</p>

<p>I actually mentioned a whole 
paper right have about this, where I introduced this thing called the 
drivetrain approach, where I talk about like ways to think about how to 
incorporate machine learning into like how do we actually influence 
behavior? So you know that's a starting point, but then the next question 
is like okay: if we're trying to influence behavior, what kind of behavior 
should we be influencing and how and what might it mean when we start 
influencing behavior okay, because, like nowadays like a lot of The 
companies that you're going to end up working at are bigger as companies 
and you'll be building stuff that can influence millions of people right. 
So what does that mean? So I'm? Actually, I'm not going to tell you what it 
means because, like I don't know, all I'm going to try and do is make you 
aware of some of the issues right and and make you believe, two things 
about them. First, that you should care right and second, that they're big 
current issues right. The main reason I want you to care is because I want 
you to want to be a good person and show you that, like not thinking about 
these things will make you a bad person. But if you don't find that 
convincing, I will tell you this. Volkswagen were found to be cheating on 
their emissions tests. The person who was sent to jail for it was the 
programmer that implemented that piece of code.</p>

<p>They did exactly what they 
were told to do right, and so, if you're coming in here thinking, hey I'm 
just a techie. You know I'll just do what I'm told right. That's that's my 
job is to do what I'm told I'm. If you do that, you can be sent to jail for 
doing what you're told okay, so so a don't just do what you're told, 
because you can be a bad person and B you can go to jail. Okay, second 
thing to realize is, in the heat of the moment, you're in a meeting with 
twenty people at work and you're all talking about how you're going to 
implement you know this new feature and everybody's discussing it and 
there's some cut. You know and everybody's like we could do this and here's 
a way of modeling it and then we can implement it and here's these 
constraints and there's some part of you. That's thinking, I'm not sure we 
should be doing this right. That's not the right time to be thinking about 
that, because it's really hard so like step up there and say. Excuse me, 
I'm not sure this is a good idea. You actually need to think about how you 
would handle that situation ahead of time. Okay, so I want you to like 
think about about these issues now right and realize that by the time 
you're in the middle of it right, you might not even realize it's 
happening. You know like they'll, just it'll just be a meeting like every 
other meeting, and a bunch of people will be talking about how to solve 
this technical question.</p>

<p>Okay and you need to be able to recognize like. 
Oh, this is actually something with ethical implications, so Rachel 
actually wrote all of these slides. I'm sorry. She can't be here to present 
this because, like she's studied this in depth - and you know she's 
actually being in in in difficult environments herself, where she's kind of 
seen these things happening, you know, and we know how hard it is right. 
But let me </p>

<h3>13. <a href="https://youtu.be/5_xFdhfUnvQ?t=1h8m15s">01:08:15</a></h3>

<ul style="list-style-type: square;">

<li><b> IBM and the “Death’s Calculator” used in gas chamber by the Nazis.</b></li>

<li><b>Facebook data science algorithm and the ethnic cleansing in Myanmar’s Rohingya crisis: the Myth of Neutral Platforms.</b></li>

<li><b>Facebook lets advertisers exclude users by race enabled advertisers to reach “Jew Haters”.</b></li>

<li><b>Your algorithm/model could be exploited by trolls, harassers, authoritarian governements for surveillance, for propaganda or disinformation.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Give you a sense of like what happens right so so engineers trying to solve 
engineering problems. Is you know and causing problems is not a new thing 
right. So in Nazi Germany IBM that the group known as Hollerith right 
Hollerith was the original name of IBM and it comes from the guy who 
actually invented the use of punch cards for tracking the US Census. The 
first mass wide-scale use of punch cards for data collection in the world 
right and that turned into IBM, and so at this point, if this this unit at 
least was still called Hollerith, so Hollerith sold a punch card system to 
Nazi Germany and so each punch Card would like code. You know this is a 
Jew, eight, gypsy, twelve general execution for death by gas chamber six, 
and so here's one of these cards describing the right way to kill these 
various people right and so a Swiss judge ruled that IBM's technical 
assistance facilitated the tasks of the Nazis and Commission of their 
crimes against humanity. This led through the death of something like 
twenty million civilians. So, according to the Jewish virtual library, 
where I got these pictures and quotes from their view is that the 
destruction of the Jewish people became even less important because of the 
invigorating nature of IBM's technical achievement only fightin by the 
fantastical profits to be made right.</p>

<p>So this was a long time ago and you 
know hopefully you won't end up working at companies that facilitate 
genocide Ryan, but perhaps you will right because perhaps you'll go to 
Facebook who are facilitating genocide. Right now, and - and I know people 
at Facebook who are doing this - and they had no idea - they were doing 
this right so right now in facebook, the rahega in the middle of a 
genocide, a Muslim population of Myanmar babies, are being grabbed out of 
their mother's Arms and thrown into fires people are being killed. Hundreds 
of thousands of refugees when interviewed the Myanmar generals. Doing this 
say we are so grateful to Facebook for letting us know about the ringing of 
fake news that the words they use their finger, fake news that these people 
are actually not human, that they're actually animals right now, Facebook 
did not set out to enable the Genocide of their hinga people in Myanmar. 
No, instead, what happened is they wanted to maximize impression in place 
right, and so it turns out that for the data scientists at Facebook's, 
their algorithms kind of learned that if you take the kinds of stuff people 
are interested in and think them slightly more extreme versions Of that 
you're actually going to get a lot more impressions and the project 
managers are saying maximize these impressions and people are clicking and 
like it, creates this.</p>

<p>This thing right, and so the the the potential 
implications are extraordinary and global right, and this is something that 
like is literally happening. You know this is October 2017. Is it's 
happening now? Okay? Could you pass that back there, so I just want to 
clarify what was happening here. So it was the facilitation it's like fake 
news or like inaccurate media yeah. So what happened was. Let me go into it 
in more detail. So what happened was in mid-2016 Facebook fired its human 
editors right, so it was humans that decided how to order things on your 
homepage. Those people got fired and replaced with machine learning, 
algorithms, and so the machine learning algorithms written by data 
scientists. Like you, you know they had nice, clear metrics and they were 
trying to maximize their predictive accuracy and be like okay. We think, if 
we put this thing higher up than this thing will get more place. Okay, and 
so it turned out that these algorithms, for putting things on the facebook 
newsfeed had a tendency to say, like Oh human nature, is that we tend to 
click on things which, like stimulate our views and are therefore like more 
extreme versions of things. We already see okay, so so this is great for 
the kind of Facebook revenue model of maximizing engagement. It looked good 
on all of their KPIs, and so at the time you know there was some negative 
press about like you know, I'm not sure that the staff that Facebook's now 
putting on their trending section is actually that accurate.</p>

<p>That, from the 
point of view of the metrics that people are optimizing at Facebook, it 
looked terrific and so way. Back to October 2016 people started noticing 
some serious problems. For example, it is illegal to target housing to 
people of certain races in America that is illegal, and yet a news 
organization discovered that Facebook was doing exactly that right in 
October 2016. Ok, not because somebody in that data science team said like, 
let's make sure black people can't live in nice, neighborhoods right, but 
instead you know they found that their automatic clustering and 
segmentation algorithm found. There was a cluster of people who didn't like 
african-americans and that if you targeted them with these kinds of ads, 
then they would be more likely to select this kind of housing or whatever 
right. But the interesting thing is that, even after being told about this 
three times, Facebook still hasn't fixed it right and that is to say these 
are not just technical issues. They're also economic issues right when you 
start saying like the thing that you get paid for, that is ads, you have to 
change the way that you structure those so that you know you either use 
more people that cost money or you like a less aggressive on Your 
algorithms to target people, you know based on like minority group status 
or whatever you know, that can impact revenues, and so the reason I mention 
this is you will at likely at some point in your career, find yourself in a 
conversation where you're thinking, I'm not Confident that this is like 
morally ok, the person you're talking to is thinking in their head.</p>

<p>This is 
going to make us a lot of money that, and you just you, don't quite ever 
manage to have a successful conversation because you're talking about 
difficult, different things, you know, and so, when you're talking to 
somebody who may be more experienced and more senior than You and they may 
sound like they know what they're talking about right, just realized, that 
their incentives are not necessarily going to be focused on like how do I 
be a good person. You like they're, not thinking how it might be a bad 
person, but you know the more time you spend an industry. In my experience, 
the more desensitized you kind of get to this stuff of, like okay, maybe 
getting promotions and making money, isn't the most important thing right. 
So, for example, I've got a lot of friends who are very good at computer 
vision, and some of them have gone on to create startups that seem like 
they're, almost handmade to help authoritarian governments surveil their. 
You know their citizens and when I ask my friends like have you thought 
about how this could be used in that way? You know they're generally kind 
of offended that I asked you know, but but I'm asking you to think about 
this, like you know, wherever you end up working, if you end up creating a 
start-up like tools can be used for good or for evil right, and so I'm not 
saying like don't create excellent object, tracking and detection tools 
from computer vision, because yeah you could go on and use that to create 
like a much better surgical intervention, robot tool, kit right.</p>

<p>I've just 
seen like be aware of it, think </p>

<h3>14. <a href="https://youtu.be/5_xFdhfUnvQ?t=1h66m45s">01:16:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Runaway feedback loops: when Recommendation Systems go bad.</b></li>

<li><b>Social Network algorithms are distorting reality by boosting conspiracy theories.</b></li>

<li><b>Runaway feedback loops in Predictive Policing: an algorithm biased by race and impacting Justice.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>About it, talk about it, you know so here's what I'd find like fascinating 
and there's this really cool thing. Actually, that made up calm did this is 
from a made-up contour kits online. They they think about this. They 
actually thought about this. They actually thought you know what, if we 
built a collaborative filtering system like we learned about in class, to 
help people decide what meetup to go to. It might notice that on the whole 
in San Francisco, a few more men and women tend to go to techie meetups, 
and so it might then start to decide to recommend techie meetups to more 
men than women, as a result of which more men will go to Techie meet us, as 
a result of which, when women go to techie meet ups they'll be like. Oh, 
this is all men. I don't really want to go to tech. He made ups as a result 
of which the algorithm will get new data, saying that men like taking 
meetups that right, and so it continues Matt and so like a little a little 
bit of kind of that initial push from the algorithm can create this runaway 
feedback. Loop and you end up with, like almost all my old tech meetups, 
for instance right and so this kind of feedback loop, is a kind of subtle 
issue that you really want to think about when you're thinking about like 
what is the behavior that I'm changing with This algorithm that I'm 
building so another example, which is kind of terrifying, is in this paper, 
where the authors describe how a lot of the partment s -- in the US are now 
using predictive policing, algorithms right.</p>

<p>So where can we go to find 
somebody who's about to commit a crime, and so you know that the algorithm 
simply feeds back to you. Basically, the data that you've given it right. 
So if your Police Department has engaged in racial profiling at all in the 
past, then it might suggest slightly more often. Maybe you should go to the 
black neighborhoods to check for people committing crimes right, as a 
result of which more of your police officers go to the black neighborhoods, 
as a result of which they arrest more black people, as a result of which 
the data says that The black neighborhoods are less safe, as a result of 
which the algorithm says to policeman. Maybe you should go to the black 
neighborhoods more often and so forth. Right - and this is not like you 
know, vague possibilities of something that might happen in the future. 
This is like documented work from top academics who have carefully studied 
the data and the theory right. This is like serious scholarly work is like 
no. This is this is happening right now, and so you know again, like I'm 
sure, all the people that started creating this predictive, policing, 
algorithm didn't think like how do we arrest more black people right? You 
know. Hopefully they were actually thinking. Gosh I'd like my children, to 
be safer on the street. It's how do I create you know a safer society 
right, but they didn't think about this.</p>

<p>This nasty, runaway feedback loop. 
So actually this this one about social network algorithms is actually a 
article in The New York Times recently that one of my friends Renee direst 
her and she did something that was kind of amazing. She set up a second 
Facebook account all right, like a fake facebook account, and she was very 
interested in the anti vex movement at the time. So she started following a 
couple of anti-vaxxers and visited a couple of anti-vaxxer links and so 
suddenly her newsfeed starts getting full of anti-vaxxer news along with 
other stuff, like chemtrails and deep state conspiracy theories and all 
this stuff, and so she's like starts clicking on those Right and the more 
she clicked the more hardcore far-out conspiracy, stuff facebook 
recommended. So now, when Renee goes to that Facebook account, the whole 
thing is just full of angry crazy, far-out conspiracy, stuff, like that's 
all she sees, and so if that was your world right, then as far as you're 
concerned is just like this continuous reminder and proof of Of all this 
stuff right - and so again, it's like this. This is to answer your 
question. This is the kind of ran away feedback loop that ends up telling 
me and my generals, you know throughout their Facebook homepage that reinga 
</p>

<h3>15. <a href="https://youtu.be/5_xFdhfUnvQ?t=1h21m45s">01:21:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Bias in Image Software (Computer Vision), an example with Faceapp or Google Photos. The first International Beauty Contest judged by A.I.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Animals and fake news, and whatever else all right, so you know it's it's a 
lot of this comes from also from bias right and so like. Let's talk about 
bias specifically so bias in image, software comes from bias in data and so 
most of the folks. I know at Google brain building computer vision, 
algorithms very few of them are people of color, and so when they're 
training, the algorithms with you know, photos of their families and 
friends. They are training them with very few people of color, and so, when 
face up then decided we're gon na try. Looking at lots of Instagram photos 
to see which ones are like you know, I've voted the most without them, 
necessarily realizing it. The answer was, like you know, light colored 
faces. So then they built a generative model to make you more hot, and so 
this is the actual photo, and here is the hotter version right. So the 
harder version is like more white, less nostrils. You know more 
european-looking, right and so like this did not go down well, to say the 
least. So like that, so again you know, I don't think anybody at face app 
said like let's create something that makes people look more white right. 
They just trained it on a bunch of images of the people that they had 
around them. Okay - and this has kind of you - know serious commercial 
implications as well. They had to pull this feature right and they had a 
huge amount of negative pushback like ads.</p>

<p>A short right - here's another 
example: Google photos created this photo classifier, airplane, skyscrapers 
cars, graduation and no guerrillas right. So, like think about how this 
looks to like most people like most most people, they look at this. They 
don't know about machine learning, they say what the somebody at Google 
wrote some code to take black people and call them gorillas. Like that's 
what it looks like right now. We know: that's not what happened right. We 
know what happened. Is you know they're a team. You know of folks at Google 
computer vision experts who have none if a few people of color working in 
the team built a classifier using all the photos they had available to 
them, and so when the system came along, came across, you know a person 
with dark Skin it was like - oh I've, only mainly seen that before, amongst 
gorillas, so I'll put it in that category right so again, it's the bias and 
the data creates a bias in the software and again, the commercial 
implications were very significant. Like Google really got a lot of bad PR 
as they should this, this was a photo that some you know somebody put in 
their Twitter feed. They said like look. What look what Google photos just 
decided to do? You can imagine what happened with the first international 
beauty contest judged by artificial intelligence, and basically it turns 
out all the beautiful people of what okay right so like you could kind of 
see this bias in image software thanks to bias in the data thanks to by 
Lack of diversity and the </p>

<h3>16. <a href="https://youtu.be/5_xFdhfUnvQ?t=1h25m15s">01:25:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Bias in Natural Language Processing (NLP)</b></li>

<li><b>Another example with an A.I. built to help US Judicial system.</b></li>

<li><b>Taser invests in A.I. and body-cameras to “anticipate criminal activity”.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Team's building it, you see the same thing in natural language. Processing. 
Alright, so here is Turkish. Oh is the the pronoun in Turkish, which has no 
gender right. There is no he or versus she right cut him, no okay! No! He 
visit she but of course, in English we don't really have a widely used and 
gendered singular pronoun. So Google Translate converts it to this okay. 
Now there are plenty of people who saw this online and said, like literally 
so what you know it is correctly feeding back the usual usage in English. 
Like this, is you know it's it? I know how this is trained. This is like 
word to Vic vectors. I was trained on Google News corpus, Google books 
corpus, it's just telling us how things are and like from a point of view, 
that's entirely true right, like the biased data to create this biased 
algorithm is the actual data of how people are written books and newspaper 
Articles for decades, but does that mean that this is the product that you 
want to create? You know, does this mean this is the product you have to 
create, but just because the particular way you've trained the model means 
it ends up doing this, you know, is this actually the design you want and 
can you think of potential negative implications and feedback loops? This 
could create no, and you know if any of these things bother you there now 
you're lucky you.</p>

<p>You have a new cool engineering problem to work on like 
how do I create unbiased and RP solutions, and now there are some startups 
starting to do that and starting to make some money right so like opera, 
these are opportunities for you. It's like hey. Here's some stuff where 
people are creating screwed up societal outcomes because of their shitty 
models like okay. Well, you can go and build something better right. So 
like another example of the bias in word, lvesque word: vectors is 
restaurant reviews ranked Mexican restaurants worse because Mexico, the 
Mexican words, tend to be associated with criminal words in the u.s. press 
and books. More often again, this is like a real problem that is happening 
right now, so you know, Rachel actually did some interesting analysis of 
just the plain word for backward vectors where she basically pulled them 
out - and you know looked at these analogies based on some research that 
Had been done elsewhere, and so you can see like where to Vic like the the 
vector directions show that father is the doctor is the mother is too 
nervous. Man is too computer programmer, as women is the homemaker and so 
forth right so like it's, it's really easy to see. What's in these word 
vectors - and you know they're kind of fundamental to much of the NLP 
you're, probably just about all the NLP software we use today, so like 
here's a great example, so a pro public has actually done a lot of good 
work in this area.</p>

<p>Judges many judges now have access to sentencing, 
guideline software and so Sentencing Guidelines. Software says to the judge 
for this individual. We would recommend this kind of sentence right and 
now, of course, a judge doesn't understand machine learning so like they 
have two choices, which is either. Do what it says or ignore it entirely 
right and some people fall into each category right and so for the ones 
that fall into the like. Do what it says, category here's what happens for 
those that were labeled higher risk right, the subset of those that label 
high risk, but actually turned out not to rear fender quarter of whites and 
about 1/2 of african-americans. Okay. So, like nearly twice as often right, 
people who didn't really marked as higher risk, if they're african-american 
and vice versa, amongst those labeled lower risk, but actually did 
reoffended to be about half of the whites and only 28 % of the 
african-americans like so like. This is data which I I would like to think 
nobody is setting out to create something that does this right. But when 
you start with bias data right - and you know, the data says that whites 
and blacks smoke marijuana at about the same rate that blacks are jailed 
at. I think it's something like five times more often than whites like you 
know, the nature of the justice system in America, or at least at the 
moment, is that it's not it's not equal.</p>

<p>It's not fair and therefore the 
data that's fed into the machine learning model is going to basically 
support that status quo and then because of the negative feedback loop, 
it's just going to get worse and worse right now, I'll tell you something 
else interesting about this one Which research court erred Gong has pointed 
out is here are some of the questions that are being asked right. So, 
let's, let's take one, was your father ever arrested right. So your answer 
to that question: it's going to decide whether you're locked up and for how 
long now, as a machine learning researcher, do you think that might improve 
the active accuracy of your algorithm and get you a better r-squared. It 
could well, but I don't know you know, maybe it does you try it out. So oh 
I've got a bit of R squared, so does that mean you should use it like? 
Well, there's another question: like: do you think it's reasonable to lock 
somebody up for longer because of who their dad was that and yet these are 
actually the examples of questions that we are asking right now to 
offenders and then putting into a machine learning system to Decide what 
happens to them? Okay, so again like whoever designed this. Presumably they 
were like laser focused on technical excellence, getting the maximum area 
under the ROC curve, and I found these great predictors that give me 
another 0.02 right and I guess didn't stop to think like.</p>

<p>Well, is that a 
reasonable way to decide who goes to jail for longer so like putting this 
together? You can kind of see how this can that you know more and more 
scary, we take a company like taser, right and tasers. Are these devices 
that kind of give you a big electric shock, basically and tasers, manage to 
do a great job of creating strong relationships with some academic 
researchers who seem to say whatever they tell them to say to the extent 
where now, if you look at the Data it turns out that there's a much higher 
problem. You know, there's a pretty high probability that if you get tased 
that you all die, it happens, you know not. Unusually and yet you know the 
researchers who they've paid to look into this have consistently come back 
and said: oh no, it was nothing to do with the Taser. The fact that they 
died immediately afterwards was totally unrelated. It was just a random, 
you know things things happen, so this company now owns 80 % of the market 
for body cameras and they've started buying computer vision, AI companies 
and they're going to try and now use these police body camera videos to 
anticipate criminal activity. Okay and so like what does that mean right? 
So is that, like okay, I now have some augmented reality display saying 
like pays this person because they're about to do something bad, you know. 
So it's like it's kind of like a whirring direction, and so you know I'm 
sure nobody who's a data scientist at taser or at the companies that they 
bought out is thinking like you know, this is the world I want to help 
create that they could find Themselves in you know, or you could find 
yourself in the middle of this kind of discussion, where it's not 
explicitly about that topic, but there's part of you, that's just like wow. 
I wonder if this is how this could be used right and - and you know I don't 
know exactly what the right thing to do in that situation is because, like 
you can ask and of course people gon na be like no.</p>

<p>No, no! No! So it's 
like you know, are you gon na? You know what what could you do know you 
could like ask for some kind of written promise. You could decide to leave. 
You could you know start doing some research into the legality of things to 
say like? Oh, I would at least protect my own. You know legal situation. 
</p>

<h3>17. <a href="https://youtu.be/5_xFdhfUnvQ?t=1h34m30s">01:34:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Questions you should ask yourself when you work on A.I.</b></li>

<li><b>You have options !</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>I don't know like have a think about how you would respond to that. So 
these are some questions that reaction Rachel created as being things to 
think about right. So if you're looking at building a data product - or you 
know using a model like if you're, building machine learning model, is your 
a reason? Okay, if you're trying to do something right. So what bias may be 
in that data right? Because, whatever bias is in that data ends up being a 
bias in your predictions, potentially then biases the actions you're 
influencing potentially then biases, the data that you come back and you 
may get a feedback loop. If the team that built it isn't diverse, you know 
what might you be missing yeah so, for example, one senior executive at 
Twitter called the alarm about major Russian bot problems at Twitter way 
back well before the election. That was the one black person in the exact 
team, a Twitter, the one and shortly afterwards they lost their job right 
and so like it definitely having a more diverse team. That means having a 
more diverse set of opinions and beliefs and ideas and things to look for 
and so forth. So non diverse teams seem to make more of these bad mistakes. 
Can we order a code as an open-source check for the different error rates 
amongst different groups? It's there like a simple rule.</p>

<p>We could use 
instead, that's like extremely interpretive all and easy to communicate 
and, like you know, if something goes wrong, do we have a good way to deal 
with it? Okay, so when, when we've talked to people about this and a lot of 
people like have come to Rachel and said, like I'm, I'm concerned about 
something my organization's doing, you know what do I do or I'm just 
concerned about my toxic workplace? What do I do and very often you know 
Rachel will say like well, have you considered leaving and they will say 
all I I don't want to lose my job, but actually, if you can code you're in 
like 0.3 percent of the population, if you can code And do machine learning 
you're in probably like 0.01 percent of the population? You are massively 
massively in demand so, like realistically, you know. Obviously it's an 
organization does not want you to feel like you're somebody who could just 
leave and get another job. That's not in your interest in their interest, 
but that is absolutely true right, and so one of the things I hope you'll 
leave this course with is, is enough self-confidence to recognize that you 
have the skills you know to get to get a job, and particularly once You've 
got your first job. Your second job is an order of magnitude, easier right, 
and so you know this is important, not just so that you feel like you 
actually have the ability to act ethically.</p>

<p>But it's also important to 
realize, like if you find yourself in a toxic environment right which is 
which is pretty damn common. Unfortunately, like there's a lot of shitty 
tech cultures, environments, particularly in the Bay Area. Right, you find 
yourself in one of those environments. The best thing to do is to get the 
hell out right and, and if you don't have the self-confidence to think you 
can get another job, you can get trapped right. So you know it's really 
important. You're really important to know that you are leaving this 
program with very in demand skills and, particularly after you have that 
first job you'll nail somebody with in-demand skills and a track record of 
being employed in that area. Okay, okay, great! So! Yes, this is kind of 
just a broad question, but what are some things that you know of that 
people are doing to treat bias in data? You know it's kind of like a bit of 
a controversial subject at the moment, and there are there are like people 
are trying to use. Some people try to use an algorithmic approach. You know 
where they're basically trying to say how can we identify the bias and kind 
of like subtract it out, but like that, the most effective ways I know of 
the ones that are trying to treat it at the data level. So, like start with 
a more diverse team, particularly a team involved, it you know and 
concludes people from the humanities like sociologists, psychologists, 
economists, people that understand, feedback, loops and implications for 
human behavior, and they tend to be equipped with.</p>

<p>You know good tools for 
kind of identifying and tracking these kinds of problems and so and then 
kind of trying to incorporate the solutions into the process itself. Let's 
say there isn't kind of. Like a you know some standard process. I can point 
you to and say here's how to solve it. You know if there is such a thing we 
haven't found it yet. You know it requires a diverse team of smart people, 
aware of the problems and what had of them it's the short answer. How can 
you pass that back please? This is just kind of a general thing. I guess 
for the whole class, if you're interested in the stuff that I read a pretty 
cool book, Jeremy you've probably heard of it weapons of mass destruction 
by Cathy O'neil. It covers a lot of the same stuff, yeah, just more topics. 
Yeah thanks the recommendation, so Kathy's great she's also got a TED talk. 
I didn't manage to finish the books. It's so damn depressing. I was just 
like yeah no more but yeah it's it's! It's very good all right. Well, 
that's it! Thank you, everybody! You know this has been. This has been 
really intense for me, you know. Obviously this was meant to be something 
that I was sharing with Rachel, so I've you know, ended up doing one of the 
hardest things in my life, which is to take two peoples worth, of course, 
on my own and also look after a sick wife and have A toddler and also do a 
deep learning course, and also do all this with a new library that I just 
wrote. So I'm looking forward to getting some sleep, but it's been, it's 
been totally worth it because you've been amazing, like I'm thrilled with 
how you've you know, reacted to that kind of.</p>

<p>You know the opportunities 
I've given you and also to the feedback that I've given you. So, 
congratulations. </p>





  </body>
</html>
