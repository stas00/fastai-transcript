<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 9: Object Detection</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://course.fast.ai/">fast.ai: Deep Learning Part 2 (v2) (2018)</a></p>
  <h1><a href="http://course.fast.ai/lessons/lesson9.html">Lesson 9: Object Detection</a></h1>
  <h2>Outline</h2>
<p>In today‚Äôs lesson we‚Äôll move from single object to multi-object detection. It turns out that this slight difference makes things much more challenging. In fact, most students found this the most challenging lesson in the whole course. Not because any one piece is highly complex, but because there‚Äôs a lot of pieces, so it really tests your understanding of the foundations we‚Äôve learnt so far. So don‚Äôt worry if a lot of details are unclear on first viewing ‚Äì come back to this lesson from time to time as you complete the rest of the course, and you should find more and more of it making sense!</p>

<p>Today‚Äôs focus is on the single shot multibox detector (SSD). This is a way to handle multi-object detection by using a loss function that can combine losses from multiple objects, across both localization and classification. It also uses a custom architecture that takes advantage of the difference receptive fields of different layers of a CNN. And we‚Äôll see how to handle data augmentation in situations like this one where the dependent variable requires augmentation too. Finally, we‚Äôll discuss and simple but powerful trick called focal loss which is used to get state of the art results in this field.</p>

  <h2>Video Timelines and Transcript</h2>

<h3>1. <a href="https://youtu.be/I-P363wSv0Q?t=30s">00:00:30</a></h3>

<ul style="list-style-type: square;">

<li><b>  Contribute to, and use Lesson 8 Wiki</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Okay, so today we're going to continue working on object detection, which 
means that, for every object in a photo with one of 20 classes, we're going 
to try and figure out what the object is and what its bounding box is. Hi 
that model to a new data set of unlabeled data and add those medals to the 
general approach. We're going to use this to start simple and gradually 
make it more complicated. So we started last week with a simple classifier, 
the three month of classifier. We then made it slightly more complex to 
turn it into a bounding box without a classifier today, we're going to put 
those two pieces together to make a classifier plus a bounding box. All of 
these are just for a single object, the largest object and then from there 
we'll roll it up to something which is closer to final goal. You should go 
back and make sure that you're understanding all of these concepts from 
last week before you move on. If you don't go back and really go through 
the locals carefully, I won't read them all to you, because you can see in 
the video needs to be enough. That practice is the most important knowing 
how to jump around source code in whatever, but the lambda functions. 
Lambda function is also particularly important. They come up everywhere, 
and this idea of a custom head is also gon na, come up in pretty much every 
lesson. I've also added here a a reminder of what you should know from part 
one of the course, because, quite often, I see questions on the forum 
asking. Basically, why isn't my model working like? Why doesn't it start 
training or having trained? Why doesn't it seem to be any use and nearly 
always the answer to the question is: did you print out the inputs to it 
from a data </p>

<h3>2. <a href="https://youtu.be/I-P363wSv0Q?t=2m">00:02:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Experiments on Image/Neural Style Transfer</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Loader, did you print out the outputs from a after evaluating and normally 
the answer is no and I try printing it and it turns out all the inputs, 0 
or all of the appellants negative reports when it was really obvious. So 
that's just part of something wanted to remind you about this. You need to 
know how to do these two things. If you can't do that, that's gon na be 
very hard to debug models and B. If you can do that, if you're not doing 
it, then it's going to be very happy to debug models. You could have debug 
models by staring at a source code, hoping your error, pops out your debug 
models by checking all the intermediate steps. Looking in the data 
printing, it out plotting its histogram, making sure it says okay, so we 
were working through Pascal notebook and we just quickly zipped through the 
bounding box of the largest object without a classifier, and there was one 
bit that I skipped over and said. I've come back to so. Let's do that now, 
which is to talk about augmentations data augmentations of the the Y of the 
dependent variable before I do. I just mention something pretty awkward in 
all this, which is I've got here image classifier data continuous equals 
true. This makes no sense whatsoever. A classifier is anything where the 
dependent variable is categorical or binomial, as opposed to regression, 
which is anything with the pen and variable is continuous, and yet this 
parameter here continuously was true, says that the dependent variable is 
continuous. So this claims to be creating data for a classifier where the 
dependent is continuous.</p>

<p>This is the kind of awkward rough edge that you 
see when we're kind of at this. Like you know, at the edge of the pasta, 
our code is not quite solidified. Yes, so probably by the time you watch 
this in the MOOC, this will be sorted out. This is before even you regress 
it or something like that, but you know I just wanted to kind of point out 
this this issue and also because sometimes people were getting confused 
between regression, business classification, and this is okay. So let's 
create some data augmentations right there. Normally, when we create data 
augmentations, we tend to type in like transform, sidon or transform spa 
gym. But if you look inside that fast, they are transforms. Module you'll, 
see that they are simply defined as a list. So this one called transforms 
basic, which is 10 degree, rotations, plus 0.05 brightness and contrast, 
and then sidon adds to that and random horizontal flips or else top down as 
to that random dihedral group of symmetry flips, which basically means, if 
we possible 90 degree rotation optionally. So like these are just a little 
shortcuts that I added because time, but you can always create your own 
list of augmentations right and, if you're not sure what augmentations are 
there, you can often see check the past AI source or, if you just start 
typing random. They all start with, so you can see.</p>

<p>So, let's take a look 
at what </p>

<h3>3. <a href="https://youtu.be/I-P363wSv0Q?t=5m45s">00:05:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Advanced tips from Keras on Neural Style Transfer</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Happens if we create some data, augmentations create a model data object 
and let's just go through and rerun the iterator a bunch of times, and we 
all do two things we'll print out the bounding boxes. And so you can see 
the value box is the same time and we will also draw the pictures so you'll 
see this lady is, as we would expect, flipping around and spinning around 
getting darker and lighter, but the bounding box, a is not moving and B, is 
In the wrong spot, so this is the problem with data augmentation, when your 
dependent variable, you is pixel values or is in some way connected to your 
independent variable, its who need to be augmented together and in fact you 
can see that from the printout. These numbers are bigger than two to four, 
but these images are of size, two to four that what we requested in this in 
this transpose, and so it's not even being like scaled or cropped or 
anything right. So you can see that how the pendant variable needs to go 
through all of the same geometric transformations without independent 
variable. So to do that every transformation has an optional transform Y 
parameter. It takes a transform type-in um. The transform type-in um has a 
few options. All of which we'll cover in this course, the co ward up ssin, 
says that the Y values represent coordinates in this case.</p>

<p>Bounding box 
coordinates okay, and so, therefore, if you flip, you need to change the 
coordinate derivative represent that flip or if you rotate you to change 
the coordinate, represent a rotation, so I can add, transform type a chord 
to all of my augmentations. I also have to add the exact same thing to my 
transforms from model function, because that's the thing that does the 
cropping, and/or, zooming and or padding and or resizing, and all of those 
things need to happen to the dependent variable. So if we add all of those 
together and rerun, this you'll see the bounding box changes each time and 
you'll see us in the right spot now, you'll see. Sometimes it looks a 
little odd like here. Why is that bounding box there and the problem is. 
This is just a constraint that the information we have right, the bounding 
box does not tell us that actually, her head isn't way over here in the top 
left corner. Alright. But actually, if you do a thirty degree, rotation in 
her head was over here, the top left corner, then the new bounding box 
would need would go really high right. So this is actually the correct 
bounding box based on the information and has available, which is to say 
this is this is how higher Mobe so basically, you've got to be careful of 
not doing to higher rotations with bounding boxes, because there's not 
enough information for them To stay totally accurate, just fundamental 
limitation of the information we're given if we were doing like polygons or 
segmentations or whatever we wouldn't have this problem.</p>

<p>Okay, so I'm gon 
na do a maximum of three degree rotations, I'm also going to only rotate 
half the time. My random flip, my brightness/contrast changing and so 
there's my set of transformations that I can use. So we briefly looked at 
this custom head idea, but basically, if you look at dot summary dot 
summary does something pretty cool, which is. It basically runs a small 
batch of data through a model and prints out how big it is at every every 
layer, and we can see that at the end of the convolutional section before 
we get the flatten it's 512 by 7, by 7, okay and so 512. By 7, 5 7 10 serve 
breakfast reach answer at that size. If we flatten it out into a single 
rank, one tensor into a vector, it's going to be 225 thousand a ninety 
eight long right, </p>

<h3>4. <a href="https://youtu.be/I-P363wSv0Q?t=10m15s">00:10:15</a></h3>

<ul style="list-style-type: square;">

<li><b> More tips to read research papers &amp;</b></li>

<li><b>‚ÄúA Neural Algorithm of Artistic Style, Sep-2015‚Äù</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>So then, that's why we had this linear layout to 500 for the bosses right. 
So stick that on top of a pre-trained ResNet and trade it for a while, 
okay. So that's where we got to last time. So, let's now put those two 
pieces together so that we can get something that classifies and does 
bounding boxes and there are. There are three things that we need to do 
basically to train a neural network ever right. We need to provide data. We 
need to pick some kind of architecture and we did a loss function. Okay, so 
the loss function says you know something anything that gives a lower 
number here is a better network using this data in this architecture. So 
we're going to need to create those three things for our classification 
plus bounding box regression. So that means we need a model data object 
which has as the independence the images and as the dependence I want to 
have a tupple the first time. One of the tuples should be the bounding box 
coordinates and the second element, as a couple should be there class. 
Okay, there's lots of different ways. You could do this. The particularly 
lazy and convenient way I came up with was to create two mobile data 
objects representing the two different dependent variables I want so one 
with the bounding box coordinates one of the classes just using the CSVs. 
We go over four, and now I'm going to merge them together.</p>

<p>So I create a 
new data set class and a data set class is anything which has a length and 
an index service or something that lets. You use it as like lists, and so 
in this case I can have a constructor which takes an existing data set and 
the second dependent that I want the length then is just obviously the 
length of the data set. The first data set and then getitem is grab the X 
and the y from the data set that I passed in and return that X and that Y 
and the ayth of the second so there's a data set. That basically adds in a 
second. As I said, there's lots of ways you could do this, it's kind of 
convenient, because now what I could do is I can create training data set 
in the validation data set. Based on that, so here's an example: you can 
see. It's got a couple of the bounding box coordinates in the class. We can 
then take the existing training and validation data loaders. Now so you 
replace their data sets with these and unknown okay. So we can now test it 
by grabbing a mini batch of data and checking it says: okay, so there's one 
way to customize data set. So what we're going to do this time now is: 
we've got the data, so now we need an architecture, so the architecture is 
going to be the same as the architectures that were used for the classifier 
and for the bounding box, aggression, but we're just going to Combine them 
so, in other words, if there are C classes, then the number of activations 
we need in the final layer is 4 plus C for panama's coordinates and the C 
probabilities one per class.</p>

<p>So this is the final layer, a linear layer 
that has four plus men of categories divisions. The first player is before 
is a flattened. We could just join those up together, but in general I want 
my my custom head to like, hopefully be capable of solving the problem that 
I give it on its own. If the pre-trained backbone is connected to is, you 
know, is appropriate, and so in this case I'm thinking. Okay, I'm trying to 
do quite a bit here, two different things: there classifier and balanced 
regression. So just a single linear layer doesn't sound like enough. So I 
put in a second millionaire okay, and so you can see we basically go over 
Al. You drop out Lydia, rarely veteran or dropout yeah. If you're wondering 
why there's no better on that here at the resonant backbone, it already has 
a match norm as it's. Finally, out okay, so this is basically nearly the 
same custom headers before it's just. It's got two linear layers, one and 
nonlinearities. Okay. So that's piece to but data we've got architecture. 
Now we need a loss function, so the loss function needs to look at these 
four plus C activations and aside. Are they good right? Are these numbers 
accurately reflecting the position and class of the largest object in this 
image? We we know how to do that. For the last, for the first four, we use 
l1 loss, just like we did in the bounding box regression before remember l1 
loss is like mean squared error.</p>

<p>Rather than sum of squares is some of our 
values and then for the rest of the activations. We can use cross-entropy 
loss, so let's go ahead and do that so we're going to create something 
called detection, loss and loss functions, always take an input and a 
target. That's what pytorch always calls them. So this is the activations. 
This is the ground truth. So remember that our our date, custom data set 
returns. A tuple containing the bounding box coordinates in the classes. So 
we can D structure that use D, structuring assignment, to grab the bounding 
boxes and the classes of the target, and then the bounding boxes and the 
classes of the input are simply the first four elements of the input and 
the four onwards. Elements of the universe and remember, we've also got a 
batch dimension. So that's it. We've now got the bounding box target 
bounding box import class target plus input further bounding boxes. We know 
that they're going to be between 0 & amp 2 to 4. The coordinates because 
that's how they got images - okay, so let's grab a sigmoid to force it 
between 0 & amp, 1, multiply it by 2 to 4 and that's just helping our 
neural net. You knowget close to what we you know be in the range we know 
it has to be as a general rule. Is it better to put bash norm before or 
after RL you? I would suggest that you should put it after a value, because 
batch norm is meant to move towards a computer, one random variable and if 
you put value after it, then you're truncating it 0. So there's no way to 
create negative numbers about a huge port value.</p>

<p>Lembeck norm, having said 
that, and I think that that way of doing it gives slightly better results. 
Having said that, it's not too big a deal either way and you'll see during 
this part of the course most of the time. I go well you and then vet norm, 
but sometimes they go vetch moment in value. If I'm consistent, so I think 
originally the batch normal is put it off to the activation, so, okay, so 
so this is kind of to help our data or force our data into the right range, 
which you know if you can do stuff like that, it makes It easier to Train 
yes, Rachel, one more question: what's the intuition behind using dropout 
with P equals 0.5 after a batch norm, does a batch alarm already do a good 
job of regularizing fessional doesn't okay Java vaporizing and if you think 
that to part one we have Had that list of things we do to avoid overfitting 
and adding batch long as one of them is david augmentation, but it's 
perfectly possible that you'll still be okay. So one nice thing about 
dropout is that it has a parameter to say how much to drop out, and so 
that, like parameters, are great like well specifically, parameters that 
decide how much to regularize. Because it lets you build a. I speak over 
priced model and then decide on interiorize it so yeah I tend to always 
drop out and then, if it turns out, I'm you know I'll start with P equals 
zero and then, as you know, I can just change my grammar without worrying 
about you Know if I saved a model, I'm gon na be able to load it back, but 
if I have dropout ladies and one and not in another or load me more, this 
way it stays consistent.</p>

<p>Okay, so now that I've got my inputs and targets, 
I can just go: hey calculate the l1 loss and add to it the cross-entropy 
okay. So that's our that's! Our loss function. It's a surprisingly easy 
brass. Now, of course, the cross entropy and the l1 lost. Maybe of wildly 
different scales, in which case in the loss function, the larger one is 
going to dominate, and so I just ran this in a debugger checked what you 
don't? You just use print check how big each of the two things were and 
found if they multiply by 20. That makes them about the same about the same 
scale as your training, it's nice to print out information as you go, so I 
also grabbed the l1 part of this and put it in our in a function, and I 
also created a function for accuracy so that I could make the metrics and 
so they're down alright, so we can how about something which is printing 
out our object, detection loss, detection, accuracy and detection l1 and so 
chain it for a while and it's looking good a detection. Accuracy is in the 
low 80s, which is the same as what it was before. That doesn't surprise me 
because, like ResNet was designed to do classification, so I wouldn't 
expect us to be able to improve things in such a simple way, but it 
certainly wasn't designed to do bounding box regression. It was explicitly 
actually designed in such a way is to be as to kind of not care about 
geometry. Rather it takes that last seven, most seven creative activations 
and averages them all together and throws away all of the information that 
is going wrong.</p>

<p>So so you can see that the when we only train the last 
layer. The detection l1 is pretty bad running poor and it really improves a 
lot, but where else the accuracy doesn't improve, it stays exactly the 
same. Interestingly, the l1, when we do accuracy and bowing boss at the 
same time, 8.5 seems like it's a little bit better than when we just do 
bounding box regression. </p>

<h3>5. <a href="https://youtu.be/I-P363wSv0Q?t=23m">00:23:00</a></h3>

<ul style="list-style-type: square;">

<li><b> From Style Transfer to Generative Models</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>And if that's counterintuitive to you, then this would be one of the main 
things to think about. After this lessons, it's a really important idea and 
the idea is this figuring out figuring out what the main object in an image 
is, is kind of the the eye part and then figuring out like exactly where 
the bounding box is and what class it is, is Kind of the easy part in a 
way, and so when you've got a single network, that's about saying what is 
the object and where is the object? It's going to share all of the 
computation about like finding the object, and so all that shared 
information. All that shared computation is very efficient, and so, when we 
vet propagate the errors in you know the class and in the place, that's all 
information. That's going to help the computation around like finding the 
biggest object. So anytime, you've got multiple tasks, which kind of share 
some some concept of what those tasks would need to do to complete their 
work. It's very likely they should share at least some layers of were 
together and we'll look later today, a place where most of the layers are 
shared, but are just the last one, isn't? Okay, so you can see. This is 
some doing a good job as before of anytime. There is just a single major 
object, sometimes it's getting a little confused. It thinks the main object 
here is the dog and it's kind of served with the dog, although it's kind of 
recognize that actually, the main object is us so far, and so the 
classifier is doing the right thing with the bounding boxes labeling the 
wrong thing, which Is yours when there are two birds you can only pick one, 
so it's just kind of hitching in the middle get over these lots of cows and 
so forth, doing good job with this car all right.</p>

<p>So that's! So! That's 
that all right there's not much new. There, although in that last bit, we 
did learn about you know some simple custom data sets and simple custom 
Lots functions. Hopefully you can see now how easy that is to do so next 
stage for me would be to do multi-label classification. So this is this 
idea that I just want to keep building models that are slightly more 
complex than the last model, but hopefully don't require too much extra 
concepts, so I can kind of keep seeing things working and if something 
stops working, I know that wherever I Find building everything at the same 
time so model label classification is so easy, is there's not much to 
mention so we've moved to a Pascal multi. Now that's what we're going to 
do. The multi object stuff so for the multi object, stuff, I've just copied 
and pasted the functions from the previous notebook that we used so they're 
all at the top. So we can create now a multi class, a multi class CSV file, 
using the same basic approach that we did last time and I'll mention by the 
way one of our students actually who's visiting from India. Funny pointed 
out to me that all this stuff we're doing with default, dicks and stuff 
like that, he actually showed her a way of doing that which was much 
simpler, using pandas and he shared that on the forum. So I totally bow to 
his much better approach. Simpler and more concise approach, yeah, it's 
definitely true.</p>

<p>Like the more you get to know handers, the more often you 
realize is a good way to solve lots of different problems, so definitely 
check that out when you're building out the smaller models and you're 
iterating. Do you reuse those models as pre-trained waits for this link 
larger one, or do you just toss it all away and then retrain from scratch 
with when I'm kind of like figuring stuff out? As I go like this, I would 
generally towards tossing away because they're kind of reusing, pre-trained 
weights introduces complexities that are not really to think about. 
However, if I'm trying to get to a point where I can run something on 
really big images, doing these much smaller ones and often I don't reuse 
those ways, okay, so in this case, what we're doing is we are just joining 
up all of the classes with A space which gives us a CSV in the normal 
format and once we've got the CSV in a normal format. It's the usual three 
lines of code and we train it and we've checked the results. So, there's 
literally nothing to show you yeah and, as you can see, it's done a great 
job. The only mistake I think it made was it called this dog for us it 
should have been dog and so far okay, so Maury class classification is, is 
pretty straightforward. One minor tweak here is to note that I used a set 
here because I don't want to list all of the objects.</p>

<p>I only want each 
object type type here once and so the set plus is a way of deem cute. 
Locating so that's why I don't have person person, person, person, person 
just appears so yeah. These sum. These object classification, pre-trained 
networks. We have a really pretty good at recognizing multiple objects as 
long as you only have to mention each one once so. That works pretty. Well. 
All right, so we've got this idea that we've got an input image that goes 
through a con ComNet. You know which was kind of treated the black box and 
it spits out a tensor vector of size, 4 plus C. Right C is the number of 
classes, and so that's what we've got and that gives us a an object 
detector for a single object. The largest object Americus. So let's now 
create one which doesn't find a single object, but that finds 16 objects. 
Okay, so an obvious way to do that would be to take this last. This is just 
a n n dot, minion right, which has got, however many inputs and 4 plus C 
outputs. We could take that linear layer and rather than having 4 plus C 
outputs, we could have 16 times 4 plus C outputs. So it's now spitting out 
enough things to give us 16 sets of class probabilities and 16 sets of 
bounding box coordinates, and then we would just need a loss function that 
will check whether those 16 sets of bounding boxes correctly represented 
the up to 16 objects that Were represented in the image now, there's a lot 
of hand waving about the loss function, we're going to it later as to what 
that is.</p>

<p>But let's pretend we have one: okay, assuming we had a reasonable 
loss function, that's totally going to work right that that is an 
architecture which has the necessary output activations. But with the 
correct loss function, we should be at a trainer to do what we wanted to 
do. Okay, but that's just one way to do it, there's a second way. We could 
do it rather than having a n n dot linear what if, instead, we took from 
our ResNet convolutional background backbone, not in linear, but instead we 
added a an end, come to D with stride to right, so the final layer of resin 
app is gets. You a seven by seven by 512 as all right, so this would give 
us a four by four by whatever number of filters result, maybe for the 
number of filters that say we picked 256 okay, so it would be four four 
four by four by 256 has Well, actually, </p>

<h3>6. <a href="https://youtu.be/I-P363wSv0Q?t=32m50s">00:32:50</a></h3>

<ul style="list-style-type: square;">

<li><b> ‚ÄúPerpetual Losses for Real-Time Style Transfer</b></li>

<li><b>&amp; Super-Resolution, Mar-2016‚Äù</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Look, let's change that, let's not make it four by four by 256; better 
still, let's do it all in one step, let's make it four by four by four plus 
C, because now we've got a tensor, where the number of elements is exactly 
equal to the number Of elements we wanted, so in other words we could we 
could now. This would well to if we created a loss function that took a 
four by four by four plus c tensor and mapped it to sixteen objects in the 
image and checked whether each one was correctly represented by those four 
plus c activations. That would work like these are two exactly equivalent 
sets of activations, because they've got the same number of elements 
they're just reshaped yeah, so it turns out that both of these approaches 
are actually used. The approach where you basically just spit out one big 
long vector from fully connected linear layer, is used by a class of models 
known as yellow. Where else the approach of the convolutional activations 
is used by models which started with something called SSD single. What I 
will say is that, since these things came out of very similar times in late 
2015, things are very much moved towards here. So the point where this 
morning, Yolo version 3 came out and is now doing it, the SSD. Ok, so 
that's what we're going to do right, we're gon na do this and we're gon na 
learn about why this makes more sense as well, and so the basic idea is 
this: let's imagine that, on top of underneath this we had another another 
come to D Stripe 2 and we'd have something which was to buy to buy again 
less sales for plus C, alright, that's nice and simple, and so basically, 
it's creating a grid.</p>

<p>That looks something like this one, two, three four 
okay, so that would be like how the activations are. You know the geometry 
of the activations of that second extra convolutional straight laughs. 
Remember it's fair to convolution does the same thing to the geometry of 
the activations. As a stripe, one convolution below biomass pulling okay. 
So let's talk about what we might do here, because the basic idea is like 
we want to kind of say: alright, this top left grid cell is responsible for 
identifying any object. That's in the top of left this one in the top right 
is responsible for identifying something in the top right. This one bottom 
left this one at the bottom right, okay, so in this case you can actually 
see it started. It said: okay, this one is going to try and find the chair 
this one, that's actually made a mistake introducing a table, but there are 
actually one two three chairs here as well right, so basically, each of 
these grid cells, it's going to be told in the Loss function, your job is 
to find the object. You know the big object is in that part of the image. 
So what so? For a multi-label classification, i saw you had a threshold on 
there, which i guess we're getting your relative. Let's, let's work through 
it. Okay right, so why do we care about the idea that we would like this 
convolutional grid cell to be responsible for finding things that were in 
this part of the image? And the reason is because of something called the 
receptive field of that car and the basic idea is that, through actual 
convolutional layers, every every piece of those tensors has a receptive 
field, which means which part of the input image was responsible for 
calculating that cell right And, like all things in life, the easiest way 
to see this is with Microsoft Excel. So do you remember our convolutional 
neural net and this was emili¥s and we had the number 7 and it went through 
a a two channel filter channel 1 channel 2, which therefore created a 2 
channel output.</p>

<p>Okay and then the next layer was another convolution. So 
this tensor is now a 3d tensor, okay, which then creates saved again to 
channel output, and then, after that, we had our max pooling layer. Okay. 
So let's look at this part of this output and the fact this is common, 
followed by max pool. Let's just pretend as a stripe to come. That's 
basically the same! So, let's see where this number 27 came from. So if 
you've got Excel, you can go formulas trace precedents right, and so you 
can see this came from these 4 okay. Now, where did those 4 come from? 
Those 4 came from obviously the convolutional filter, Colonel kernels and 
from these 4 parts of time, one right </p>

<h3>7. <a href="https://youtu.be/I-P363wSv0Q?t=39m30s">00:39:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Implementation notebook w/ re-use of ‚Äòbcolz‚Äô arrays from Part 1.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Because we've got four things here, each one of which has a 3 by 3 filter, 
and so we have 3, 3, 3. 3, the other boy, oh, where did this four come 
from? Those four came from obviously our filter and this entire part of the 
input image. Okay and what's more, you can see - and it also comes through 
this whole duration as well, and you can see that these bits in the middle 
have lots of weights coming out batgirls. These bits on the outside only 
have one weight coming out. So we call this here, the receptive field of 
this activation right, but note that the receptive field is not just saying 
it's this here box, but also that the center of the box has more 
dependencies. So this is a critically important concept when it comes to 
kind of understanding, architectures and understanding why confidence work 
yeah of the receptive field, and there are some great articles. If you just 
Google for convolution receptive field, you can find lots of through the 
articles. I'm sure some of you will write much better ones during the week 
as well. So that's the basic idea there right is that the receptive field 
of this convolutional activation is generally centered around this part of 
the input image. So it should be responsible for finding objects that here, 
so that's the architecture. The architecture is that we're going to have a 
resonant backbone, followed by one or more 2d convolutions, and for now 
we're just going to do one right, which is going to give us a four by four 
grid. So, let's take a look at that, so here it is, we start with our Lu 
and drop out.</p>

<p>We then do at the start at the output. Well, I just go 
through and see what we've got here. This one is not being used. We start 
with a straight one convolution and the reason we start with a straight 
one. Convolution is because that doesn't change the geometry at all. It 
just lets us add an extra layer of calculations right, let's create you 
know not just a linear layer, but now we have like a little mini neural 
network in our custom here, all right, so we start with this. Dr1 
convolution and standard college is just something i defined up here, which 
does convolution value vaginal. Like most research code, you see, won't 
define a class like this. Instead, they'll write the entire thing again and 
again and again, convolution don't be like that right, like that kind of 
typical code, leads to errors and leads to poor understanding, and I 
mentioned that also because this week I released </p>

<h3>8. <a href="https://youtu.be/I-P363wSv0Q?t=43m">00:43:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Digress: how ‚Äúpractical‚Äù are the tools learnt in Part 2, vs. Part 1 ?</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>The first draft of the FASTA, a style guide and the faster I style guide, 
is very heavily Orion towards the idea of expository programming, which is 
the idea that programming code should be something that you can use to 
explain an idea, ideally as readily as mathematical notation To somebody 
that understands your your coding method, and so the idea actually goes 
back a very long way, but it was best described in the Turing award like 
Genesis like the Nobel in computer science, the cheering award lecture of 
1979 by probably my greatest computer science hero Can I was he had been 
working on a since well well before in 1964, but 1964 was the first example 
of this approach to programming. He released something called APL and then, 
25 years later he won the Turing award. He then passed on the baton to his 
son, Eric Iverson and there's been basically 50 or 60 years now of 
continuous development of this idea of like what does programming look like 
when it's designed to to be a notation notation as a tool for thought for 
expository Programming and so I've made a very shoddy attempt at taking 
some of these ideas and thinking about how can they be applied to 
programming, with all the limitations, by comparison that python has anyway 
so, but you know, here's a very simple example is that if you write All of 
these things again and again and again, then it really hides the fact that 
you've got.</p>

<p>You know two convolutional layers, one of this dried one, one 
of this dried, so my default for standard canvas tried to as a straight 
one. This is a straight two and then at the end, so this the output of this 
is going to be four by four okay. I've got a outcomes and an outcome. Vis 
interesting you can see. It's got two separate convolutional layers, each 
of which is straight one. So it's not changing the geometry of the input. 
Okay, one of them is of length of the number of classes. Just ignore K for 
now K is equal to K is equal to 1 at this point in the code. So it's not 
doing anything. So what is equal to the length of the number of classes? 
One is equal to four, and so this is this idea of, rather than having a 
single comp layer that outputs 4 plus C, let's have two complex, one of 
which outputs for one of which outputs C and then I will just return them 
as a list of Two items: that's nearly the same thing: it's nearly the same 
thing as having a single column player that outputs 4 + C, but let's it 
lets these layers specialize just a little bit alright. So, like we talked 
about this idea that when you've got kind of multiple tasks, they can share 
layers, but they don't have to share all the layers. So in this case our 
two tasks, which is fine, create a classifier and create down paths. 
Regression share every single layer is set the very last one okay, and so 
this is going to spit out two separate tensors and activations one of the 
classes and one of the coordinates.</p>

<p>Why am i adding one? That's because I'm 
going to have one more class for background right, so if there aren't 
actually sixteen objects to detect or if there is an object in this corner 
represented by this population or grid cell, then I want you to predict 
background. So that's the entirety. That's the entirety of our 
architecture. It's incredibly simple right, but the point is now that we, 
you know we have this convolutional air at the end. One thing I do do is 
that I at the very end I flatten out the convolution. Basically because I 
wrote the loss function to expect flattened out tensor, but I could totally 
try doing that during the week and see which one was easier to understand. 
Okay, so we've got our data, we've got our architecture, so now all we need 
is a loss function. Okay, so the loss function needs to look at each of 
these 16 sets of activations, each of which you're going to have four 
bounding box, coordinates and C plus one class probabilities and aside, are 
those activations close or far away from the object, which is kind of 
Closest to this this this grid cell, in the image and if nothing's there, 
then you know, are you predicting background correctly, so that turns out 
to be very hard to do, because let's go back to the 2x2 example to keep it 
simple, the loss function actually needs To take each of the objects in the 
image and match them to one of these convolutional grid cells. To say, like 
this grid cell is responsible for this particular object.</p>

<p>This grid cell is 
responsible for this particular object. So then it can go ahead and say 
like okay, how close are the four coordinates and how close are the master 
probabilities right? So this is called the matching problem and in order to 
explain it, I'm going to show it to you. But what I'm going to do first, is 
I'm going to take a break okay and we're going to come back and understand 
the maxing map? The matching problem so during the break have a think about 
how would you design a loss function here? How would you design a function 
which has a lower value if these 16 times 4 plus K activations, you know 
somehow better reflect the up to 16 objects which are actually in the 
ground. Truth image and we'll come back at 7:40. So here's our goal - our 
dependent variable, basically looks like that, and those are just an 
extract from our CSV file trapped in dependence and our final convolutional 
layer is going to be a bunch of numbers which initially is a four by four. 
I, in this case I think C, is equal to twenty plus we've got one for 
background right, so four plus 21 equals 26 all right four by four okay and 
then we we flatten that out into vector we flatten that out into a vector, 
and so basically Our goal, then, is to say to some particular set of 
activations that ended up coming out of this model for some let's, let's 
pick some particular dependent variable.</p>

<p>We need some function that takes 
in that and that right and where, if it feeds back a higher number, if 
these activations aren't a good reflection of the ground, truth bounding 
boxes or a lower number. If it is a good reflection of the ground through 
the bounding boxes, that's how cool we need to create that much, and so the 
general approach to creating that function will be to first of all, to 
simplify it. Down with the two-by-two version will be the first of all. 
Well, actually I'll show you here's a model I trained earlier. Okay and 
let's run through I've, taken the loss function and I've split it line by 
line so that you can see every line that goes into menu. Okay, so so, let's 
grab our </p>

<h3>9. <a href="https://youtu.be/I-P363wSv0Q?t=52m10s">00:52:10</a></h3>

<ul style="list-style-type: square;">

<li><b> Two approaches to up-sampling: Deconvolution &amp; Resizing</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Validation set data loader grab a batch from it turn them into variables, 
so we can stick them into a model, put the model in evaluation mode. Stick 
that data into. We don't stick that data into our model to grab a batch of 
activations and remember that the final output convolution returned two 
items that the classes and the bounding boxes. So we can do destructuring 
assignment to grab the two pieces, the batch of classes and outputs and the 
batch of bounding box. Okay, and so, as expected, the batch of class 
outputs is batch, size 64 by 16 grid cells by 21 classes and then 64 by 
sixteen. By four through the bounding box coordinates: okay, hopefully that 
all makes sense and after class go back into spec sure. It's not obvious 
why? These are the shapes, make sure you get to the point where you 
understand where they are. Let's now go back and look at the ground truth, 
so the ground truth is is in this Y variable. So let's grab the bounding 
box part and the plas part and put them into these two Python variables and 
print them out and so there's our ground truth. Bounding boxes and there's 
our ground truth classes. So this this image apparently has three objects. 
You know. So, let's draw a picture of the three objects and there they are 
okay. We already have a show ground. Truth function, the torch ground, 
truth function, simply converts the tensors into numpy and passes them on 
so that we can print them out. So here we've got.</p>

<p>The bounding box 
coordinates you'll notice that they've all been scaled to zero to what 
between 0 and 1 okay. So, basically we're treating the image as being like 
1 by 1, so these are all relative to the size of the image. There's our 
three classes - and so here they are chair - is 0 dining table is 1 and 2 
is so far. This is not a model. This is the ground truth. Great here is our 
4 by 4 grid cells from our final convolutional letter. So each of these 
square boxes different papers call them different things. The three terms 
you're here are anchor boxes prior boxes or default boxes. Okay and through 
this explanation, you'll get a sense of what they are, but for now think of 
them as just these 16 squares, I'm going to stick with the term anchor 
boxes. Ok, these 16 squares on our end devices. So what we're going to do 
for this loss function is we're going to go through a matching problem 
where we're going to take every one of these 16 boxes and we're going to 
see which one of these three round truth objects has the highest amount of 
overlap With this square, okay, so to do that, we're going to need to know 
we really have to have some way of measuring amount of overload and there's 
a standard function for this, which is called the jacquard in this, and the 
jacquard index is very simple I'll. Do it through example? Let's take this 
sofa okay.</p>

<p>So if we take this sofa and let's take the jacquard index of 
this sofa with this grid cell here all right, what we do is we find the 
area of their intersection. So here is the area of their intersection: 
okay, and then we find the area of their union. So here is the area of 
their unit, smart phone, here's, the area of their union, okay and then we 
say, take the intersection divided by the union. Okay, and so that's 
jacquard index, also known as iou intersection over all right so of two 
things overlap by more compared to their total sizes. Together they have a 
higher jacquard, alright, so we're going to go through and find the 
jacquard overlap for each one of these. Three objects versus each of these 
16 anchor boxes, and so that's going to give us a 3 by 16 matrix, but for 
every ground truth object and above every anchor box. How much overlap is 
there? So here are the coordinates of all of our anchor boxes. In this 
case, they're printed as Center and height width, and so here is the amount 
of overlap between and, as you can see, it's 3 by 16 right so for each of 
the three ground truth objects each of these 16 anchor boxes. How much do 
they overlap? Right, so you can see here: 0. 1. 2. 3. 4. 5. 6. 7. 8. The 8 
anchor box overlaps a little bit with the second ground. Truth object, 
okay, so what we could do now is we could take the max of dimension 1 
right. So the max of each row and that will tell us for each ground truth 
object. What's the maximum amount that overlaps with some grit zone - and 
it also tells us - remember pytorch when you say Mets, two things, it says 
what is the max and what is the index both amass? So, for each of these 
things, the 14th grid cell is the largest is the largest overlap for the 
first round truth: thirteen for the second and 11 okay.</p>

<p>So that tells us. 
You know a pretty good way of assigning each of these ground. Truth objects 
to a grid cell what what the matches is, which one is the highest overlap, 
but we're going to do a second thing. We're also going to look at max over 
dimensions, zero and Max over dimension. Zero is going to tell us what's 
the maximum amount of overlap for each grid cell, so across all of the 
ground. Truth objects right, and so particularly interesting here - tells 
us for every grid cell of sixteen. What's the index of the ground truth 
object which overlaps with it? The most zero is a bit overloaded here, zero 
could either mean the amount of overlap was zero or it could mean its 
largest overlap, is with object, index zero. It's going to turn out not to 
matter so there's a function called map to ground truth which I'm not going 
to worry about for now. It's it's super simple code, but it's slightly 
awkward to to think about, but basically what it does is it combines these 
two sets of overlaps in a way described in the SSD paper, to assign every 
anchor box to a ground truth object and basically the way of The signs that 
is each of these ones, each of these three gets assigned in this way right. 
So this one, this object is assigned to bound to anchor box 14 this one to 
13 and this one through 11 and then of the rest of the anchor boxes.</p>

<p>They 
get assigned to anything which they have an overlap of at least point 5 
with, if anything that doesn't, which isn't in either of those criteria, ie 
either isn't a maximum or doesn't have a greater than 0.5 overlap is 
considered to be a cell which contains background. Okay, so that's all the 
map to ground truth motion does, and so after we go through it, you can see 
now a list of all of the assignments, and you can also see anywhere that 
there's a zero here. It means it was assigned a background, in fact 
anywhere. It's less than point five here, give us a sign in the background, 
so you can see those three which kind of forced assignments that puts a 
high number in just to make sure all of their assigned all right. So we can 
now go ahead and convert those two classes, and then we can make sure we 
just grab those which are at least point five in size, and so finally, that 
allows us to spit out the three pluses that are being predicted. We can 
then put that back into the bounding boxes, and so here are what each of 
those bounding boxes is sorry, what each of those anchor boxes is meant to 
be predicting okay, so you can see sofa dining room table chair, which 
makes perfect sense. If we go back to here, this is meant to be predicting 
so far. This is over. This is meant to be predicting dining room table. 
This has been to be predicting chair and everything else has been to be 
predicting background. So that's the matching stage.</p>

<p>So once we've done the 
matching stage, we're basically done we can take the activations just grab 
those which which matched that's. What this positive indexes are subtract 
from those the ground. Truth bounding boxes just for those which matched 
positive ones, take the absolute value of the difference. Take the mean of 
that and that's not one loss and then for the classifications. We can just 
do a cross entropy and then before we can add them together. Okay, so 
that's the basic idea, there's a few, and so this is. This is what's going 
to happen right, we're going to end up with 16 recommended, you know 
predicted bounding boxes coming out. Most of them will be background, see 
all these ones that say BG, but from time to time. They'll say this is a 
cow. This is potted plant. This is par okay, if you're wondering like what 
does it predict in terms of the bounding box of background, the answer is a 
totally ignores it right. That's why we had this. Only positive indexes 
thing here right. So if it's background, there's no, you know sense of like 
where's the correct bounding box in background. That's totally meaningless, 
so the only ones where the bounding box makes sense out of all these are 
the ones that there are some important literal tweets. What is that the? 
How do we interpret the activations, and so the way we interpret the 
activations is defined here in activation, two bounding box, and so 
basically we grab the activations.</p>

<p>We stick them throughthe an and so 
remember. Fan is the same as sigmoid shape, except it's scaled to be 
between negative 1 and 1, but read zero okay. So it's basically a sigmoid 
function that goes between negative one and one, and so that forces it to 
be within that range and we then say: okay, let's grab the the actual 
position of the anchor boxes and we will move them around according to the 
value of The activations divided by two, so in other words each each each 
activate each edge, as predicted bounding box, can be moved by up to fifty 
percent of a grid size from where its default position is and ditto for its 
height and width. It can be up to twice as being all half as big as its 
default size. So so that's one thing is we have to convert the activations 
into some kind of way of scaling those default length of box positions. 
Another thing is, we don't actually use cross. Entropy we actually use 
binary cross entropy loss. Okay, so remember binary. Cross entropy loss is 
what we normally use for. Multi-Label classification like in the planet, 
amazon, satellite competition, each satellite image - you could have 
multiple things in it: okay, so if it's got multiple things in it, you 
can't use soft max because soft max kind of really encourages just one 
thing to have in our case, each Anchor box can only have one object 
associated with it, so it's it's not for that reason that we're avoiding 
song max it's something else, which is it's possible for an anchor box to 
have nothing associated with it.</p>

<p>So there'd be two ways to handle. That is 
this either at background. One would be to say you know what backgrounds 
just a class right. So let's use soft max right and just treat background 
as one of the classes that the soft max could could predict a lot of people 
have done it. This way, I don't like that, though, right, because that's a 
really hard thing to ask of your network, do it's basically to say: can you 
tell whether this grid cell doesn't have any of the 20 objects that I'm 
interested with a jacquard overlap of more than 0.5? Now that's a really 
hard thing to put into a single computation. On the other hand, what if we 
just had for each class, you know, is it a motorbike? Is it a bus? Is it a 
person, it's the bird? Is it a dining room table right and then it can 
check each of those would be no? No, no, no and it snowed all of them, and 
it's like. Oh it's background, all right, so that's that's the way. I'm 
doing it is it's not that we could have multiple, true labels, but we can 
have 0, and so that's what's going on here we take our target and we do a 
one hot, embedding with number of classes plus 1. So this stage we do have 
the idea of background, but then we remove the last problem. So the Batman 
columns now gone right, and so now this vectors either of all zeros. 
Basically meaning there's nothing here or it has at most one one. And so 
then we can use binary cross-entropy predictions with that.</p>

<p>That is a minor 
tweak right, but like it's the kind of minor tweak that I I want you to 
think about and understand, because it's a really like it makes a. It makes 
a really big difference in practice to your training and it's the kind of 
thing that you'll see a lot of papers talk about like often when there's 
some increment over some previous paper it'll. Be something like this that 
we somebody to realize this, like Oh trying to predict a background 
category using a soft mass, is really hard to do what if we use the binary 
cross-entropy instead, you know - and so it's kind of like if you 
understand what this is Doing and more importantly, why yeah that's a 
really good test of your understanding of the material, and, if you don't 
that's fine Brad just shows you. This is something that you need to. Let me 
go back and re-watch this part of the video and talk to some of your 
classmates and, if necessary, ask for the forum and sure you understand 
what are we doing? Okay? So that's what this that's! </p>

<h3>10. <a href="https://youtu.be/I-P363wSv0Q?t=1h9m30s">01:09:30</a></h3>

<ul style="list-style-type: square;">

<li><b> TQDM library: add a progress meter to your loops</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>What this binary cross, entropy loss loss function is doing so, basically, 
in this part of the code, we've got this custom loss function. We've got 
the thing that calculates the declared index. We've got the thing that 
converts activations to bounding balls. We've got the thing that does map 
to ground truth. Okay and that's it all that's left is the SSD loss 
function, so the SSD loss function. This is actually what we set yeah as 
they are crit, as our criterion is SSD, so what SSD lost class? Is it it 
loops through each image in the minivan and it calls SSD one loss, so 
Assisting loss for one image, so this function is really where it's all 
happening. This is calculating the SSD loss for one image, so we D 
structure our bounding box in class and basically, there's a what is this 
doing here? Actually, this is worth mentioning a lot of code you find out 
there on the Internet doesn't work with mini-batches. You know it only does 
like one thing at a time. So in this case we you know all this stuff is 
working. It's not exactly a mini batch at a time. It's on a whole bunch of 
ground, truth objects at a time, and the data loader is being fed a mini 
batch at a time to do or the convolutional layers, because we could have 
different numbers of ground. Truth objects in each image, but a tensor has 
to be the strict rectangular shape fastai automatically with zeros 
anything. That's not the same life thing I fairly recently added, but it's 
super handy.</p>

<p>Almost no other libraries do that, but that does mean that you 
then have to make sure that you get rid of those zeros right. So you can 
see here I'm checking to find all of the all of the non zeros and I'm only 
keeping those. This is just getting rid of any of the bounding boxes that 
are actually I'm just padding, yeah, okay, so get rid of the padding turn 
the activations bounding boxes do the jacquard doing about this is all the 
stuff we just went through it's on line by line Underneath right check that 
there's an overlap greater than something around 0.4 0.5 different papers 
use different values. For this find the things that match, but the class 
put the background class for those and then finally get the l1 loss for the 
localization part. Get the binary. Cross-Entropy boss, for the 
classification part, turn those two pieces and then finally add them 
together. So that's a lot going on and it might take a few watches of the 
video to in the code to fully understand it. But the basic idea now is that 
we now have the things we need. We have the data, we have the architecture 
and we have the loss function. So now we've got those three things. We can 
train my finder and train for a bit and we get down to 25 and the end. We 
can see how we went so. Obviously, this isn't quite what we want when we do 
practice we'd kind of remove the background ones or some threshold, but 
it's like it's on the right track.</p>

<p>There's a dog in the middle: let's go to 
0.34 there's a bird here in the middle of 0.94. You know something's 
working, okay, yeah, I've got a few concerns. I don't think it's. I don't 
see anything saying motorcycle here. It says bicycle, which is in great 
there's nothing for the potted plant. That's big enough, but that's not 
surprising, because all of our anchor boxes were small. They were 4x4 grid 
so to go from here to something that's going to be more accurate. What 
we're gon na do is to create way more and the buses okay. So there's a 
couple of ways: we can create quick question and I'm just getting lost in 
the fact that the anchor boxes in the bounding boxes are. How are they not 
the same? I must be missing something. Anchor boxes are the square, the 
fixed square grid cells. These are the anchor boxes they're in an exact, 
specific unmoving location. The bounding boxes are, these are three things: 
the bounding boxes, these 16 things at anchor boxes; okay, so we're going 
to create lots more anchor bosses. So there's three ways to do that and 
I've kind of drawn some of them printed. Some of them here one is to create 
anchor boxes of different sizes and orientations. So here you can see you 
know, there's a upright rectangle, there's a line down rectangle and 
there's a square. It's a question for the multi-label classification.</p>

<p>Why 
aren't we multiplying the categorical loss by a constant like we did 
before? That's a great question because later on it'll turn out, we don't 
need to so yeah. So you can see here like this square, and so I don't know 
if you can see this blue shield up. You basically got one two: three 
squares of different sizes and for each of those three squares. You've also 
got a line down rectangle and up rectangle. That's we've got three aspect 
ratios at three zoom levels. That's why don't you do? We can do this and 
this is for the one by one grid. So, in other words, if we added two more 
strata, convolutional layers, you eventually get to a one by one grid, and 
so this is for the one by one include. Another thing we could do is to use 
more compositional layers as sources of anchor boxes so as well as our and 
I've I've randomly gated these a little bit. So it's easy to see right so, 
as well as our 16 by 16 grid cells. These cells we've also got two by two 
grid cells, and we've also got the one by one itself, alright. So, in other 
words, if we add three straight hours prior to convolutions to the to the 
end, we'll have 4 by 4, 2 by 2 and 1 by 1 sets of grid cells, all of which 
have anchor boxes and then for every one of those we can Have all of these 
different shapes and sizes? Okay, so obviously those two combined with each 
other to create lots of anchor boxes and if I try to print that on the 
screen is just one big toss. So that's what this code is right.</p>

<p>It says all 
right. What are all the grid cell sizes? I have for the anchor boxes, what 
are all the zoom levels I have for the anchor boxes and what are all the 
</p>

<h3>11. <a href="https://youtu.be/I-P363wSv0Q?t=1h17m30s">01:17:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Fast Style Transfer w/ ‚ÄúSupplementary Material, Mar-2016‚Äù</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Aspect ratios, I have for the anchor boxes and the rest of this code then 
just goes away and creates the top left and bottom right corners inside 
anchor corner and the middle and height width in mecca's. So that's all 
this does and you can go through it and print out the anchor in a corner. 
So the key. The key is to remember this basic idea that we have a vector of 
ground truth, stuff right where that stuff is like sets of four bounding 
boxes. This is what we were given. It was in the JSON files that it's the 
ground truth. It's the dependent variable sets of four bounding boxes and 
for each one, also a class right. So this is a person in this location. 
This is a dog in this location. That's the ground! Truth that we've given 
yes, yeah exactly left X, Y bottom right, X Y. So that's what we printed 
here right, we print it out. This is what we call the ground truth, there's 
no model! This is what we're told is what we this is, what the answer is 
meant to be, and so remember any time we train an ear on that we have a 
dependent variable and then we have a neuro nap from blackbox neural net. 
That takes some input and spits out some output activations and we take 
those activations and we compare them to the ground truth. We calculate a 
loss. We find the derivative of that and adjust the weights according to 
the derivative times a learning rate. Okay, so the loss is calculated using 
a must function. Something I wanted to say is, I think them.</p>

<p>One of the 
challenges with this problem is part of. What's going on here is we're 
having to come up with an architecture. That's letting us predict this 
ground truth like it's, not because you can have you know any number of 
objects in your picture. It's not an you know immediately obvious, like oh, 
what's the correct architecture, that's gon na, let us predict that sort of 
ground truth. That's so I'm gon na kind of make this plain, as we saw when 
we looked at the kind of Yolo versus SSD, that, like there are only two 
possible architectures, the last layer is fully connected or the last layer 
is convolutional and both of them work perfectly. Well, I'm sorry, I meant, 
in terms of by creating this idea of anchor boxes and anchor boxes with 
different locations, locations and sizes. That's giving you a format that 
kind of lets you get to the activations you're right like high level. Is 
that you see okay? So that's that's really entirely in the loss function, 
not in the architecture like and if we use the Yolo architecture where we 
had a fully connected layer like literally there would be no concept of 
geometry at all right, so I would suggest, like kind of forgetting the 
Architecture and just like treated as just a given it's a theme that is 
spitting out 16 times 4 plus C activations right and then I would say our 
job is to figure out how to take those 16 times, 4 plus C activations and 
compare them to our Ground truth, which is like 4 plus it's 4 plus 1, but 
if it was one hot encoded, it would be C and I think that's easier to think 
about so call it 4 plus C times.</p>

<p>However, many ground truth objects. There 
are for that particular image. All right, so, let's pour that right, so we 
need a loss function that can take these two things and spit out a number 
that says how good are these activations that that's that's what we're 
trying to do so to do it. We need to take each one of these M ground. Truth 
objects and decide which set of 4 plus C activations is responsible for 
that object, which one should we could be comparing and saying like. Yes, 
the right class will not and yeah it's false or not, and so the way we do, 
that is basically to say. Okay, let's decide the first for the first 4 plus 
C activations are going to be responsible for predicting the bounding box 
of the thing. That's closest to the top left and the last 4 plus C you'll 
be predicting those the furthest to the bottom right and kind of everything 
in between. So this is this matching and then, of course, we're not using 
the yellow approach where we have a single vector. We're using the SSD 
approach, where we spit out a convolutional output, which means that it's 
it's not arbitrary as to which we mash up, but actually we want to match up 
the set of activations, whose receptive field most closely reflects. You 
know as the maximum density from where this real object is, but that's a 
that's a minor tweak. You know, I guess like that.</p>

<p>It that's the easy way 
to have taught this, but if we just start with the Yolo approach, where 
it's just like an arbitrary vector and we can decide which activations 
correspond to which agree on trees object as long as it's consistent, it's 
going to be a consistent Rule because like if, in the first image, the top 
left object, corresponds with the first four plus C activations and then 
the second image we threw things around and suddenly it's now going with 
the last four plus the activations. The neural net doesn't know what to 
learn about that. The neural net needs to like a loss function needs to be 
like some consistent task right, which in this case the consistent task is 
try to make these activations reflect the bounding box in this general 
area. That's basically what this loss function is trying to do. Is it 
purely coincident that you know the 4x4 in the kong-kong 2d is the same 
thing as no you're 16. Not at all coincidence. It's it's because, though, 
that 4x4 comp is going to give us activations whose receptive field 
corresponds to those locations in the infinite. So it's it's carefully 
designed now. Remember I told you before part two that, like the stuff we 
learn in part, two is going to assume that you are extremely comfortable 
with everything you learnt in part one and for a lot of you, you might be 
realizing now.</p>

<p>Maybe I wasn't quite as familiar with the starting part, one 
as I first thought and that's fun right, but just realize you might just 
have to go back and really think deeply. It's there more with understanding 
with life. What are the inputs and outputs to each layer and a 
convolutional Network? How big are they one of their rank? Is that V? How 
are they calculated so that you really fully understand the idea receptive 
field? What's a loss function really, how does back propagation work 
exactly like these things all need to be like deeply felt intuitions, but 
you only get through to practice and once they're all deeply felt 
intuitions, then you can we watch this video and you'll be like? Oh, I see 
okay, see that you know these activations just need some way of 
understanding what task they're being given, that is being done by the loss 
function and the loss function is encoding a task, and so the task of the 
SSD loss function is basically two Parts part one is figure out which 
ground truth object is closest to which grid cell, which anchor boss. When 
we, where we started doing this, the grid cells of the convolution and the 
anchor boxes were the same right. But now we're starting to introduce the 
idea that we can have multiple anchor boxes per grid cell okay, a little 
bit more complicated, so every ground truth object. We have to figure out 
which anchor boxes are closest to every anchor box.</p>

<p>We have to decide which 
ground truth object isn't responsible for if any and once we've done that 
matching it's trivial now we just basically go through and going back to 
the single object detection. It now is just this: it's once </p>

<h3>12. <a href="https://youtu.be/I-P363wSv0Q?t=1h27m45s">01:27:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Ugly artifacts like ‚Äúcheckerboard‚Äù: cause and fixes; Keras UpSampling2D</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>We've got every ground truth object mesh to anchor box to a set of 
activations. We can basically didn't say: okay, what's the cross, entropy 
loss of the categorical part, what's the r-1 loss? So really it's the 
matching part which is kind of I don't know kind of slightly surprising bit 
and then this idea of picking those in a way that the convolutional Network 
gives it the best opportunity to calculate that part of the space. Is then 
the final cherry on top and this um I'll tell you something else this class 
is by is by far, I think, going to be the most conceptually challenging and 
part of the reason for that is that, after this we're going to go and do 
some Different stuff and we're going to come back to it in lesson 14, and 
do it again with some tweaks, alright, we're gon na add in some of the new 
stuff we learned afterwards, so you're gon na get like a whole second round 
for this material. Once we add some some extra stuff at the end, so we kind 
of dinner revise it, as we normally do. Remember. In part 1, we kind of 
went through computer vision and RP structured data back to NLP back to 
computer vision. You know so we revised everything from this at the end. 
It'll be kind of similar yeah, so yeah, so don't worry if it's a bit 
challenging in fist you'll get there okay, so so for every grid cell there 
can be different sizes.</p>

<p>We can have different orientations and zooms 
representing different different anchor boxes, which are just like 
conceptual ideas that basically every one of these is associated with one 
set of four plus e activations in a model right. So, however, many of these 
ground truth boxes, we have. We need to have that x, 4 plus C activations 
in the model. Now that does not mean that each convolutional layer needs 
that many filters right because remember the 4x4 convolutional layer 
already has 16 sets of filters. The 2x2 accomplished layer already has four 
sets activations and then finally, the one by one has one set, so we 
basically get 1 plus 4 plus 16 for free. Just because that's how 
convolution works it that relates things at different locations. So we 
actually only need to know K where K is the number of zooms by the number 
of aspect ratios where else the grids we're gon na get for free through our 
architecture. So, let's check out that architecture, so the model is nearly 
identical to what we had before all right that we're gon na go we're going 
to have a number of strive. </p>

<h3>13. <a href="https://youtu.be/I-P363wSv0Q?t=1h31m20s">01:31:20</a></h3>

<ul style="list-style-type: square;">

<li><b> ImageNet Processing in parallel</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>To convolutions, which is going to take us through to 4 by 4, 2 by 2, 1 by 
1, you straight to convolution, perhaps our grid sizes, okay and then after 
we do our first convolution to get to 4 by 4, we're going to grab a set of 
Outputs from that, because we want to save away the 4 by 4, which anchors 
and then once we get to 2 by 2. We grab another set now two by two anchors 
and then finally, we get to 1 by 1 and we saw we get another set of the 
Ovilus right. So you can see. We've got like a whole bunch of these 
outcome. This first one we actually not easy. So at the end of that we can 
then concatenate torque at concatenate them all together. So we've got the 
four by four activations, the two by two activations, the one by one and 
visions. So that's going to give us the correct number of activations to 
give us one activation for every for every bounding, a periphery anchor 
boss. So then we just set our criterion as before to SSD loss and we go 
ahead and train right and the way we go so in this case, I'm just printing 
out those things with at least probability of point one, and you can see, 
we've got some things Booked: okay, some things: don't our big 
</p>

<h3>14. <a href="https://youtu.be/I-P363wSv0Q?t=1h33m15s">01:33:15</a></h3>

<ul style="list-style-type: square;">

<li><b> DeVISE research paper</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Objects like bird we've got a box here with a point: nine three 
probability: it's looking to be in about the rest, but our person is 
looking pretty hopeful, but our motorbike has nothing at all a lot of 
plants looking pretty horrible. That bus is all the wrong size. What's 
going on so, but what's going on here, will tell us a lot about the kind of 
history of object, detection, and so these five papers are key steps in the 
history recent modern history of our vegetation, and so they go back to a 
balance of maybe 2013, this paper called scalable object, detection using 
deep neural networks. This is what basically set everything up and when 
people refer to the multi box method, they're talking about this paper - 
and this was the basic one that came up with this idea - that we can have a 
loss function that has this matching process, and then you can Kind of use 
that to do object addition, so everything since that time has been trying 
to figure out basically how to make this better. So, in parallel, as a guy 
quarter, Oscar shik, who was going down a totally different direction, 
which was he had these two-stage processes? Where the first stage used like 
classical computer vision, approaches to like fine kind of edges and 
changes of gradients and stuff to kind of guess which parts of the image 
may represent us objects and then fit each of those into a convolutional 
neural network which was basically Designed to figure out is that actually 
the kind of object I'm interested in, and so this was the kind of our CNN 
and fast.</p>

<p>I see a hybrid of traditional computer vision and so what Russ 
and his team then did was they basically took this multi box. Ide the air 
and replaced the traditional non deep learning computer vision, part of 
their two-stage process with a confident, so they now have two conquers one 
comp net that basically sped out something like this, which we call these 
region proposals. You know all of the things that might be objects and then 
the second part was the same as his earlier work, because, basically 
something the top each of those fitted into a separate part net, which was 
designed to classify whether or not. That particular thing really isn't. 
Interesting at a similar time, these two papers came out, Yolo and SSD. Now 
both of these did something pretty cool, which is they got. The same kind 
of performance is faster, a CNN, but with once okay, and so they basically 
took the multi boss idea and they tried to figure out how to deal with this 
mess. It's done and the basic ideas were to use, for example, called hard- 
mining where they would like go through and find all of the matches that 
didn't. Look that good and further away has some very tricky and complex 
data. Augmentation methods all kinds of hackery basically, but they got it 
to work pretty pretty well, but then, through really cool happened. Late 
last year, which is this thing called focal versus paper philosophy, dense 
object attention. The network object is called written written where they 
actually realized.</p>

<p>Why? This messy crap wasn't working and I'll describe 
why dismissive fat loss by trying to describe why it is that we can't find 
the motive a so here's the thing when we look at this. We have three 
different granularities of convolutional route, four by four two by two one 
by one, the one by one, it's quite likely to have a reasonable overlap with 
some object, because most photos have sometimes main so. Okay, on the other 
hand, in the four </p>

<h3>15. <a href="https://youtu.be/I-P363wSv0Q?t=1h38m">01:38:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Digress: Tips on path setup for SSD vs. HD</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>By four those sixteen grid cells unlikely, most of them are not going to 
have a much of an overlap with anything like in this motor pipe cases guys. 
So, if somebody was to say to you like, you know, 20-buck bet. What do you 
reckon this little quick? Is you know and you're not sure you're gon na say 
background because most of the time in this background right and so here's 
the thing? Okay, I understand why we have a four by four grid of receptive 
fields with one anchor box each to coarsely. Localize objects in the image, 
but I think I'm missing is why we need multiple receptive fields at 
different sizes. The first version already included 16 receptive fields, 
each with a single anchor box associated with the addition. There are now 
many more anger boxes to consider. Is this because you constrain how much a 
receptive field could move or scale from its original size, or is there 
another reason it's kind of backwards? The reason I did the constraining 
was because I knew I was going to be adding more boxes later, but really 
the reason is that the jacquard overlap between one of those 4x4 grid 
cells, and you know, a picture, a single object that takes out most of the 
Image is never going to be 0.5 because, like the intersections much smaller 
than the Union, because the object is too big.</p>

<p>So for this general idea to 
work where we're saying, like you're responsible for something that you've 
got a better than 50 % overlap with, we need anchor boxes which, which 
will, on a regular basis, have a 50 % or higher overlap. Which means we 
need to have a variety of sizes and shapes and scales yeah. So this is this 
this this all happens. This all happens in the last match. Basically, the 
vast majority of the interesting stuff in all of the object detection stuff 
is the loss function, because there is only three things last function, 
architecture, so the this is the focal most paper for kalasa dense, object, 
detection from August 2017, his Ross Perik still doing This stuff coming 
her, you might recognize as being the the rezneck guy a bit of an all-star 
cast here and this. The key thing is this very first picture. The blue line 
is a picture of binary cross-entropy loss. The x-axis is, what is the 
probability or what is the activation? What is the probability of the 
ground truth class? So it's actually a motorbike. I said with point six 
chance: it's a motorbike or it's actually not a motorbike, and I said with 
plot point six chance. So this blue line represents the level of the value 
of cross-entropy loss, so you can draw this in Excel or Python or whatever 
a simple plot of cross-entropy loss. So the point is: if the answer is 
because remember, we do in binary custom to be lost. If the answer is not a 
motorbike - and I said yeah, I think it's not a motorbike.</p>

<p>I'm point six 
sure it's not a </p>

<h3>16. <a href="https://youtu.be/I-P363wSv0Q?t=1h42m">01:42:00</a></h3>

<ul style="list-style-type: square;">

<li><b> <code>words, vectors = zip(*w2v_list)</code></b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Motorbike this blue line is still it like a loss at about 0.5. It's it's 
it's! It's there's a lot of. It's still pretty bad right, so I actually 
have to keep getting more and more confident that it's not a motorbike. So 
if I want to get my loss down, then for all of these things, which are 
actually background, I have to be saying like I am sure that background you 
know or I'm sure it's not a motorbike or a bus or a person or a diagram 
there, Because if I don't say I'm sure it's not any of these things, then I 
still get plus. So that's why this doesn't work all right. This doesn't 
work because even when it gets to here and it wants to say, I think it's a 
motorbike - there's no payoff for it to say so, because if it's wrong right 
and it kick it, it gets killed and the vast majority of the time. It's not 
anything the vast majority of time its background, and even if it's not 
background, it's not enough just to say it's not background you're gon na 
say which of the 20 things. It is right. So for the really big things. It's 
fine because that's the one by one grid, you know so it's it generally is a 
thing, and you just have to figure out which thing it is where else for 
these small ones and generally it's not anything so generally small ones. 
We just prefer to be like I've got nothing to say no comment.</p>

<p>Okay, so 
that's why this is empty and that's why, even when we do have a bus right, 
it's using a really big grid cell to say it's a bus, because these are the 
only ones where it's like confident enough to make a call that something 
right, because The small grid cells it very rarely is something so the 
trick is to try and find a different loss function instead of binary cross, 
which we lost. It doesn't look like the blue line, but it looks more like 
the green or purple line and they actually end up suggesting the purple 
line, and so it turns out. This is cross-entropy loss. Negative log PT 
focal loss is simply 1 minus PT to the gamma, where gamma is some parameter 
right and they recommend using 2 times the cross entropy loss. That's it's 
literally just a scaling, and so that takes you to if you use camera, 
that's true that takes you to this purple line. So now, if we say yeah, I'm 
point 6 sure that it's not a motorbike than the loss function is way good 
for you. No worries, ok, so that's what we want to do. We want to replace 
cross entropy loss with vocalist, and I mentioned a couple of things about 
this fantastic paper. The first is like the actually contour. The actual 
contribution of this paper is to add Y minus P. To the gamma to the start 
of this equation, which sounds like nothing right, but actually people have 
been trying to figure out this down problem for years that and I'm not even 
sure that realize it's a problem.</p>

<p>There's just this assumption that you 
know object. Detection is really hard and you have to do all of these 
complex data, augmentations and have- mining and blah blah blah to get the 
damn thing to work. So a it's like this recognition of like. But why are we 
doing all those things? And then this realization is I'm like. Oh, if I do 
that it goes away, it's fixed alright. So when you come across a paper like 
this, which is like game-changing, you shouldn't assume you're gon na have 
to write 100 thousand lines of code. Very often is one line of code or the 
change of a single, constant or adding log to a single class. So, let's go 
down to the bit where it all happens where they describe personal loss, and 
I just wanted to point out a couple of terrific things about this paper. 
The first is here is their definition of cross-entropy and if you're not 
able to write cross-entropy on a piece of paper right now, then you need to 
go back and study it, because we're going to be assuming that you know what 
it is, what it means why It's that what the shape of it looks like 
cross-entropy appears everywhere: binary, cross-entropy and kind of 
variable, cross, entropy and the softmax that most people, most of the 
time, we'll see cross entropy written as like an indicator on y Times, log 
K plus an indicator on Y Minus y times y minus PE, this is like kind of 
awkward notation, often people. We use that Dirac Delta functions like 
that, or else this this paper just says.</p>

<p>You know what it's just a 
conditional. The cross, yet repeat, simply is a lot negative luck. P of Y 
is 1 negative, 1 1 minus P of us, so this is y is 1. If it's a motorbike 0 
or not in this paper, they say what, if it's about by 4 negative 1, and 
then they do something which mathematicians never do they refactor? All 
right check us out hey what, if we replace what, if we define a new term 
called PT, which is equal to the probability if Y is 1 or 1 minus P. 
Otherwise, if we did that, we could now redefined seee as that we're just 
super cool. Like it's such an obvious thing to do, but as soon as you do it 
all of the other equations get simpler as well because later on straight at 
the variants paragraph, they say, hey one way to deal with class in 
balance. Ie lots of stuff is background. Would just be to have a different 
weighting factor, the background. This is not so like for class one. You 
know we'll have some number alpha. + 4 + 0 will have 1 minus alpha, but 
then they're like hey, let's define alpha to you the same way and so now 
our cross entropy. Well, you know with a weighting factor like this, and so 
then they can wrap their focal loss with the same concept and then 
eventually they say, hey, let's take focal loss and combine it with class 
waiting like so yeah. So often when you see in a paper huge big equations, 
it's just because mathematicians don't know how to be back, though, and 
you'll see like the same pieces, are kind of repeated all over the place 
right very, very, very often, and by the time you've turned it Into non 
pile code suddenly is super simple, so this is a million times better than 
so great paper to read to understand how papers should be a terrible paper 
to read to understand what most his long rap okay.</p>

<p>So, let's try this we're 
going to use this yeah now remember: negative log P is </p>

<h3>17. <a href="https://youtu.be/I-P363wSv0Q?t=1h49m30s">01:49:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Resize images</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>The cross entropy loss. So therefore, this is just equal to some number 
times the cross entropy loss and when I defined the binomial cross, entropy 
loss. If you remember, or if you noticed, I had a weight which by default, 
was none right and when you call binary cross entropy Swaggart's pipe torch 
thing, you can optionally pass in the web. This is nothing that's more 
applied by everything and if it's none, then there's no work so since we're 
just going to multiply across entropy by something we can just define get 
away so here's the entirety. This is the thing that, like suddenly made 
object, detection business. Okay, so this was late last year. Suddenly it 
got rid of all of the complex messy pattern right and so sigmoid is that PT 
is W, and here you can see 1 minus P, 2 e to the power of gamma right. So 
we're gon na hammer of to our first point: 2. 5. If you're wondering why 
this paper, because they tried lots of different values of gamma and alpha, 
and they found that 2 and 0.25 work well. Okay, so there's our new loss 
function. It derives from our BC loss, adding a weight to it, focal loss 
other than that there's nothing else to do. We could just train our model 
again, okay, and so this time things are looking quite a bit better. You 
know we now have motor bike. Bicycle person motorbike like it's, it's 
actually having a go at finding something yeah, it's still doing a good job 
with big ones. In fact, it's looking quite a lot better.</p>

<p>It's finding quite 
a few people. It's finding a couple of different birds. It's looking pretty 
good right! So our last step is for now is to basically figure out how to 
pull out just the interesting stuff like. Let's take this barking, this 
sofa right. How do we pick out a dog and a sofa, and the answer is 
</p>

<h3>18. <a href="https://youtu.be/I-P363wSv0Q?t=1h52m15s">01:52:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Three ways to make an algorithm faster:</b></li>

<li><b>memory locality,</b></li>

<li><b>simd/vectorization,</b></li>

<li><b>parallel processing</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Incredibly simple: all we're going to do is we're going to be going to go 
through every pair of these bounding boxes and if they overlap by more than 
some amount say 0.5 using jacquard, and they both predict in the same 
class. We're going to assume that the same thing and we're just got to pick 
the one with the higher p-value, and we just keep doing that repeatedly. 
That's really boring code. I actually didn't write it myself. I copied it 
on somebody else's code, non maximum suppression. Yes, no reason 
particularly to go through it, but that's one of us okay, so we can now 
show the results of the non maximum suppression and yeah. Here's the sofa, 
here's, the dog yeah. Yes, the bird is the person this person cigarette 
looks like it's firework. This one it's like it's okay, but not great, like 
it's found a person and his bicycle of a person and his bicycle with his 
bicycle in the wrong place place. You know you can also see that, like some 
of these smaller things that lower p-values the top button, where the back 
of just point one sticks. This is same time of bus. So there's something 
still two things here right and the trick will be to use something called 
featured here and that's what we're going to do in less than 14. What I 
wanted to do in the last few minutes of class was to talk a little bit more 
about the papers and specifically to go back to the SSD paper.</p>

<p>Okay, so 
this is single-shot multipass detector, and when this came out I was very 
excited because it was kind of you know it and Yolo were like. You know. 
The first kind of single pass good quality object, detection methods that 
come along, and so I kind of ignored, object, detection until this time or 
this to pass stuff with and then fast, our CNN and faster us. You know, 
because there's been as kind of continuous repetition of history and the 
deep learning world, which is things that involve multiple passes of 
multiple different pieces over time. You know, particularly where they 
involve some long deep learning pieces like over time. They basically 
always get turned into a single end-to-end deep learning model, so I tend 
to kind of like ignore them until that happens, because that's the point 
where it's like okay, now, people have figured out how to show this as a 
deep learning problem as soon as People do that they generally end up 
something it's much faster and much more accurate and so SSD and Yolo were 
really important. So here's the SSD paper, let's go down to the key piece, 
which is where they describe the model and let's try and understand it. So 
the model is basically one two three four paragraphs right, so tape is a 
really concise mat, which means that you kind of need to read them pretty 
carefully. Partly though you need to know which bits to read carefully so 
the bits where they say here, we're going to prove the error bounds on this 
model, you could ignore that right because you don't care about proving the 
intervals. But the bit which says here is what the model is careful, so 
here's the bit called model, and so hopefully you'll find.</p>

<p>We can now read 
this together and understand it. So SSD is a feed-forward content and it 
creates a fixed size, collection of bounding boxes and scores for the 
presence of object class instances in those boxes so fixed size that ie the 
the convolutional great time. Okay, you know different aspect ratios and 
stuff and each one of those has 4 plus C activations, followed by a non 
maximum suppression step to take that massive gump and turn it into. You 
know just a couple of novel adding different objects. The early layer is 
based on the standard architecture, so we just use rest man. This is pretty 
standard. As you know, you can kind of see this consistent theme, 
particularly in kind of how the pasta you know library tries to do things 
which is like grab a pre trade network. That already does something pull 
off the envious, decant, a new enemy, all right, so early network players. 
We use the standard, classifier truncate, the classification layers, as 
we've always do that automatically when we use Carmona - and we call this 
the base Network. Some campers call that the backbone and we then add an 
auxilary structure. Okay, so the auxiliary structure, which we call the 
custom head, has multi scale feature nests. So we add compositional layers 
to the end of this base Network and they decrease in size progressively. So 
a bunch of side to convice, so that allows predictions of detections and 
multiple scales. The grid cells are different size. It is.</p>

<p>The model is 
different for each feature: layer compared to Yolo that operate on a single 
feature, so you're low, as we discussed dispenser, is one vector for us. We 
have different harmless H, added feature layer gives you a fixed set of 
predictions using a bunch of filters for a filter layer where the grid size 
is n by n 4x4 with pay channels where, in fact, let's take the pick as 
well. Seven by seven with 512 panels, the basic element is going to be a 
three by three by P cone, which, in our case is a three by three by four 
for the shape offset bit or three by three by C for the score for category 
vision. That's so those are those three. Those are those two pieces at each 
of those grid cell locations. It's going to produce an output value and the 
bounding box offsets measured relative to that default. Cost position which 
we've been calling an anchor box position relative to the feature, mount 
location. What we've been calling the grid cell? Okay as opposed to Yolo 
right, which has a fully connected layer? And then they go on to describe 
the default boxes, what they are for? Each feature now cell organs in grid 
cell, they Tyrell the picture map in a convolutional manner, so the 
position of each box relative to its grid cell is best. So hopefully you 
can see you know we end up with C plus 4 times K filters.</p>

<p>If there are K 
boxes at each location, okay, so these are similar to the anchor bosses so 
like. If you jump straight in and read a paper like this, without knowing 
like only solving and Maya solving, yet when what's the kind of man acted 
shirt circle, those four paragraphs would probably make almost no sense, 
but now that we've gone through it, you read those four Paragraphs and 
hopefully you're thinking - oh that's just what Jeremy said only they said 
it better than Jerry and less words. Okay. So so I have the same problem 
when I started reading the SST paper and I read those four paragraphs - and 
I didn't have before this time much of a background in object notation, 
because I decided to wait until she passed me more, and so I read this And 
I was like what the hell right and so the trick is to then start reading 
back over the citation right. So, for example - and you should go back and 
read this paper now look here's the matching strategy but and that all 
matching strategy that I somehow spent like it now we're talking about. 
That's just a paragraph, but it really is right for each ground truth. We 
select from default boxes based on location aspect, ratio and scale. We 
match each ground truth to the default box, with the best jacquard overlap, 
and then we met two default boxes, anything with jacquard over that I own 
printer. That's it that's the one sentence version and then we've got the 
loss function, which is basically to say, take the average of the last 
based on classes, plus the lost based on localization, with some weighting 
factor now with focal loss I found.</p>

<p>I didn't really need the weighting 
factor anymore, they both, but in this case, as I started reading this, I 
didn't really understand exactly what LMG and all this stuff was, but it 
says well, this is derived from the more t-bar subjective. So then, I went 
back to the paper that defined Modi bas and I found in their proposed 
approach. They've also got a section called training objective, also known 
as loss function, and here I can see it's the same. Notation l, GE, and so 
this is where I can go back and see the detail and after you read a bunch 
of papers, you'll start to see things very quickly. For example, when you 
see these double bars, you'll realize every time there's mean squared 
error. That's how you're right mean square error right. This is actually 
called the two norm. The two long is just the sum of squared differences 
right and then there's two up here means normally they take the square 
root. So we just don't do this, so this is just M SC. All right anytime! 
You see like. Oh, yes, a lot of C and his see. You know that's basically, a 
binary cross-entropy right, so it's like you, you're, not actually gon na, 
have to read every day at every equation. Right. You are kind of a bit at 
first right, but after a while, your brain just like immediately knows 
basically what's going on, and then I say: oh I've got a Luxio panel of 1 
minus C and as expected I should have my X and here's. My 1 minus X, okay, 
there's all the pieces there that I would expect to see.</p>

<p>So then, having 
done that, that then kind of allowed me okay and then they get combined 
with the two pieces and oh there's the multiplier that I expected, and so 
now I can kind of come back here understand what's going on, okay, so we're 
going to be Looking at a lot more papers right, but maybe this week go 
through the code and go through the paper all right and be like. What's 
what's going on and remember what I did to make it easier for you was I 
took that loss function. I copied it into a cell and it has lit it up so 
that each bit was in a separate cell and then, after every cell I other 
printed or plotted that value that. So, if I hadn't have done that for you, 
you should do it yourself right. Like if there's no way, you can understand 
these functions without trying putting things in single time, yeah, okay, 
so hopefully this is kind of a good good start. Okay! Well thanks! 
Everybody have a great week, see you next Monday, </p>






  </body>
</html>
