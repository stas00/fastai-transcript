<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 10: NLP Classification and Translation</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://course.fast.ai/">fast.ai: Deep Learning Part 2 (v2) (2018)</a></p>
  <h1><a href="http://course.fast.ai/lessons/lesson10.html">Lesson 10: NLP Classification and Translation</a></h1>
  <h2>Outline</h2>
<p>After reviewing what we‚Äôve learned about object detection, today we jump into NLP, starting with an introduction to the new fastai.text library. This is a replacement for torchtext which is faster and more flexible in many situations. A lot of today‚Äôs class will be very familiar‚Äîwe‚Äôre covering a lot of the same ground as lesson 4. But today‚Äôs lesson will show you how to get much more accurate results, by using transfer learning for NLP.</p>

<p>Transfer learning has revolutionized computer vision, but until now it largely has failed to make much of an impact in NLP (and to some extent has been simply ignored). In this class we‚Äôll show how pre-training a full language model can greatly surpass previous approaches based on simple word vectors. We‚Äôll use this language model to show a new state of the art result in text classification.</p>

  <h2>Video Timelines and Transcript</h2>

<h3>1. <a href="https://youtu.be/uv0gmrXSXVg?t=16s">00:00:10</a></h3>

<ul style="list-style-type: square;">

<li><b> Picking an optimizer for Style Transfer (student post on Medium)</b></li>

<li><b>Plus other student posts and tips on class project.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>So welcome to lesson 10, or is somebody on the forum described at lesson 
10, not seven, which is probably a clearer way to think about this, we're 
going to be talking about NLP before we do. Let's do a quick review of last 
week because last week you know, there's quite a few people who have kind 
of flown here to San Francisco for this inverse. Of course, I'm seeing them 
pretty much every day, they're working full time on this, and quite a few 
of them are still struggling to understand the material from last week 
right. So, if you're, finding it difficult, that's that's fine. One of the 
reasons I kind of put it up there up front is so that we've got something 
to cogitate about and think about gradually work towards for the by lesson. 
14. Mod 7. Your you'll get a second crack at it. Okay, but it's it's world 
work that there's so many pieces, and so hopefully you can keep developing 
a better understanding to understand the pieces. You'll need to understand. 
You know the shapes of convolutional layer, outputs and receptive fields 
and loss, functions and and everything right. So it's all stuff that you're 
going to understand need to understand for all of your deep learning 
studies anyway right, so everything you do to develop an understanding of 
last week's lesson is going to help you everything else.</p>

<p>So one key thing I 
wanted to mention is we started out with something which is really pretty 
simple, which is single person, classifier, a single object, class player, 
single object, bounding box without a classifier, and then single object, 
classifier and darling box, and anybody who's hopefully spent Some time 
studying since lesson 8 mod seven has got to the point where they 
understand this bit right now. The reason I mention this is because the bit 
where we go to multiple objects is actually almost identical to this, 
except we first have to solve the matching problem. We ended up creating 
far more activations than we need for our number of bounding boxes. Ground 
truth, bounding boxes, and so we match each ground. Truth object to a 
subset of those activations right and once we've done that the loss 
function that we then do to which matched pair is almost identical to this 
loss function right. So if you're feeling stuck go back to lesson eight 
right and make sure you understand the data set, the data loader and, most 
importantly, the loss function from the end of lesson, eight or the start 
of lesson: nine: okay, okay! So once we've got this thing, which can 
predict the class and bounding box for one object, we went to multiple 
objects by just creating more activations. We had to then deal with the 
matching problem.</p>

<p>Having done with the dealt with a matching problem, we 
then basically moved each of those anchor boxes in and out a little bit and 
around a little bit, so they tried to line up with a particular ground. 
Truth objects and we talked about how we took advantage of the 
convolutional nature of the network to to try to have activations that had 
a receptive field that was similar to the ground truth object. We were 
predicting and khloÈ sultan provided this fantastic picture. I guess for 
her own notes, but she shared it with everybody which is lovely to talk 
about like what is SSD mode. He had to forward do line by line, and I 
apparently wanted to show this to help you with your revision. But I also 
can't you wanted to show through this show this to kind of say doing this 
kind of stuff is very useful for you to do like walk through and in 
whatever way, helps you make sure you understand something, and you can see 
what Chloe's done Here is she's focused, particularly on the dimensions of 
the tensor at each point, and the in the in the path has be kind of 
gradually down sampling, using these tragic convolutions making sure she 
understands why those grid sizes happen and then understanding how the 
outputs come out Of those okay, and so one thing you might be wondering, is 
well how did Chloe calculate these numbers? So I don't know the answer I 
have spoken to her, but obviously one approach would be like from first 
principles just thinking through it, but then you want to know what am i 
right right, and so this is where you've got to remember this pdb dot set 
Trace idea right so I just went in just before class and went into SSD 
multi-head dot forward and entered pdb dot set trace, and then I ran a 
single batch right and so at that.</p>

<p>So I put the trace at the end and then I 
could just print out the size of all of these guys right so which, by the 
way, reminds me last week. There may have been a point where I said: 21 
plus 4 equals 26, which is not true in most universes and like by the way 
when I code I do that stuff. Like that's the kind of thing I do all the 
time. So that's why we have debuggers and know how to check things and do 
things in small little bits along the way anyway. So this idea of putting a 
debugger inside your forward function and printing out the sizes is 
something which is damn super or you could just put a print statement here 
as well. So I actually don't know if that's how Khloe figured it out, but 
that's how I would if I was her and then we talked about increasing K, 
which is the number of anchor boxes for each convolutional grid cell, which 
we can do with different zooms and different Aspect ratios, and so that 
gives us a plethora of activations and therefore predicted bounding boxes, 
which we then went down to a small number using non maximum suppression and 
I'll. Try and remember, to put a link. There's a really interesting paper 
that one of our students told me about that. I hadn't heard about which is 
attempting to like you know: I've mentioned on maximum suppression. It's 
like kind of hacky kind of leads totally heuristic.</p>

<p>You know didn't even 
talk about the code because it seems kind of hideous, so somebody actually 
came up with a paper recently attempts to do an end-to-end ComNet to 
replace that NMS piece. So I'll I'll put that paper up nobody's created a 
torch, a pipe torch implementation yeah, so be an interesting project. If 
anyone wanted to try that um one thing, I've noticed in our study groups 
during the week is not enough people reading papers, the the what we are 
doing in class now is implementing papers. The papers are the real ground. 
Truth right - and I think you know from talking to people a lot of the 
reason people aren't reading papers is because a lot of people don't think 
they're capable of reading papers. They don't think they're the kind of 
people that read papers, but you are you're here right and like we started 
looking at a paper last week and we read the words that were in English and 
we largely understood them right. So it's like if </p>

<h3>2. <a href="https://youtu.be/uv0gmrXSXVg?t=7m30s">00:07:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Use Excel to understand Deep Learning concepts</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>You actually look through this picture from SSD carefully you'll realize 
SSD multi-head, but forward is not doing the same as this and then you 
might think. Oh well, I wonder if this is better, you know - and my answer 
is probably right, because that's his team, all he had up forward, was like 
the first thing. I tried just to get something out there. You know, but you 
know that there are between this and the yellow vert and three paper and 
stuff they're, probably much better ways. One thing you'll notice in 
particular: is they use a smaller K, but they have a lot more sets of grids 
one by one three by three: five by five, ten by ten 19 by 19 and 38 by 38, 
8700 per class right. So a lot more, then, the we hats, they've been 
interesting thing to experiment with. Another thing I noticed is that where 
else we had four by four two by two one by one, which means there's a lot 
of overlap like every set fits within every other set in this case, where 
you've got one three: five: there's that you don't have that Overlap right, 
so it might actually make it easier to learn so there's lots of interesting 
things you can play with based on stuff. That's out, you know either trying 
to make it closer to the paper or think about other things. You could try 
that aren't in the paper or whatever. Perhaps most important thing I would 
recommend, is to put the code and the equations next to each other.</p>

<p>Yes, 
Rachel. There was a question of whether you could speak about the used 
cyclic learning rate argument and the fit function we broke it there so put 
the code and the equations from the paper next to each other and draw in 
one of </p>

<h3>3. <a href="https://youtu.be/uv0gmrXSXVg?t=9m20s">00:09:20</a></h3>

<ul style="list-style-type: square;">

<li><b> ImageNet Processing (continued from Lesson 9)</b></li>

<li><b>&amp; Tips to speed up your model (simd &amp; parallel processing)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Two groups: you are either a code person like me who's, not that happy 
about math, in which case I start with a code, and then I look at the math 
and I learn about how the math master the code and end up eventually 
understanding the math. All your PhD in stochastic differential equations 
like Rachel, whatever that means, in which case you can look at the math 
and then learn about how the code implements the math right then either way 
unless you're, one of those rare people who is equally comfortable in 
either world You'll learn about one or the other. Now learning about code 
is pretty easy because there's documentation and we know how to it's 
indexed up and so forth. Sometimes learning the math is hard because the 
notation might seem how to look up, but there's actually a lot of 
resources. For example, list of mathematical symbols on Wikipedia is 
amazingly great. It has examples of the explanations of what they mean and 
tells you what to search for to find out more about it, really terrific, 
and if you, google, for math notation cheat sheet you'll find more of these 
kinds of terrific resources. Ok, so over time you do need to learn the 
notation, but as you'll see from the Wikipedia page, there's not actually 
that much division right, obviously, there's a lot of concepts behind it, 
but once you know the notation, you can then quickly look at the concept as 
It pertains to a particular thing: you're studying nobody learns all of 
math and then starts learning machine learning right, everybody, even top 
researchers. I know when they're reading a new paper will very often come 
to bits of math they haven't seen before, and they'll have to go away and 
what that that bit of math.</p>

<p>Another thing you should try doing is to 
recreate things that you see in the papers. Right so here was the key most 
important figure, one from the focal loss paper, the retina paper so 
recreate it right and like very often I put these challenges up on the 
forums right so like keep an eye on the lessons register either put on the 
forums And so I put this challenge up there and within about three three 
minutes, serrata had said done it in Microsoft, Excel naturally along with 
actually a lot more information within the original paper. The nice thing 
here is that she was actually able to draw a line showing at a point, five 
ground, truth probability. What's the loss for different amounts of gamma, 
I'm just kind of cool, and if you want to cheat she's, also provided Python 
code to, I did discover a reminder bug in my code last week. The way that I 
was flattening out, the convolutional activations did not line up with how 
I was using them in the loss function and fixing that actually made it 
quite a bit better. So my motorbikes and cows and stuff we're actually in 
the right place. So when you go back to that, notebook you'll see it's a 
little less bad than it was okay. So there's some there's some quick 
coverage of what's gone before yes, quick question: are you gon na put the 
PowerPoint on github I'll, put a subset of it on? Okay and then, secondly, 
usually when we down sample, we increase the number of filters or depth 
when we're doing sampling from 77 to 44. Why are we decreasing the number 
from 512 to 256? Why not decrease dimension and SSD head? Is it 
performance-related? 77? Well, seven by seven, two four by four: I guess 
they've got star, it's it's it's because well my treats because that's kind 
of what the papers tend to do is is we've got a number of well.</p>

<p>We have a 
number of our paths and we kind of want each one to be the same, so we 
don't want each one to have a different number of filters, and also this is 
what the the papers did. So I was trying to match up with that and having 
these 256 it's it's a different concept because we're taking advantage of 
not just the last layer but the layers before that as well life's easier if 
we make them more consistent, okay, so we're now going to Move to NLP, and 
so let me kind of lay out where we're going here. We we've seen a couple of 
times now this idea of lab taking a pre trained model. In fact, we've seen 
it in every lesson: take a pre trained model whip off some stuff. On the 
top, replace it with some new stuff, get it to do something similar right, 
and so what we're going to do so, and so we've kind of dived in a little 
bit deeper to that to say, like okay, with convolute a dot pre-trained. It 
had like a standard way of like sticking stuff on the top, which does a 
particular thing which, with some classification, okay and then we learn. 
Actually we have. We can stick any pytorch module. We like on the end and 
have it do anything we like with a in a custom here and so suddenly you 
discover wow, there's, there's some really interesting things we can do in 
fact. That reminds me reminds me, young Lou, said well what, if we did a 
different kind of custom in here, and so the different custom hit was well, 
let's take the original pictures and rotate them and then make our 
dependent variable the op.</p>

<p>You know the opposite of that rotation, 
basically and see if it can learn to unrotated, and this is like a super 
useful thing. Obviously, in fact, I think Google photos nowadays has this 
option that it'll actually automatically rotate your photos for you, but 
the cool thing is, as young lord shows here. You can build that Network 
right now by doing exactly the same as our previous lesson, but your custom 
had is one that spits out a single number, which is how much to rotate by 
and your data set, has a dependent variable, which is how much did you 
Rotate, though yeah so like, you suddenly realized with this idea of a 
backbone plus a custom head, you can do almost anything you can think 
about. So today we're going to look at the same idea and say like okay: 
well, how does that apply to NLP and then in the next lesson, we're going 
to go further and say like well, if NLP in computer vision kind of lets, 
you do the same basic Ideas, how do we combine the two and we're going to 
learn about a model that can actually learn to find word structures from 
images or images from word structures or images from images, and that will 
form the basis if you wanted to go further of doing things Like going from 
an image to a sentence, that's called image captioning for going from a 
sentence to an image which will kind of have to do a phrased image and so 
from there.</p>

<p>You know we've got to go deeper, then into computer vision to 
think like okay, what other kinds of things we can do with this idea of a 
pre-trained network plus a custom head, and so we'll look at various kinds 
of image enhancement like increasing the resolution of A low-res photo to 
guess what was missing or adding artistic filters on top of photos or 
changing photos of horses into photos of zebras and stuff. Like that and 
then finally, that's gon na bring us all the way back to bounding boxes 
again and so to get there. We've got a first of all that about 
segmentation, which is not just learnt figuring out, where a bounding 
boxes, but figuring out what every single pixel in an image is part of. So 
this pixel is part of a person. This pixel is part of a car and then we're 
going to use that idea, particularly an idea court unit. It turns out that 
this idea from unit we can apply to bounding boxes where it's called 
feature pyramids. Everything has to have a different name in every slightly 
different area and we'll use that to hopefully get some better, better 
results and really good results. Actually it bounding boxes, so that's kind 
of our path from here um. So it's all gon na kind of build on each other 
that take us into lots of different areas. Now for NLP last part, we relied 
on a pretty great library called torch text, but as pretty great as it was, 
I've since then found the limitations of it too problematic to keep using 
it. As a lot of you complained on the forums, it's pretty damn slow, partly 
that that's because it's doing parallel not doing parallel processing and 
partly it's because it doesn't remember what you did last time and it does 
it all over again from scratch.</p>

<p>And then it's kind of hard to do fairly, 
simple things like a lot of you were trying to get into the toxic comment: 
competition on cattle, which was a multi-label problem and trying to do 
that with torch text. I eventually got it working, but it took me like a 
week of hacking away, which is kind of ridiculous. So to fix all these 
problems, we've created a new library called fastai blood test. fastai dot 
text is a replacement for the combination of torch text and fastai, NLP 
yeah. So don't use fast. I don't NLP anymore. Okay, that's like that's 
obsolete. It's! It's slower! It's more confusing! It's less good in every 
way. Right but there's a lot of overlaps. Okay, like intentionally a lot of 
the classes, have the same names. All of the functions have the same names, 
but this is the non torch text: okay, okay, so we're going to work with 
IMDB again, alright. So, for those of you have forgotten, go back and check 
out lesson 4, that basically this is a data set of movie reviews and you 
remember, we used it to find out whether we might enjoy some be getting or 
not, and we thought probably my kind of Thing so we're going to use the 
same data set and by default it it calls itself a co IMDB. So this is just 
that the raw data set that you can download and, as you can see, I'm doing 
from fastai dot text, import, star, there's no torch text and I'm not using 
faster. I don't an LP I'm going to use path Lea, but, as per usual, we're 
going to learn about what these tags are later.</p>

<p>So you might remember, the 
basic paths for NLP is that we have to take sentences and turn them into 
numbers and there's a couple of steps to get there. Okay, so at the moment, 
somewhat intentionally fastai dot text doesn't provide that many helper 
functions. It's really designed more to let you handle things in a fairly 
flexible way right. So, as you can see here, I wrote something called get 
texts which goes each thing in classes, and these are the three classes 
that they have in IMDB negative, positive and then there's another folder 
unsupervised that stuff they haven't gotten around to labeling. Yet so I'm 
just going to call that class now, and so I just go through each one of 
those classes, and then I just find every file in that folder with that 
name, and I open it up and read it and check it into the end of This all 
right, okay and as you can see with path lab, it's super easy to grab stuff 
and pull it in, and then the label is just whatever class or not so right. 
So I'll go ahead and I'll do that for the Train bit and I'll. Do that for 
the test fit so there's 70,000 and Tre and 25,000 in test talk about 
50,000. If the Train ones are unsupervised, we won't actually be able to 
use them when we get to the classification piece okay, so I actually find 
this much easier than the kind of torch text approach of having lots of 
layers and wrappers and stuff, because in the end, Reading text files is 
not it's not that hard.</p>

<p>Okay, one thing, that's always good idea is to sort 
things randomly. It's useful to know this simple trick for sorting things 
randomly, particularly when you've got multiple things. You have to sort 
the same way in this case. You've got labels and texts, NP, dot, random 
block permutation. If you give it an integer, it gives you a random list 
from 0 up to and not including the number you give it in some random order, 
and so you can then just pass that in as a indexer right to give you a 
list. That's sorted in that random order, so in this case it's going to 
sort train texts and train labels in the same random way. Okay, so that's a 
useful little idiom to use. So now I've got my texts and my labels sorted. 
I can go ahead and create a data frame from them. Why am I doing this? 
Well, the reason I'm doing this is because there is a somewhat standard 
approach, starting to appear for text classification datasets, which is to 
have your training set as a CSV file, with the with the labels first and 
the text of the NLP documents. Second, you know trained on CSV and a test 
or CSB, so it basically looks like this. You have your labels and your 
texts and then a file called classes text which just lists the classes. I 
think somewhat standard is just in a reasonably recent academic paper. Yarn 
lakorn and a team of researchers looked at quite a few data sets and they 
use this format for all of them, and so that's what I've started using as 
well for my recent paper.</p>

<p>So what I've done is you'll find that this 
notebook, if you put your data into this format, the whole notebook will 
work every time and so far than having a thousand different classes or 
formats and readers and writers or whatever have you said. Let's just pick 
a standard format and your job, your coders, you can do it perfectly well - 
is to put it in that format, which is the CSV file. Okay, the CSV files 
have no header okay by default, all right now, you'll notice. At the start 
here that I had two different paths: one was the classification path. One 
was the language model of path in NLP, you'll, see LM all the time. L M 
means language model, so the classification path is going to contain the 
information that we're going to use to create a sentiment analysis model. 
The language model path is going to contain the information we need to 
create a language model, so they're a little bit different. One thing 
that's different is that when we create the train, dot CSV in the 
classification path, we move everything that has a label of two, because 
label of two is unsupervised okay, so we remove the unsupervised data from 
the classifier. We cannot use it so </p>

<h3>4. <a href="https://youtu.be/uv0gmrXSXVg?t=26m45s">00:26:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Adding Preprocessing to Keras ResNet50</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>That means that this is going to have actually 25,000 positive, 25,000 
negative, and the second difference is the labels for the classification 
path. The labels are the actual labels, but for the language model there 
are no labels, so we just use a bunch of zeros and that just makes it a 
little bit easier because we can use a consistent data frame format or CSV 
format. Okay, now the language model we can create our own validation set 
and so you've probably come across by now. scikit-learn but model 
selection, dot, train test split, which is a really simple, little function 
that grabs a data set and randomly splits it into a training set and the 
validation set according to whatever proportion you specify, and so in this 
case I concatenate my classification, training And validation together, so 
that's got to be a hundred thousand altogether split it by ten percent, and 
so now I've got 90,000 training. Ten thousand validation for my language 
model, so I'll go ahead and save that. So that's my basic, you knowget the 
data in a standard format for my language model in my classifier right. So 
the next thing we need to do is tokenization, so tokenization means at this 
stage we've got for a document for a movie review. We've got a big long 
string and we wanted to pretend it into a list of tokens which are kind of 
a list of words, but not quite right.</p>

<p>For example, don't we want to be 
door? Hmm, you probably want full stop to be a token and so forth. 
</p>

<h3>5. <a href="https://youtu.be/uv0gmrXSXVg?t=28m30s">00:28:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Transfer Learning with ResNet in Keras: difficulty #1</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Right, so tokenization is something that we passed off to a terrific 
library called spacey, partly terrific, because in australian wrote it and 
partly terrific, because it's good at what it does we put a bit of stuff on 
top of Spacey, but the vast majority, the works being Done by Spacey before 
we pass it to Spacey I've written this simple fix-up function, which is 
basically each time I don't put a different data set and I've looked at 
about a dozen in building this. Everyone had different, weird things that 
need to be replaced. So here all the ones I've come up with so far. 
Hopefully this will help you out as well, so I HTML and escape all the 
entities and then there's a bunch more things. I replace have a look at the 
result of running this on text that you put in and make sure there's not 
more weird tokens in there. It's amazing how many weird things people do to 
test. So basically, I've got this function called get all which is going to 
go ahead and call get texts and texts is going to go ahead and do a few 
things, one of which is to apply that fix up that we just mentioned. So, 
let's kind of look through this because there's some interesting things to 
point out, so I got to use pandas to open our train CSV from the language 
model path, but I'm passing in an extra parameter.</p>

<p>You may not have seen 
before called chat, sighs, Python and pandas can both be pretty inefficient 
when it comes to storing and using text data, and so you'll see that very 
few people in NLP are working with large corpuses, and I think part of the 
reason is That traditional tools have just made it really difficult. You 
ran out of memory all the time, so this process, I'm showing you today, I 
have used on corpuses of over a billion words successfully using this exact 
code right and so one of the simple tricks is to use this thing called 
Chuck size with pandas. What that means is that pandas to not return a data 
frame, but it returns an iterator that we can iterate through chunks of a 
data frame, and so that's why, I don't say: truck train equals, get texts 
because here is a finger texts, but instead I call Get all which loops 
through the data frame, but actually what it's really doing, is its looping 
through chunks with the data frame. So each of those chunks is basically a 
data frame representing a subset of the data. When I'm working with NLP 
data many times I come across data with foreign text or characters, is it 
better to discard them or keep them? No, no definitely keep them, and this 
whole process is is Unicode and I've actually used this on Chinese text. 
This is designed to work on pretty much anything yeah yeah in general. Most 
of the time.</p>

<p>It's not a good idea to remove anything like old-fashioned. 
Nlp approaches tend to do all this like limit ization and all these kind of 
normalization steps to kind of get rid of. You know that lower case 
everything blah blah blah, but you know that's throwing away information 
which you don't know ahead of time, whether it's useful or not. So don't 
throw away information. Ok, so we go through each chunk, each of which is a 
data frame, and we call get texts. Get texts is going to grab the labels 
and makes them into ants it's going to grab. Then the the texts and I'll 
point out a couple of things. The first is that before we include the text, 
we have this beginning of stream - token, which you might remember we used 
way back up here, there's nothing special about these particular strings of 
letters they're. Just once I figured don't appear in normal text, it's very 
often, so every text is going to start with X POS now. Why is that? Because 
it's often really useful for your model to know when a new text is 
starting, for example, if it's a language model right, we are going to 
concatenate all the text together and so it'd be really helpful for it to 
know all this articles finished and a New one started, so I should probably 
like forget some of their context. Yeah Devo is quite often, texts have 
multiple fields like a title and abstract and then the main document, and 
so by the same token, I've got this thing here, which alerts us actually 
have multiple fields in our CSV okay.</p>

<p>So this really, this process is 
designed to be </p>

<h3>6. <a href="https://youtu.be/uv0gmrXSXVg?t=33m40s">00:33:40</a></h3>

<ul style="list-style-type: square;">

<li><b> Transfer Learning with ResNet in Keras: difficulty #2</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Very flexible and again at the start of each one we put a special field, 
starts here token, followed by the number of the field. That's starting 
here for as many fields as we have okay, then we apply our fix up to it and 
then, most importantly, we tokenize it and we tokenize it by doing a 
process or multiprocessor multi processing. I guess I should say, and so 
tokenizing tends to be pretty slow, but we've all got multiple cores and 
our machines now and some of the better machines on AWS and stuff can have 
dozens of course, here on a university computer, we've got 56 cause, so 
Spacey Is not very amenable to multi processing, but I finally figured out 
how to get it to work, and the good news is it's all wrapped up in this one 
function now, and so all you need to pass to that function is a list of 
things to tokenize, Which each part of that list will be tokenized on a 
different core, and so i've also created this function called partition by 
cause which takes a list and splits it into sub lists. The number of sub 
lists is the number of cores that you have in your computer, ok, so on on 
my machine without more -- processing. This takes about an hour and a half 
and with more processing. It takes about two minutes alright. So it's a 
really handy thing to have and now that this codes here, you know, feel 
free to look inside it and take advantage of it for your own stuff right. 
Remember. We all have multiple processes protocol cause, even in our 
laptops and very few things in place that and take advantage of it unless 
you make a bit of an effort to make it work.</p>

<p>So, there's a couple of tricks 
to get things working quickly and reliably as it runs. It prints out how 
it's going and so here's the result at the end right, beginning of stream 
token, beginning of field number one token: here's the tokenized text 
you'll see that the punctuation is on the whole. Now a separate token 
you'll see there's a few interesting little things. One is this: what's 
this t up, t up MGM for MGM obviously was originally capitalized right, but 
the interesting thing is that normally people are the lowercase everything 
or they leave the case, as is now, if you leave the case, as is then screw 
you or caps, And screw you lower case are two totally different sets of 
tokens that have to be learned from scratch or if you love a case them all, 
then there's no difference at all between screw or you and right. So how do 
you fix this so that you both get a semantic impact of like I'm shouting 
now right, but not have every single word have to learn the shouted version 
versus the normal version, and so the idea I came up with and I'm sure 
other people Have done this too, is to come up with a unique token to mean. 
The next thing is all uppercase. So then I lowercase it so now, whatever 
used to be up case, it's now lowercase, it's just one token, and then we 
can learn the semantic meaning of all uppercase, and so I've done a similar 
thing. If you've got like 29 exclamation marks in a row, we don't learn a 
separate token for 29 exclamation marks.</p>

<p>Instead, I put in a special token 
for the next thing repeats lots of times, and then I put the number 29 and 
then I put the explanation. Mark. Okay and so there's a few little tricks 
like that, and if you're interested in LP have a look at the code tokenizer 
for these little tricks that I've added in because some of them are kind of 
fun. Okay, so the nice thing with doing things this way is we can now just 
NP dot, save that and load it back up later, like we don't have to 
recalculate all this stuff. Each time like we tend to have to do with with 
torch text or a lot of other libraries </p>

<h3>7. <a href="https://youtu.be/uv0gmrXSXVg?t=38m">00:38:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Use batches to overcome RAM ‚ÄúOut of Memory‚Äù</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Okay, so we've now got it tokenized. The next thing we need to do is to 
turn it into numbers, which we call numerical Eisinger and the way we 
numeric lies. It is very simple: we make a list of all the words that 
appear in some order and then we replace every word with its index into 
that list, the list of all the words that appear or all the tokens that we 
call the vocabulary. So here's an example of some of the vocabulary, the 
counter class in in plaque. That is very handy for this. It basically gives 
us a list of unique items and their counts. Okay, so here are the 25 most 
common and things in the vocabulary. You can see there are things like 
apostrophe s and double quote and end of paragraph and also stuff like 
that. Now, generally speaking, we don't want every unique token in our 
vocabulary, if it doesn't appear at least two times, then might just be a 
spelling mistake or a word I mean we can't learn anything about it if it 
doesn't appear that often also the stuff that we're Going to be learning 
about at least so far in this part gets a bit clunky once you've got a 
vocabulary bigger than 60-thousand time permitting. We may look at some 
work. I've been doing recently on handling larger vocabularies, otherwise 
that might have to come in a in a future class okay, but actually for 
classification. I've discovered that doing more than about 60,000. Words 
doesn't seem to help anyway, so we're going to limit our vocabulary to 
60,000 words, things that appear at least twice, and so here's a simple way 
to do that: use that dot most common pass in the mass vocab size, that'll 
sort it by the frequency by The way and if it appears less often than a 
minimum frequency, then don't bother at all okay, so that gives us Ida s. 
That's the the same name that torch text used.</p>

<p>Remember. It means that into 
string. So this is just the list of tokens unique tokens in the vocab. I'm 
going to insert two more tokens a token for unknown, evoke a bite of your 
unknown and vocab item for padding. Okay, then we can create the dictionary 
which goes in the opposite direction. So string to end and that won't cover 
everything, because we intentionally truncated it down to 60,000 words 
right, and so, if we come across something that's not in the dictionary, we 
want to replace it with zero for unknown, and so we can use a default dick. 
For that, with a lambda function that always returns Europe, okay, so you 
can see all these things were using the kind of kick coming coming back up. 
So now that we've got our s to a dictionary defined. We can then just call 
that for every word for every sentence right and so there's an Americanized 
version and there it is okay, and so, of course, the nice thing is again. 
We can save that step as well. That's so, each time we get to another step. 
We can save it, and these are not very big files compared to what you used 
to with with images text is generally pretty small, very important to also 
save that vocabulary, but because the this, this list of numbers means 
nothing. Unless you know what each number refers to and that's what I to 
restaurants, you okay, so you save those three things and then later on, 
you can load them back up. So now our vocab size is 60,000 and term and 
training language model has 90,000 documents.</p>

<p>You know okay, so 
</p>

<h3>8. <a href="https://youtu.be/uv0gmrXSXVg?t=42m">00:42:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Final layers to our ResNet model</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>That's the pre-processing you do. We can probably rap a little bit more of 
that in little utility functions if we want to, but it's all pretty 
straightforward and basically that exact code will work for any data set. 
You have once you've got it in that CSV format. So here is a kind of a new 
insight, that's not new at all, which is that we'd like to pre train. 
Something like we know from lesson, four, that if we pre train our 
classifier by first creating a language model and then fine-tuning that as 
a classifier. That was helpful. Do you remember it actually got us a new 
start at the out result. We got the best IMDb classifier result that had 
ever been published, but quite a bit. Well, we're not going that far 
enough, though right, because IMDB movie reviews are not that different to 
any other English document. You know compared to how different they are to 
a random string or even to a Chinese document. So, just like imagenet 
allowed us to train things that recognized stuff. That kind of looks like 
pictures and we could use it on stuff. That was nothing to do with image 
net like satellite images, why don't we train a language model? That's just 
like good at English and then fine tune it to be good at major reviews. So 
this basic insight Leadbeater try building a language model on Wikipedia, 
so my friend Steven marady has already processed Wikipedia found a subset 
of nearly you know the most of it, but throwing away the stupid little 
articles so most of the bigger articles, and he calls that Wiki text 103, 
so I grabbed wiki text 103 and I trained a language model honor and we and 
I used exactly the same approach, I'm about to show you for trading an IMDB 
language model, but instead I trained a wikitext 103 language model and 
then I saved It and I've made it available for anybody who wants to use it 
at this URL. So this is not a URL for wikitext 103. The documents this is 
the wiki text - 103, the language model, and so the idea now is: let's 
train an IMDB language model which starts with these words there, hopefully 
to you folks.</p>

<p>This is a extremely obvious, extremely non-controversial 
idea, because it's basically what we've done in nearly every class so far, 
but when I first when I first mentioned this to two people in the NLP 
community. I guess, like June July of last year, there couldn't have been 
less interest. You know I asked on Twitter, where a lot of the top thread 
of research is kind of people that I follow and they'll be back. I was like 
hey what if we like pre-trained like a general language model like no old 
language, is different. You know you can do that or, like I don't know why 
you would bother. Anyway. I've talked to people at conferences and they're, 
like pretty sure, people have tried that and it stupid like there was just 
this like. I don't know this weird like frank past, and you know I guess, 
because I am arrogant in obstreperous. I ignored them, even though they 
know much more about NLP than I do, and just trade it anyway, and let me 
show you what happened so: here's how we do it right, grab the wiki text 
models right and if you use double you get minus R it'll. Actually, 
recursively grab the whole directory. It's got a few things or not. We need 
to make sure that our language model has exactly the same, embedding size, 
number of hidden and number of layers. As my wiki text, one did otherwise. 
You can't load the way it's in here, so here's our pre-trained path, here's 
our preteen language model path; let's go ahead and torch dot load in those 
weights from the forward.</p>

<p>Wickety text 103 model; okay, we don't normally 
use torch dot load. But that's that's! That's the you know the pytorch way 
of </p>

<h3>9. <a href="https://youtu.be/uv0gmrXSXVg?t=47m">00:47:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Nearest Neighbors to look at examples</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Grabbing a file - and it basically gives you a dictionary containing the 
name of the layer and a tensor of those weights or an array of those words. 
Now, here's the problem that wiki text language model was built with a 
certain vocabulary, which was not the same as this one was built on right. 
So my number 40 was not the same as wiki text. 103 models number 40. So we 
need to map one to the other. Okay, that's very very simple, because 
luckily I saved the I to s for the wiki text: okay right! So here's the 
list of what each word is when I trained the wiki text 103 model, and so we 
can do the same default dict trick to map it in Reverse right and I'm going 
to use minus 1 to mean that it's not in the wiki text. Dictionary, and so 
now I can just say: okay, my new set of weights is just a whole bunch of 
zeros, with vocab sighs by embedding size, so we're going to create an 
embedding matrix. I'm then going to go through every one of the words in my 
IMDB vocab room. Okay, I am going to look it up in s, I so a string to int 
for the wiki text 103 vocabulary and see if it's words there. Okay - and if 
that is word there, then I'm not going to get this minus one right, so I 
will be greater than or equal to zero. So in that case, I will just set 
that row of the embedding matrix two to the weight that I just looked that 
which was stored inside this this named element, and so these these names 
you can look at.</p>

<p>You can just look at this dictionary and it's pretty 
obvious what each name corresponds to. It looks very similar to the names 
that you gave it when you set up your module. So here are the encoder 
weights. Okay, so I grabbed it from the encoder weights. If I don't find 
it, then I will use the row mean in other words, here is the average 
imbedding weight across all of the wikitext? What are three things? Okay, 
so that's pretty simple. So I'm going to end up with an embedding matrix 
for every word. That's in both my vocab room for IMDB and the week in text 
103 vocab. I will use the wiki text 103, embedding matrix weights for 
anything else. I will just use whatever was the average weight from the 
wiki text 103, embedding matrix, okay and then I'll go ahead, and I will 
replace the encoder weights with that turn into a tensor. We haven't talked 
much about weight tying. We might do so later, but basically the decoder. 
So the thing that turns the final prediction back into a word uses exactly 
the same weights. So I pop it there as well and then there's a bit of a 
weird thing with how we do embedding dropout. That ends up with a whole 
separate copy of them for a reason that doesn't matter much anyway, so we 
just popped the weights back where they need to go. So this is now 
something that a dictionary we can now or a set of torch state which we can 
load in.</p>

<p>So let's go ahead and create our language model, okay and so the 
basic approach we're going to use - and I'm going to look at this in more 
detail in a moment. But the basic approach are going to use. Is I'm going 
to concatenate all of the documents together into a single, a single list 
of tokens of length? Twenty four point: nine: nine: eight million okay. So 
that's going to be what I pass in as my trainings, so the language model 
right, we basically just take all our documents and just concatenate them 
back to that: okay and we're going to be continuously trying to predict. 
What's the next word after these words? What's the next word after these 
words, have a look at these details in a moment, I'm going to set up a 
whole bunch of drop out. Look at that in detail moment. Once we've got a 
model data object. We can then grab the model from it. So that's going to 
give us a learner, okay and then, as per usual, we can call lo not fit so 
we first of all, as per usual, just do a single epoch on the last layer 
just to get that. Okay and the way I've set it up is the last layer is 
actually the embedding words because that's obviously the thing that's 
going to be the most wrong, because, like a lot of those, embedding weights 
didn't even exist in the vocab, so we're just gon na train A single epoch 
of just the inventing weights and then we'll start doing a few epochs of 
the full model, and so how is that? Looking? Well, here's lesson, four, 
which was our academic world's best ever result and after 14 he parks.</p>

<p>We 
had a four point: two three loss here. After one epoch, we have a four 
point: one two loss right so by pre-training on wikitext 103. In fact, 
let's go and have a look. We kept training and training at a different 
rate. Eventually, we got to four point, one six, so by pre training on 
wikitext 103, we have a better loss after one Èpoque than the best loss we 
got for the language model. Otherwise, yes, Rachel. What is the wikitext 
103 model? Is it a double UDL STM again? Yeah and we're about to dig into 
that. It's the way I trained. It was literally the same lines of code that 
you see here, but without pre-training it only takes one two, three okay, 
so let's take a 10-minute break, come back at 7:40 and dig in and have a 
look at these models. Okay, welcome back before we go back into language 
models and NLP classifiers a quick discussion about something pretty new at 
the moment, which is the faster I dock project. So the goal of a fast IO 
dock project is to create documentation that makes readers say wow. That's 
the most fantastic document, documentation, I've ever read, and so we have 
some specific ideas about how to do that. But it's the same kind of idea of 
like top down. You know thoughtful, take full advantage of the medium 
approach. You know interactive experimental code first that we're all 
familiar with if you're interested in getting involved the basic approach 
you can see in in the docs directory. So this is the this is the readme in 
the docs directory.</p>

<p>In there there is a, amongst other things, a transforms 
template dot, a doc. What the hell is a doc, hey Doc is asciidoc. How many 
people here have come across sq doc? That's awesome. Ascii table is people 
are laughing because there's one hand up, and it's somebody who was in our 
study group today, who talked to me about ask you dog, ask you doc is the 
most amazing project, it's like markdown, but it's like what markdown needs 
to be to Create actual books and like a lot of actual books are written in 
ASCII doc, and so it's as easy to use this back down about there's way more 
cool stuff. You can do with it. In fact, here is an ASCII doc file here, 
right and as you'll see, it looks very normal there's headings right and 
this is pre formatted text and there's yeah </p>

<h3>10. <a href="https://youtu.be/uv0gmrXSXVg?t=55m">00:55:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Fine-Tuning our models and more ‚ÄúOut of Memory‚Äù fixes</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>There's lists and whatever else it looks pretty standard and actually I'll 
show you a more complete sq doc thing more standard after doc thing, but 
you can do stuff like say, put a table of contents here, please, you can 
say means put a definition list here. Please plus means this is a 
continuation of the previous list item and so there's just like little 
things that you can do, which are super handy or like put this thing, make 
it slightly smaller than everything else. So it's like turbocharged 
markdown, and so this asciidoc creates this HTML and I didn't add any CSS 
or do anything myself like. We literally started this project like four 
hours ago. So this is like just an example. Basically right, and so you can 
see, we've got a table of contents. We can jump straight to here. We've got 
a cross reference. We can click on to jump straight to the cross reference, 
each method, kind of comes along with its details and so on and so forth 
right and to make things even easier, rather than having to know that this 
is meant to be like. The argument list is meant to be smaller than the main 
part, or how do you create a cross reference or how you meant to format the 
arguments to the method name and list out each one of its arguments? We've 
created a special template where you can just write various stuff in curly 
brackets. Like please put the arguments here and here is an example of one 
argument, and here is a cross reference, and here is a method and so forth. 
So we're in the process of documenting the documentation, template that 
there's basically like five or six of these little curly braket things 
you'll need to learn.</p>

<p>But for you to create the documentation of class or a 
method, you can just copy one. That's already there basically, and so the 
idea is we're going to have like it'll almost be like a book. You know, 
there'll be tables and pictures, little videos, segments and hyperloop 
throughout and all that stuff. You might be wondering what about 
docstrings, but actually I don't know if you've noticed, but if you look at 
the Python standard library and look at the doc string, for example, for 
reg X, compile it's single-line. Nearly every dock string in Python is a 
single line, and Python then does exactly this. They have done a website 
containing the documentation that says like hey. This is what regular 
expressions are, and this is what you need to know about them and if you 
want them to grow fast to unity, his compile and here's what some 
information about compile and his examples. It's not in the dog stream and 
that's why we're doing it as well. Our dog strings will be one line unless 
you need like two. Sometimes it's going to be very similar to Python, but 
even better. So everybody is welcome to help contribute to the 
documentation and hopefully by the time, if you're, watching this on the 
MOOC it'll be reasonably fleshed out and we'll try to keep a list of things 
to do all right. So I'm going to do one first. So one question that came up 
in the break was: how does this compare to word to vet, and this is 
actually a great thing for you to spend time.</p>

<p>Thinking about during the 
week is how does this compare to work? LÈvesque I'll give you the summary 
now, but it's a very important conceptual difference. The main conceptual 
difference is what is word to vet word. Tyvek is a single embedding matrix. 
Each word has a vector and that's it so, in other words, it's a single. 
It's a single layer from a pre-trained model and specifically that layer is 
the input layer and also specifically, that pre-trained model is a linear 
model. Okay, that is free trained on something called a co-occurrence 
matrix. So we have no particular reason to believe that this model has 
learned anything much about the English language or that it has any 
particular capabilities, because it's just a single linear layer and that's 
it. So, what's this wikitext 103 model, it's a language model and it has a 
four hundred dimensional, embedding matrix three hidden layers with 1,150 
activations per layer and regularization, and all that stuff tied input. 
Output matrix equator sees it's basically a state-of-the-art AWD ASTM. So 
like. What's the difference between a single layer of a single linear model 
versus a three layer, recurrent neural network everything you know: they're 
they're, very different levels of capability and so you'll see when you try 
using a pre trained language model. This is a what Vic layer you'll get 
very, very different results to the vast majority of tasks.</p>

<p>What if the 
numpy array does not fit in memory? Is it possible to write a pytorch data 
loader directly from a large CSV file? It almost certainly won't come up, 
so I'm not going to spend time on it like these things are tiny. These 
they're just hints think about how many Institute would need to run out of 
memory. That's not gon na happen. They don't have a kitten's jpg memory. 
Just in your memory, so I've actually done a another Wikipedia model which 
I called Giga wiki, which was on all of Wikipedia and even that easily fits 
in there and the reason I'm not using. It is because it turned out not to 
really help very much business. Wiki text 103, but you know I've built a 
bigger model than anybody else. I've found in the academic literature 
pretty much and it fits in memory on a single machine. What is the idea 
behind averaging the weights of embeddings they're going to be set to 
something? You know like there are words that weren't there, so other 
options is, we could leave them a zero, but that seems like a very extreme 
thing to do. Like zero is a very extreme number. Why would it be zero? We 
could set it equal to some random numbers, but if so, what would be the 
mean and standard deviation of those random numbers, or should they be 
uniform if we just average the rest of the you know the embeddings, then we 
have something that's reasonable scale. I just declare this is how you're 
initializing words that didn't appear in the training.</p>

<p>Yeah, that's right 
and then I think you've pretty much kind of just answered this one. But 
someone had asked if there's a specific advantage to creating our own 
pre-trained, embedding over using glob or word back yeah. I think I know 
we're not creating a preacher and embedding we're putting a pre training 
model. Okay, so um. Let's talk a little bit more with this is a ton of 
stuff we've seen before, but it's changed a little bit. It's actually a lot 
easier than it was in part one, but I want to go a little bit deeper into 
the language model. Loader, okay. So this is the language model loader, and 
I really hope that by now you've learned in your editor or IDE how to jump 
to symbols. Okay, yeah! I don't want it to be a burden for you to find out 
what the source code of language model loader is alright, and if it's still 
a burden, please, you know go back and try and learn those keyboard 
shortcuts in vs code. You know like if </p>

<h3>11. <a href="https://youtu.be/uv0gmrXSXVg?t=1h3m">01:03:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Find images similar to a word or phrase &amp;</b></li>

<li><b>Find images similar to an image !</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Your editor doesn't make it easy, don't use that editor anymore. Okay, 
there's lots of good free editors that make this easy; okay, so so here's 
the source code for language model, loader and yeah. It's it's it's 
interesting to notice that it it's not doing anything particularly tricky. 
It's not deriving from anything at all right. What makes it something 
that's capable being a data loader is that it's something you can iterate 
over okay, and so specifically, okay, so, specifically, here's the fit 
function inside foster yo model. This is like whatever, where everything 
ends up. Eventually, which goes to each epoch and then it creates an 
iterator from the data loader and then just does a for loop through it. 
Alright, so anything you can do a for loop through can be a data. Loader 
specifically, it needs to return tuples of mini-batches, independent and 
dependent variable for mini batches. So anything with a Dunda in a method 
is something that can act as an iterator and yield is a neat little Python 
keywords. You probably should learn about. If you don't already know it, 
but it basically spits out a thing and wets for you to ask for another 
thing: normally in like a for loop or something. So in this case we start 
by initializing the language model, passing it in the numbers.</p>

<p>So this is 
the numerical eyes, big big long list of all of our documents, concatenated 
together, and the first thing we do is to batch of fudge, and this is the 
thing which quite a few of you got confused about last time right if our 
batch size Is 64 and our we have 24 25 million numbers in our list. We are 
not creating items of length 64 we're not doing that we're creating 64 
items in total, so each of them is of size, T divided by 64, which is three 
hundred and ninety thousand. Okay, so that's what that's what we do here 
when we reshape it, so that this axis here is of length 64 and then this 
minus one is everything else. So that's three hundred and whatever a 
thousand three hundred ninety thousand blob, okay and then we transpose it. 
So that means that we now have 64 columns three hundred ninety thousand 
rows and then what we do, each time we do an iterate is we grab one batch 
of some sequence length, we'll look at that in a moment, but basically it's 
approximately equal to VP TT, Which we set to 70 stands for back prop 
through time and we just grab that many rows. Okay, so from i to i plus 70 
rows, and then we try to predict that plus one remember so we're trying to 
predict when past, where a wrap, so we've got 64 columns and each of those 
is one sixty-fourth of our 25 million or whatever it was Tokens, you know 
hundreds of thousands long and we just grab.</p>

<p>You know 17 at a time, so each 
of those columns each time and Gravatt is going to kind of hook up to the 
previous column. Okay, and so that's why we get this consistency. This 
language model is it's stateful, which is really important. Pretty much. 
All the cool stuff in the language model is is stolen from Steven Mara, 
T's, AWD, LS TM, including this little trick here, which is if we always 
grab 70 at a time. And then we go back at me to a new epoch, we're going to 
grab exactly the same batches every time, there's no randomness. Now, 
normally we shuffle out data every time we do an epoch or every time we 
grab some data, we grab it at random. You can't do that with a language 
model, because this set has to join up to the previous set, because it's 
fun to trying to learn the sentence right and if you suddenly jump 
somewhere else, then that doesn't make any sense as a sentence. So Stevens 
idea is to say: okay. Well, since we can't shuffle the order, let's step 
randomly change, the size, the sequence length, okay and so basically, he 
says all right: 95 % of the time, we'll use vacate 80 70, but 5 % of the 
time we'll use. Half that right and then he says you know what I'm not even 
going to make that the sequence length I'm going to create a normally 
distributed random number with that average and a standard deviation of 
</p>

<h3>12. <a href="https://youtu.be/uv0gmrXSXVg?t=1h8m15s">01:08:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Homework discussion</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>5 and I'll make that the sequence length right, so the sequence length is 
seventy ish, and that means every time we go through we're getting slightly 
different batches. So we've got that little bit of extra randomness. I 
asked him Steven marady, where he came up with this idea. Did he think of 
it and he was like, I think I thought of it, but it seemed so obvious, and 
I bet I didn't think of it, which is like true of like every time. I come 
up with an idea or deke why I think it always seems so obvious that um 
somebody else is thought of it, but I think he thought of it so yeah. So 
this is like a nice thing to look at if you're trying to do something. A 
bit unusual with the data motor, it's like, okay, here's, a simple kind of 
role model you can use as to creating a data loader from scratch, something 
that spits out batches of do so so our language model loader, just took in 
all of the documents concatenated Together, along with a batch size and the 
VPT, now, generally speaking, we want to create a learner and the way we 
normally do, that is by getting a model, data object and they're, calling 
some kind of method which have various names. But sometimes you don't often 
we call that method get model, and so the idea is that the model data 
object has enough information to know what kind of model to give you. So we 
have to create that model data object, which means we need that that class, 
and so that's very easy to do right.</p>

<p>So here are all of the pieces we're 
going to create a custom learner, a custom model data class and a custom 
class. So a model data class again this one doesn't inherit from anything. 
So you really see it is, there's almost nothing to do right. You need to 
tell it most importantly: what's your training set, give it a data loader? 
What's the validation set, give it a data loader and optionally, give it a 
test, set data, loader, plus anything else that needs to know right, so it 
might need to know the VPT. It needs to know the number of tokens. That's 
the vocab size. It needs to know what is the padding index and so that it 
can save temporary files and models model data, as always need to know the 
path. Ok, and so we just grab all that stuff and we dump it right and 
that's it. That's the entire initializer. There's no logic there at all. 
Okay, so then, all of the work happens inside get model right and so get 
model calls something we'll look at later, which just grabs a normal pie, 
torch and end module architecture, okay and Chuck's it. On the GPU note, 
with pytorch. Normally we would save cuda with faster, i it's better to say 
to GPU, and the reason is that if you don't have a GPU it'll leave it on 
the cpu and it also provides a global variable. You can set to choose 
whether it goes on the GPU or not.</p>

<p>So it's a it's a better approach, so we 
wrapped the model in a language model and the language model is this. 
Basically, a language model is a subclass of basic model. It basically 
almost does nothing except it defines layer groups and so remember how, 
when we do like discriminative learning rates where different layers have 
different learning rates or like we freeze different amounts, we don't 
provide a different learning rate for every layer because there can be like 
A thousand layers we provide a different learning rate for every layer 
group right. So when you create a custom model, you just have to override 
this one thing which returns a list of all of your layer groups, and so in 
this case my last layer group contains the last part of the model and one 
bit of drop out and the Rest of it, this star here means Paul, is apart, so 
this is basically going to be one layer per RNN layer, okay, so that's all 
that is and then finally turn that into a learner and so alone. Oh, you 
just pass in the model and it turns it into a real owner. In this case, we 
have overridden learner and the only thing we've done is to say I want the 
default loss function to be across entropy, okay, so this you know entire 
set of model, customer model data, custom learner or fits on a single 
screen, and they always Basically, look like this right, so that's a kind 
of little dig inside this pretty boring part of the code this.</p>

<p>So the 
interesting part of this code base is getting out now, because that 
language model is actually the thing that gives us our AWD lsdm, and it 
actually contains the big idea that the big, incredibly simple, idea that 
everybody else here thinks is really obvious - that everybody In the NLP 
community I spoke to, thought was insane, which is basically every model 
can be thought of. Pretty much. Every model can be thought of as a backbone 
plus a head, and if you pre, train the backbone and stick on a random head, 
you can do fine tuning and that's a good idea, all right, and so here's 
these two bits of the code literally right. Next to each other, this is 
kind of all. There is inside this bit of faster I've, dot LM RNN, here's 
gate, language model, here's gate, classifier, that language model creates 
an R and n encoder and then creates a sequential model that sticks on top 
of that. A linear decoder classifier creates an R Ln encoder and then a 
sequential model that sticks on top of that appalling linear. Classifier 
well see these. What these differences are in a moment, but you get the 
basic idea right, they're, basically doing pretty much the same thing. 
They've got this: it's head linear layer on top, so that's worth digging in 
a little bit deeper and seeing what's going on here.</p>

<p>Yes, Rachel. There was 
a question earlier about whether that any of this translates to other 
languages. Yeah this whole thing works in any language you, like, 
obviously, would you have to retrain your language model on a corpus from 
that language, yeah yeah, so the wiki text, 103 pre-trained language model 
knows English right. You could use it, maybe as a pre train start for, like 
a French short German model, start by retraining. The embedding layer from 
scratch might be helpful - Chinese - maybe not so much but like, given that 
a language model can be trained from any and labeled documents at all. 
You'll never have to do that right because every every almost every 
language in the world has you know plenty of documents. You can grab 
newspapers web pages. You know parliamentary records, whatever you know, as 
long as you've got a few thousand documents showing somewhat normal usage 
of that language, you can create a language model, and so I know some of 
our students. You know one of our students is a bill collector in a week 
very embarrassing, tried this approach for Thai and he said like the first 
model he built easily beat the previous Dennis the anti classifier, like 
it's yeah like for those of you that are international fellows. This is an 
easy way for you to kind of to whip out a paper in which you, you know, 
either create the first ever classifier in your language or beat everybody 
else's classifier in your language.</p>

<p>And then you can tell them that you've 
been a student of deep moaning for six months and piss off all the 
academics in your country, okay, so here's </p>

<h3>13. <a href="https://youtu.be/uv0gmrXSXVg?t=1h16m45s">01:16:45</a></h3>

<ul style="list-style-type: square;">

<li><b> How to: multi-input models on large datasets</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Our edit encoder, it's just a standard inin module most of the text in it 
is actually just documentation. As you can see it, it looks like there's 
more going on in it. Then there actually is, but really all there is. Is we 
create an embedding layer? We create an LST M for each layer. That's been 
asked for that's it. Everything else in it is dropped out right. Basically, 
all of the interesting stuff just about in the AWA DL STM paper is all of 
the places you can put drop out and then the forward is basically the same 
thing right. It's call the embedding layer. Add some dropout go through 
each layer. Call that are a ten liya append it to our list of our ports. 
Add dropout, that's about it! Okay, so it's it's! It's really pretty 
straightforward and the the paper you want to be reading, as I've 
mentioned, is the AWD lsdm paper, which is this one here. Regularizing and 
optimizing lsdm language models and it's it's well-written and pretty 
accessible right and entirely implemented inside fastai as well right. So 
you can see all of the code for that paper and like a lot of the code, 
actually is I'm shamelessly plagiarized, with Stephens permission from his 
excellent github, repo WBLS TM and the process of which I picked some of 
his bugs as well. I even told him about them so yeah, so I talked you know, 
I'm talking and increasingly about. Please read the papers, so here's the 
paper. Please read this paper and it refers to other papers.</p>

<p>So for things 
like, why is it that the encoder wait and the decoder wait are the same 
right? Well, it's because there's this thing called tie weights. This is 
inside. This is inside that gate, language model. There's a thing called 
tie weights the defaults to true, and if it's true then the we actually 
tie, we literally use the same weight matrix for the encoder and the 
decoder so like they're, literally pointing at the same block of memory. If 
you like, and so why is that, what's the result of it, that's one of the 
citations and Stephens paper is also a well-written paper. You can go and 
look up and learn about wait time. So there's a lot of cool stuff in there. 
Okay, so we have basically a standard iron in the only reason where it's 
not standard is it's just got lots more types of dropout in it and then a 
sequential model. On top of that, we stick a linear decoder, which is 
literally half the screen of code. It's got a single linear layer, we 
initialize the weights to some range. We add some dropout and that's it. So 
it's a linear layer. Okay, so we've got an iron N on top of that mystic 
linear layer with dropout and we finished okay. So that's the language 
model, so um. What dropout you choose matters a lot and through a lot of 
experimentation, I found a bunch of dropouts. You can see here. We've got 
like each of these corresponds to a particular accurate, a bunch of 
dropouts that tend to work.</p>

<p>Pretty well for language models, but if you 
have less data for your language model, you'll need more dropout. If you 
have more data, you can benefit from less dropout. You don't want to 
regularize more than you have to make sense right, rather than having to 
tune every one of these five things right. My claim is they're already 
pretty good ratios to each other, so just tune this number. I just multiply 
it all by something. Okay, so there's really just one number you have to 
choose so if you're overfitting, then you'll need to increase this number. 
If you're underfitting you'll need to decrease this number, because other 
than that these ratio is actually seem pretty good. So one important idea, 
which may seem pretty minor but again it's incredibly controversial - is 
that we should measure accuracy when we look at a language model, so 
normally language models. We look at this. This lost value, which is just 
cross entropy loss, but specifically, where you nearly always take e to the 
power of that which the NLP community calls perplexity. Okay, so perplexity 
is just a ^ cross. Entropy there's a lot of problems with comparing things 
based on cross entropy loss. I'm not sure I've got time to go into it in 
detail now, but the basic problem is that it's kind of like that thing. We 
learned about focal loss, cross entropy loss if you're right yet wants you 
to be really confident that you're right, you know, so it really penalized 
as a model that doesn't kind of say like I'm so sure this is wrong.</p>

<p>It's 
wrong, whereas accuracy doesn't care at all about how confident you are. 
This cop cares about whether you're right - and this is much more often 
than the thing which you care about in real life. So this accuracy is what 
how many? How often do we guess the next word correctly and I just find 
that a much more stable number to keep track of so so that's a simple 
little thing that I do okay, so so we train for a while </p>

<h3>14. <a href="https://youtu.be/uv0gmrXSXVg?t=1h23m15s">01:23:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Generative Adversarial Networks (GAN) in Keras</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>And we get down to a 3.9 frost, entropy loss and if you go e ^ that and to 
kind of give you a sense of like what's happened with language models, if 
you look at academic papers from about 18 months ago, you'll see them 
talking about. Perplexities stayed at the our complexities of like over a 
hundred okay, like the the the rate at which our ability to kind of 
understand language, and I think, like measuring language model. Accuracy 
or perplexity, is not a terrible proxy for understanding language. If I can 
guess what you're going to say next, it's you know, I pretty much need to 
understand language pretty well and also the kind of things you might talk 
about pretty well, so this numbers just come down so much. It's been 
amazing, NLP in the last 12 to 18 months, and it's going to come down a lot 
more. It really feels like 2011-2012 computer vision. You know we're just 
starting to understand, transfer, learning and fine tuning, and these basic 
models are getting so much so much better. So everything you thought about 
like what NLP can and can't do it's very rapidly going out of that. Like 
there's lots of stuff NLP is not good at to be clear right. Just like in 
2012, there was what's the stuff computer vision wasn't good at, but it's 
changing incredibly rapidly and now is a very, very good time to be getting 
very, very good at NLP or starting startups, based on NLP, because there's 
a whole bunch of stuff which Computers would absolutely at two years ago 
and now like not quite as good of people and then next year, they'll be 
much better than people yeah, two questions, one: what is your ratio of 
paper reading versus coding in a week gosh? What do you think Rachel? You 
say me, I mean it's a lot more coding right, it's a lot more coding.</p>

<p>I feel 
like it also really varies from week to week, like I feel like they're like 
with that bounding box stuff. You know that, like with that bounding box 
stuff, there was all these papers and no man through, and so I didn't even 
know which one to read first and then I'd read the citations and didn't 
understand any of them, and so there was a few weeks of Just kind of 
reading papers before I even know what to start coding. That's unusual, 
though, like most of the time I'm yeah, I don't know anytime, I start 
reading a paper. I'm always convinced that I'm not smart enough to 
understand it, always regardless of the paper and somehow eventually I do 
but yeah I try to spend as much time as I can coding and then the second 
question is your dropout rate, the same through the training or Do you 
adjust it and the weights accordingly? I just say one more thing about the 
last bit, which is very often like the vast majority. Nearly always I after 
I've read a paper even after I've read the bit that says this is the 
problem, I'm trying to solve I'll kind of stop there and try to implement 
something, but I think might solve that problem and then I'll go back and 
read the Paper - and I read little bits about like all these - are how I 
solve these problem bits and I'll, be like. Oh, that's, a good idea and 
then I'll try to implement those, and so like that's why, for example, I 
didn't actually implement SSD. You know, like my custom here, is not the 
same: it's because I kind of read the gist of it, and then I tried to 
create something best as I cord and then go back to the papers and try to 
see why and so so by the time I got to the focal most paper.</p>

<p>Rachael will 
tell you. I was like driving myself crazy with like how come. I can't find 
small objects. How come it's always predicting background? You know I read 
the focal loss paper and I was like that's why you know so. Like it's so 
much better when you deeply understand the problem they're trying to solve 
- and I do find the vast majority of the time by the time I read that bit 
of the paper, which is like solving a problem - I'm then like yeah, but 
these three ideas. I came up with, they didn't try, you know and he 
suddenly realized that you've got new ideas or else. If you just implement 
the paper, you know mindlessly it's. You know you tend not to have these 
insights about better ways to do it. Yeah um varying drop out is really 
interesting and there are some recent papers, actually that suggest 
gradually changing drop out, and it was either a good idea to gradually 
make it smaller or gradually make it bigger, I'm not sure which let's try. 
Maybe one of us can try and find it. During the week I haven't seen it 
widely used. I tried it a little bit with the most recent paper I wrote and 
it I had some good results. I think I was graduating, like you get smaller 
yeah and then the next question is. Am I correct in thinking that this 
language model is built on word embeddings? Would it be valuable to try 
this with phrase or sentence embeddings? I ask that I ask this because I 
saw from Google the other day universal sentence: encoder yeah.</p>

<p>No, this is 
like this is much better than that. You like to shoot. I mean like it. This 
is. This is not just an embedding of a sentence. This is an entire model 
right, so an embedding by definition, is like a fixed thing. Oh, I think 
they're asking they're saying that this language. Well, i the first 
question is: is this language model built on word embedded? Yes, but but 
it's not saying it's a sentence or a phrase: embedding is always a model 
that creates that right and we've got a model. That's like trying to 
understand language, it's not just as phrase it's not just a sentence. You 
know it's a it's a document in the end and it's not just an embedding. 
That's we're training through the whole thing so like this has been a huge 
problem with NLP for years. Now is this attachment they have to embeddings, 
and so even the the paper that the community's been most excited about 
recently from the from AI to the Allen Institute called Elmo yo mo, and 
they found much better results across lots of models. But again it was an 
embedding. They took a fixed model and created a fixed set of numbers which 
they then fed into a model, but in in computer vision. We've known for is 
that that approach of having a fixed, fixed set of features, they're called 
hyper columns. In in computer vision, people stopped using them like three 
or four years ago, because fine-tuning the entire model works much better 
right. So for those of you that have spent quite a lot of time with NLP and 
not much time with computer vision, you're going to have to start 
relearning right all that stuff.</p>

<p>You have been told about this idea that 
there are these things called embeddings and that you learn them ahead of 
time, and then you apply these things, whether it be word level or phrase 
level or whatever level. Don't do that all right, you want to actually 
create a pre trained model and fine-tune it and to it then you'll see some 
you'll see some specific results, all right. So, as you answer the existing 
lines for using accuracy instead of perplexity as a metric for the model, 
could we work that into the loss function rather than just use it as a 
metric? No, you never want to do that, whether it be computer vision or NLP 
or whatever it's too bumpy right so first have to be spying, is a loss, 
function and other thing instead of I use it. In addition to you know, I 
think it's good to look at the accuracy and to look at the cross entropy, 
but for your loss function you need something nice and smooth accuracy. 
Doesn't work very well. You'll, see, there's two different versions of save 
is save and save. Encoder save saves the whole model as </p>

<h3>15. <a href="https://youtu.be/uv0gmrXSXVg?t=1h32m">01:32:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Multi-Layer-Perceptron (MLP)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Per usual, save encoder saves just that bit right, in other words, in the 
sequential model it saves just that bit and not that bit. In other words, 
you know this bit, which is the bit that actually makes it into a language 
model. We don't care about the classifier. We just care about that bit. 
Okay, so that's why we save two different models here: okay, so let's now 
create the classifier okay and I'm going to go through this bit pretty 
quickly because it's the same but like when you go back during the week and 
look at the code convince yourself. It's the same right. We do get all P, 
tiresias, P, again, Chuck sighs again get all again save those tokens. 
Again, we don't create a new I to s vocabulary. We obviously want to use 
the same vocabulary. We had in the language model, okay too, because we're 
about to reload the same encoder. Okay same default, dict same way of 
creating our Americanized list, which, as per before, we can save okay. So 
that's all the same later on. We can reload those rather than having to 
rebuild them, so all of our hacker parameters are the same, we're not all 
of them. Sorry, the construction of the modal hyper parameters are the 
same. We can change the drop hat optimize a function pick a batch size. 
That is as big as you can, that doesn't run out of memory, and so this bits 
a bit interesting, there's some fun stuff going on here. The basic idea 
here is that for the classifier we do really want to look at one. You know 
our document right.</p>

<p>We need to say, is this document positive or negative, 
and so we do want to shuffle the documents right. That's because we we like 
to shuffle things, but those documents are different lengths, and so, if we 
stick them all into one batch - and this is a handy thing that last AI does 
for you - you can stick things of different lengths into a batch and it 
will Automatically pet them, so you don't have to worry about that. Okay. 
But if they're wildly different lengths, then you're going to be wasting a 
lot of computation times. Then what you one thing there that's 2,000 words 
long and everything else 250 words long, and that means you end up with a 
2005 tensor all right, that's pretty annoying, so James Bradbury who's. 
Actually, one of Stephen Rarity's colleagues and the guy who came up with 
torch text came up with a neat idea, which was, let's sort the data set by 
length, ish right, so kind of make it so. The first things in the list on 
the whole shorter and the things at the end, but a little bit random as 
well, okay and so I'll show you how I implemented that. So the first thing 
we need is a data set right, and so we have a data set passing in the the 
documents and their labels, and so here's a text data set and it inherits 
from data data set here - is data set from torch from pytorch And actually 
data set doesn't do anything at all.</p>

<p>It says you need to get item and if 
you don't have one you're gon na get an error, you need a length if you 
don't have one you're gon na get an error okay. So this is an abstract 
class, so we're going to pass in our X we're going to pass in our Y and get 
item is going to grab the X and grab the Y and return them. It couldn't be 
much simpler right, optionally. They could reverse it optionally. It could 
stick an end or stream at the end optionally. It started beginning we're 
not doing any of those things. So literally, all we're doing is we're 
putting in an X putting in a way and that grab an item we're returning the 
X and the y. As a couple, okay and the length is X, yes, so that that's 
that's all the data sets right, something with a length that you can in 
days so to turn it into a data loader. You simply pass the data set to the 
data loader constructor, and it's now going to go ahead and give you a 
batch of that at a time. Normally, you can say shuffle equals true or 
shuffle equals false it'll decide whether to randomize it. For you in this 
case, though, we're actually going to pass in a sample up perimeter and the 
sampler is a class we're going to define that tells the data loader shuffle 
okay, so for the validation set, we're going to define something that 
actually just sorts right.</p>

<p>It just deterministically sorts it so the that 
all the shortest documents will be at the start or the longest documents 
will </p>

<h3>16. <a href="https://youtu.be/uv0gmrXSXVg?t=1h37m10s">01:37:10</a></h3>

<ul style="list-style-type: square;">

<li><b> Deep Convolutional GAN (DCGAN)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Be at the end, and that's going to minimize the amount of padding okay for 
the training sampler we're going to create this thing. I called a sort H 
sampler, which also sorts ish right. So this is where, like I really like, 
pytorch is that they came up with this idea for an API for their data 
loader, where we can, like hook in new classes, to make it behave in 
different ways right. So here's a sort sampler but is simply something 
which again, it has a length which is the length of the data source and it 
has an iterator which is simply an iterator which goes through the data 
source sorted by length. Well, the key - and I pass in as the key, a lambda 
function, which returns the links, okay and so for the sort ish sampler. I 
won't go through the details, but it basically does the same thing with a 
little bit of randomness okay. So it's a really beautiful. You know just 
another of these beautiful little design things in pytorch that I 
discovered I could take James Bradbury's ideas which he had like written a 
whole new set of classes around, and I could actually just use the inbuilt 
hooks inside Python. You will notice data loader, it's not actually PI 
tortoise data loader, it's actually faster ice data loader, but it's 
basically almost entirely plagiarized from pytorch but customized in some 
ways to make it faster, mainly by using multi-threading instead of multi 
processing. That's Rachel. Does the pre-trained LS TM depth it'd be bt te 
to match with the new one we're training? No, no, that the be PTT doesn't 
need to match.</p>

<p>That's just like how many things do we look at at a time 
it's about nothing to do with the architecture. Okay, so well now we can 
call that function. We just wore before get our inane classifier, it's 
going to create exactly the same encoder, more or less okay and we're going 
to pass in the same architectural details as before. But this time we can 
the head that we add on you've got a few more things you can do. One is 
you: can you can add more than one hidden layer, so this layers here says 
this is what the input to my classifier section. My head is going to be. 
This is the output of the first layer. This is the output of the second 
layer and you can add as many as you like, so you can basically create a 
little multi-layer, neural net classifier at the end, and so did oh. These 
are the dropouts to go after each of these layers. Okay, and then here are 
all of the AWD lsdm dropouts, which we're going to basically plagiarize 
that idea for our classifier we're going to use </p>

<h3>17. <a href="https://youtu.be/uv0gmrXSXVg?t=1h40m15s">01:40:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Wasserstein GAN in Pytorch</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>The errand in learn - oh just like, before we're going to use 
discriminative learning rates different layers. Actually this is a separate 
here. You can try using wait. Okay or not, I've been fiddling around a bit 
with that to see what happens, and so we start out just training the last 
layer, and we get ninety two point: nine percent accuracy. Then we freeze 
one more layer, unfreeze one more layer get. Ninety three point three 
accuracy, and then we fine-tune the whole thing and after three epochs 
okay, so this was so here is the famous James pepperi we're talking about. 
This was kind of the main attempt before our paper came along at using a 
pre train model and what they did is they used a pre trained translation 
model, but they didn't fine tune. The whole thing they just took the the 
activations of the translation model and when they tried IMDB, they got 
they got 91.8 %, which we beat easily. After only fine-tuning one layer. 
Okay, they weren't state-of-the-art they're. The state of the art is 94.1, 
which we beat after fine-tuning the whole thing for three epochs, and so by 
the end, we're at ninety four point: eight, which is obviously a huge 
difference because, like in terms of error rate, that's gone down from from 
five point: Nine and then I'll tell you a simple little trick is: go back 
to the start of this notebook and reverse the order of all of the documents 
and then rerun the whole thing right and when you get to the bit that says, 
WT 103 replace this FWD Before word with PWD for backward, that's a 
backward english-language model that learns to read English backwards.</p>

<p>So 
if you redo this whole thing put all the documents in Reverse and change 
this to backward. You now have a second classifier, which classifiers 
things by positive or negative sentiment based on the reverse document. If 
you then take the two predictions and take the average of them, you 
basically have like a directional model if you've trained each bit 
separately. That gets you to ninety-five point four percent accuracy. Okay, 
so we can replace eclis load up from five point. Nine to four point six. So 
this kind of like twenty percent change in the state-of-the-art, is it's 
like. It's almost unheard of. You know it's like you have to go back to 
like Jeffrey Hinton's, imagenet computer vision thing where they drop like 
thirty percent off the state-of-the-art like it doesn't happen very often, 
and so you can see this idea of like just use, transfer learning it's 
ridiculously powerful that Every new feel thinks their new field is too 
special and you can't do it right, so it's a big opportunity for all of us, 
so we turn this into a paper and when I say we, I did it with this guy 
Sebastian, Trudeau. Now you might remember his name because in lesson five 
I told you that I actually had shared lesson four with Sebastian, because I 
think he's a an awesome researcher who I thought might like it. I didn't 
know him personally at all and much to my surprise. He actually watched the 
damn video.</p>

<p>I was like what you know what an LP research is gon na watch 
some beginners video, but he watched the whole video. He was like that's 
actually quite fantastic. That's like fun! Thank you very much. That's 
awesome coming from you and he said hey. We should turn this into a paper, 
and I said I don't write papers. I don't care about papers are not 
interested in papers. That sounds really boring and he said okay, how 
about? I write the paper for you and I said you can't really write a paper 
about this yet because you'd have to do like studies to compare it to other 
things, they're called oblation studies to see which, if it's actually work 
like there's no rigor here, I just Put in everything that came in my head 
and checked it all together and it happened to work, it's like okay. What 
if I write all the paper and all the oblation studies, then can we read the 
paper and I said: well, it's like a whole library that, like I haven't, 
documented and, like you know, I'm not going to yet and like you, don't 
know how it all Works he said: okay, if I wrote the paper and do the 
ablation studies and figure out from scratch how the code works. Without 
bothering you then, can we write the paper. I was like yeah if you did a 
whole thing. The paper I was like okay, and so then, two days later he 
comes back. He says: okay, I've done a draft with a paper. So I share this 
story to say like if you're some, you know student in Ireland - and you 
want to like - do good work.</p>

<p>Don't did anybody stop you right? I did not 
encourage him at least right, but in the end it's like look. I want to do 
this work. I think it's going to be good and I'll figure it out, and you 
know he wrote a fantastic paper and he did the ablation studies and he 
figured out how fast I works and now we're planning to write another paper 
together and so like there's. Some you've got to be a bit </p>

<h3>18. <a href="https://youtu.be/uv0gmrXSXVg?t=1h46m30s">01:46:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Introduction to Pytorch</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Careful right cuz, sometimes I get messages from random people. Saying like 
I've, got lots of good ideas. Can we have coffee? I don't want to. I don't 
you know I can have coffee in my office any time. Thank you, but it's very 
different to say. Like hey, I took your ideas and I wrote a paper and I did 
a bunch of experiments and I figured out how your code works. They added 
documentation to it. You know sure we could all submit this to a 
conference. You sort of mean like there's nothing to stop, you doing 
amazing work and if you do amazing work that like helps somebody else like 
in this case. Okay, I'm happy that we have a paper. I don't care about 
papers, but I think it's cool that you know these ideas now. Have this 
rigorous study like, let me show you what he did so he took all my code 
right, so I'd already done all the faster. I got text and stuff like that 
and as you've seen it lets us work with large corpuses. So you know 
Sebastian is fantastic. We well-read and he said here's a paper that young 
lakorn, some guys just came out with where they tried lots of different 
classification datasets. So I'm gon na try running your code on all these 
data sets, and so these are the data sets right and so some of them had. 
You know many many hundreds of thousands of documents and they were far 
bigger than, and I think I had tried, but I thought it should work.</p>

<p>Okay, 
and so you know he had a few good little ideas as we went along, and so you 
should like totally make sure you, you read, the paper write this paper and 
he said well this thing that you called in the lessons differential 
learning rates, differential kind Of means something else like: maybe we 
should rename it so it's now called discriminative learning rate. So this 
idea that we had from part one where we use different learning rates for 
different layers. After doing some literature research, it does seem like 
that hasn't been done before so it's now officially, I think discriminative 
learning rates, and so all these ideas like this is something we learnt in 
Lesson one, but it now has an equation with Greek and everything. Okay. So 
when you see an equation with Greek and everything that doesn't necessarily 
mean it's more complex than anything, we did in Lesson one because this one 
isn't again that idea of, like I'm freezing a layer at a time also it seems 
to never do number four. So it's now a thing and it's got the very clever 
name: gradual, I'm freezing. So then one promised what we're going to look 
at this slanted triangular learning rates. So this actually was not my 
idea, Leslie Smith, my favorite researchers, who you all now know about 
emailed me a while ago and said I'm so over so learning rates. I don't do 
that anymore.</p>

<p>I now do it's like a different version, where I have one 
cycle which goes up quickly at the start and then slowly down afterwards, 
and he said I often find it works better. I've tried going back over all of 
my old data sets and it works better for all of them everyone. So this is 
what the learning rate looks like right. You can use it in fastai, just by 
adding use CLR equals to your fit. This first number is the ratio between 
the highest learning rate and the lowest learning rate, so here this is 
1/32 of that. The second number is the ratio between the first peak and the 
last peak, and so the basic idea is, if you're doing a cycle length. 10, 
that you want the first cycle, sorry, the first epoch to be the upward bit 
and the other nine epochs to be the downward bit. Then you would use 10 and 
I find that works pretty well. That was also Leslie. Suggestion is make 
about a tenth of it. The upward bit and about my intense the down would be 
since he told me about it. Actually, it was just maybe two days ago he 
wrote this amazing paper, a disciplined approach to neural network hyper 
parameters in which he describes something very slightly different to this 
again, but the same basic idea. This is a must-read paper. You know it's 
got all the kinds of ideas that fastai talks about a lot in great depth, 
and nobody else is talking about this stuff.</p>

<p>It's it's kind of a slog. 
Unfortunately, Leslie had to go away on a trip before he really had time to 
edit. It properly so it's a little bit slow reading, but it's don't let 
that stop you it's amazing. So this triangle this is the equation from my 
paper - was the best year and Sebastian was like Jeremy. Can you send me 
the math equation behind that poetry? Rotation? If I just wrote the code, I 
could not get it into math, so he figured out the math for it. So you might 
have noticed the first layer of our classifier was equal to embedding size 
x, 3y times 3 times 3. Because and again, this seems to be something which 
people haven't done before so new idea, concat pooling, which is that we 
take the average pool the average pooling over the sequence of the 
activations, the max pooling of the sequence over the activations and the 
final set of Activations and just concatenate them all together again. This 
is something which we talked about in part one, but doesn't seem to be in 
the literature before so it's now called Combe kept pulling and again it's 
now got an equation and everything, but this is the entirety of the 
implementation, pull with average pull with max Concatenate those two along 
with the final sequence, so you know you can go through this paper and see 
how the faster I code implements each piece. So then, to me, one of the 
kind of interesting pieces is the difference between RNN and coda, which 
you've already seen and multi batch are in an encoder.</p>

<p>So what's the 
difference there? Okay, so the key difference is that the the normal RN an 
encoder for the language model, we could just do V PTT chunk at a time 
right, no problem and predict the next book, but for the classifier we need 
to do the whole document. We need to do the whole movie review before we 
decide if it's positive or negative and the whole movie review can easily 
be 2,000 words long and I can't fit 2,000 words worth of you know gradients 
in my GPU memory for every single one of my activations. Well, sorry, for 
every one of my weights, so what do I do? And so the idea was very simple, 
which is I go through my whole sequence length one batch of BPT be PTT at a 
time right and I call super dot forward, so in other words the eridan 
encoder, all right so just couldn't call the usual iron in Encoder to grab 
its outputs and then I've got this maximum sequence length parameter where 
it says: okay, if you've as long as you're doing no more than that sequence 
length, then start appending it to my list of outputs. Okay, so in other 
words, the thing that it sends back to this pooling is is only as much 
there's only as many activations as we've asked it to keep right, and so 
that way you can basically just figure out how much what's max SEC. Do you 
can your particular GPU handle that so it's still using the whole document, 
but let's say max SEC is a thousand thousand words and your longest 
document length is two thousand words right that it's still going through 
the I am creating state for those.</p>

<p>First thousand words right, but it's not 
actually going to store the activations. For the backdrop, the first 
thousand is only going to keep the last thousand right, so that means that 
it can't back propagate the loss back to </p>

<h3>19. <a href="https://youtu.be/uv0gmrXSXVg?t=1h55m20s">01:55:20</a></h3>

<ul style="list-style-type: square;">

<li><b> Wasserstein GAN in Pytorch (cont.)</b></li>

<li><b>&amp; LSUN dataset</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Any decisions that any state that was created in the first thousand words 
you know. Basically, that's that's now gone. So it's a really simple piece 
of code. You know and honestly, when I wrote it, it was like. I didn't 
spend much time thinking about it. It seems so obviously the only way that 
this could possibly work, but again it's it seems to be a new thing, so we 
now have back prop through time for text classification. Yes, I think so 
you can see there's lots of little pieces in this paper. So what was the 
result right and so the result was on every single data set. We tried. We 
got a better result than any previous academic for text classification, so 
IMDB trekked, 6aj News, dbpedia, Yelp, all different types and honestly 
IMDB was the only one I spent any time trying to optimize the model so like 
most of them. We just did it like. Whatever came out first, so if we 
actually spent time with it, I think this would be a lot better. That and 
the things that these are comparing to most of them are like you'll, see 
like they're different on each table because, like they're optimized, you 
know these. Like customized algorithms, on the whole, so this is saying 
like one simple fine-tuning: algorithm can beat these really customized 
algorithms, and so here's the like one of the really cool things that 
Sebastian did was his ablation studies. All right, which is.</p>

<p>I was really 
keen that if you're going to publish a paper, we had to say why does it 
work right? So Sebastian went through and tried. You know removing all of 
those different contributions. I mentioned right. So what if we don't use 
gradual freezing? What if we don't use discriminative learning rates? What 
if, instead of discriminating rates, we use cosign annealing? What, if we 
don't do any pre training with Wikipedia what, if we don't do any fine 
tuning and then the really interesting one to me, was: what's the 
validation error rate on IMDB? If we only use a hundred training examples, 
this is 200 business 500 and you can see you know very. Interestingly. The 
the full version of this approach is nearly as accurate on just a hundred 
training. Examples like it's still very accurate versus for 20,000 training 
examples. We also, if your training from scratch on 100, it's like almost 
random, all right so kind of like it's what I expected. You know I've kind 
of said to Sebastian. I really think that this this is most beneficial when 
you don't have much data - and this is like - where fast a is most 
interested in contributing right, there's like small data regimes, small 
compute regimes and so forth, and so he did these studies to check. So I 
want to show you a couple of tricks as to how you can run these kinds.</p>

<p>The 
first trick is is something which I know you're all gon na find really 
handy. I know you're hoping annoyed when you're running something in a Jeep 
and a notebook, and you lose your internet connection for long enough, that 
it decides you've gone away and then your session disappears and you have 
to start it again from Spanish. Ok. So what do you do? There's a very 
simple cool thing called VNC, where, basically, you can install on your AWS 
instance or paper space or whatever X, Windows, a lightweight window 
manager, a VNC server, firefox a terminal and some fonts chuck these lines. 
At the end of your VNC x, startup configuration file and then run this 
command. It's now running a server where, if you now run for it's now 
running a server where you can then run the type VNC viewer or any VNC 
viewer on your computer. And you point it at your server right, but 
specifically what you do as you go. You use SSH port forwarding to port 
forward port five, nine one three to localhost five, nine one three right, 
and so then you connect to port five nine one three on on localhost. It 
will send it off to port five nine one, three on your server, which is the 
VNC port, because you said thirteen here and it will display an X Windows 
desktop and then you can. Click on the Linux start like button and click on 
Firefox and you now have Firefox, and so you can now run and you'll see 
here in Firefox. It says localhost because this Firefox is running on my 
AWS server right, and so you now run Firefox. You start your thing running 
and then you close your VNC viewer, remembering that Firefox is like 
displaying on this virtual VNC display, not in the real display, and so 
then later on that day you log back into VNC viewer and it flops up again. 
So it's like you're persistent desktop and it's shockingly fast.</p>

<p>It works 
really well. Okay, so there's trick number one and there's lots of 
different VNC servers and clients and whatever. But this one works fine for 
me, so there you go so you can see here. I connect to localhost five nine 
one three trick number two is to create Python scripts, but this is what we 
ended up doing so I ended up creating like a little Python script for 
Sebastian to kind of say this is the basic steps you need to do And now you 
need to create like different versions, certain thing else, and I suggested 
to him that he tried using this thing, called Google fire. What Google fire 
does is you create a function with shitloads of parameters right, and so 
these are all the things that Sebastian wanted to try doing. Different drop 
out amounts. Different learning rates. Do I use pre training or not? Do I 
use CLR or not? Do I use discriminative learning rate or not? Do I go 
backwards or not blah blah blah right? So you create a function and then 
you add something saying if 9 equals main fire dot fire and the function 
name, you do nothing else at all. You don't have to add any metadata, any 
docstrings anything at all, and you then call that script and automatically 
you now have a command. That's it right. So that's a super fantastic, easy 
way to run lots of different variations in a terminal - and this is like 
this is ends up being easier.</p>

<p>If you want to do lots of variations than 
using a notebook, because you can just like - have a bash script that tries 
all of them and spits them all out - you're fine inside the dl2 course 
directory. There's now something called IMDB scripts and I've put there all 
of the scripts that Sebastian and I used so you'll - see because we needed 
to like tokenize every or data set. We had to turn every dataset numerical 
eyes. Every data set. We had to train a language model on every data set. 
We had to train a classifier every data set. We have to do all of those 
things in a variety of different ways to compare them. We have a script for 
all those things, so you can check out and see all of the scripts that we 
used when you're doing a lot of scripts and stuff they've got different 
code all over the place. Eventually, it might get frustrating that you want 
to. You know you don't want a symlink you're, faster yo library again and 
again, but you probably don't want to pip install it because that version 
tends to be a little bit old. We move so fast. You want to use the current 
version in get if you say, pip install a dot from the fastai repos base. It 
does something quite neat which is basically creates a symlink, the faster 
a library to get. You know in your get installation right here inside your 
site packages directory your site packages directory is like your main. You 
know Python library, and so, if you do this, you can then access fastai 
from anywhere.</p>

<p>But every time you do get pull you've got the most recent 
version. One downside of this is that it installs any updated versions of 
packages from Kipp which can kind of confuse Condor a little bit. So 
another alternative here is just to Simla -- nk, the FASTA, a library to 
your site packages, library like that works just as well, and then you can 
use faster i'll again from anywhere and it's quite handy when you want to 
kind of run scripts. </p>

<h3>20. <a href="https://youtu.be/uv0gmrXSXVg?t=2h5m">02:05:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Examples of generated images</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>That use past AI from different directories under system. Okay, so one more 
thing before we, which is something you can try if you like, you, don't 
have to tokenize words. Instead of tokenizing words, you can tokenize what 
are called sub word units and so, for example, unsupervised for example, 
unsupervised quickly. Tokenized his on supervised tokenizer could be 
tokenized as token either right and then you could do the same thing. The 
language model that works on sub units, a classifier that works on sub word 
units, etc. So how well does that work? I started playing with it and with 
not too much playing. I was getting classification results that were nearly 
as good as using word level. Tokenization not quite as good, but nearly as 
good, I suspect, with more careful thinking and playing around. Maybe I 
could have got as good or better, but even if I couldn't, if you create a a 
sub word unit, wikitext model, then IMDB model language model and then 
classifier forwards and backwards, force upward units and then ensemble it 
with the forwards and backwards word level Ones, you should be able to beat 
us right, so here's an approach you may be able to beat our state if ya was 
on. Google has, as Sebastian told me about this particular project. Is 
great. Google has a project called sentence: peace which actually uses a 
neural net to figure out the optimal splitting up of words, and so you end 
up with a vocabulary of sub word units.</p>

<p>In my planning around, I found that 
creating a vocabulary of about 30,000 sub word units seems to be about 
optimal. So if you're interested there's something you can try, it's a bit 
of a pain to install it's C++. It doesn't have great error messages, but it 
will work like before it and if anybody tries to so I'm happy to drop them, 
get it working, there's been little if any experiments with ensemble abroad 
and word level stuff classification, and I do think it should be the Best 
approach: alright thanks, everybody have a great week and see you next 
Monday, [ Applause, ], </p>

<h3>21. <a href="https://youtu.be/uv0gmrXSXVg?t=2h9m15s">02:09:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Lesson 10 conclusion and assignments for Lesson 11</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>









  </body>
</html>
