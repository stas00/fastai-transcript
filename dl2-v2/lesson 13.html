<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 13: Image Enhancement</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://course.fast.ai/">fast.ai: Deep Learning Part 2 (v2) (2018)</a></p>
  <h1><a href="http://course.fast.ai/lessons/lesson13.html">Lesson 13: Image Enhancement</a></h1>
  <h2>Outline</h2>
<p>For the start of today’s lesson we’ll cover the CycleGAN, which is a breakthrough idea in GANs that allows us to generate images even where we don’t have direct (paired) training data. We’ll use it to turn horses into zebras, and visa versa; this may not be an application you need right now… but the basic idea is likely to be transferable to a wide range of very valuable applications. One of our students is already using it to create a new form of visual art.</p>

<p>But generative models (and many other techniques we’ve discussed) can cause harm just as easily as they can benefit society. So we spend some time today talking about data ethics. It’s a topic that really deserves its own whole course; whilst we can’t go into the detail we’d like in the time available, hopefully you’ll get a taste of some of the key issues, and ideas for where to learn more.</p>

<p>We finish today’s lesson by looking at style transfer, an interesting approach that allows us to change the style of images in whatever way we like. The approach requires us to optimize pixels, instead of weights, which is an interesting different way of looking at optimization.</p>

  <h2>Video Timelines and Transcript</h2>

<h3>1. <a href="https://youtu.be/-lx2shfA-5s?t=9s">00:00:10</a></h3>

<ul style="list-style-type: square;">

<li><b> <a href="http://fast.ai/">Fast.ai</a> student accepted into Google Brain Residency program</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Welcome to lesson 13, where we're going to be talking about image, 
enhancement and image enhancement, we'll cover things like this painting 
that you might be familiar with. However, you might not have noticed before 
that this painting actually has a picture of an eagle in it. The reason you 
may not have noticed that before is this painting actually didn't used to 
have an eagle in it. By the same token, actually on that first page, this 
painting did not used to have Captain America's shield on it either, and 
this painting did not used to have a clock in it either. This is a cool new 
paper. Actually that just came out a couple of days ago called deep, 
painterly harmonization and it uses almost exactly the technique. We're 
going to learn in this lesson with some minor tweaks. But you can see the 
basic idea is: take one picture pasted on top of another picture and then 
use some kind of approach to combine the two and the basic approach is 
something called a style transfer before we talk about that, though, I 
wanted to mention this Really cool contribution by William Horton, who 
added this stochastic weight averaging technique to the first library that 
is now all merged and ready to go and he's written a whole post about that, 
which I strongly recommend you check out, not just because stochastic 
weight averaging actually lets.</p>

<p>You get higher performance from your 
existing neural networks, with basically no extra work, it's as simple as 
adding these two parameters to your fit function, but also he's described 
his process of building this and how he tested it and how it contributed to 
the library. So I think it's interesting. You know if you're interested in 
doing something like this. I think William had not built this kind of 
library before so he describes how he did it. Another very cool 
contribution to the faster, a library is a new train phase API and I'm 
going to do something. I've never done before, which are actually going to 
present somebody else's notebook, and the reason I haven't done it before 
is because I haven't liked any notebooks enough to think they're worth 
presenting, but so has done a fantastic job here of not just creating this 
new API. But also creating a beautiful notebook describing what it is and 
how it works and so forth, and the background here is, as as you guys know, 
we've been trying to train networks faster, partly as part of this dawn 
bench competition and also for a reason that you'll Learn about next week - 
and I mentioned on the forums last week - it would be really handy for our 
experiments if we had an easier way to try out different learning rate 
schedules and stuff, and I basically laid out an API that I had in mind as 
it'd. Be really cool if somebody could write this because I'm going to bed 
now and I kind of need it by tomorrow, and so when I replied on the forum. 
Well, that sounds like a good challenge and by 24 hours later it was done 
and it's been super cool I wan na.</p>

<p>I want to take you through it because 
it's it's going to allow you to do research into things that nobody's tried 
before. So it's called the Train phase API and the easiest way to show it 
is to show an example of what it does, which is here here is a iteration 
against learning rate chart as you're familiar with seeing and it. This is 
one where we train for a while at the learning rate of 0.01, and then we 
train for a while a learning rate of 0.001. I actually wanted to create 
something very much like that learning rate chart because most people that 
trained imagenet use this stepwise approach and it's actually not something 
that's built into fastai, because it's not generally something we 
recommend, but in order to replicate existing papers, I wanted To do it the 
same way, and so rather than writing a number of fit -- fit -- fit -- calls 
with different learning rates, so it'd be nice to be able to basically say 
train for n at this learning rate and then M epochs. At that learning rate, 
and so here's how you do that you can say phases. So a phase is a period of 
training with you know particular optimizer parameters, and it consists of 
a number of training phase objects. A training phase objects is how many 
epochs to train for what optimization function, to use and what learning 
rate, amongst other things that we'll see, and so here you'll see the two 
training phases that you just saw on that graph.</p>

<p>So now, rather than 
calling were not fit, you say low n dot fit with an optimizer scheduler 
with these phases get op and then from there most of the things you pass in 
can just get sent across to the fit function as per usual. So most of the 
usual parameters will work fine, but in this case, generally speaking, 
actually we can just use these training phases and you'll see it fits in 
the usual way and then, when you say plot LR there it is alright and not 
only does it plot The learning rate - it also plots momentum and for each 
phase it tells you what optimizer it used. You can turn off the printing of 
the optimizers. You can turn off the printing of momentum x' and you can do 
other little. Things like a training phase could have an LR decay 
parameter, so here's a fixed learning rate and then a linear decay, 
learning rate and then a fixed learning rate, which gives us that picture - 
and this is like might be quite a good way to Train. Actually, because we 
know at high learning rates you get to kind of explore better and they're, 
not low learning rates, you get to fine-tune better and it's probably 
better to gradually slide between the two. So you know this actually isn't 
a bad approach. I suspect you can use other decay types, not as linear, so 
cosine. This probably </p>

<h3>2. <a href="https://youtu.be/-lx2shfA-5s?t=6m30s">00:06:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Cyclical Learning Rates for Training Neural Networks (another student’s paper)</b></li>

<li><b>&amp; updates on Style Transfer, GAN, and Mean Shift Clustering research papers</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Makes even more sense as a genuinely potentially useful and learning rate, 
an ailing shape exponential, which is super popular approach. Polynomial 
which isn't terribly popular but actually in the literature, works better 
than just about anything else, but seems to have been largely ignored. So 
polynomials could to be aware of and what surveillance done is he's given 
us the formula for each of these curves and so with a polynomial. You get 
to pick what polynomial to use. So here it is with a different size and I 
believe, a P of 0.9 is the one that I've seen really good results for fYI. 
If you don't give a couple of learning rates, when there's an LR decay, 
then it will decay all the way down to zero right and, as you can see, you 
can happily start the next cycle at a different point. So the cool thing is 
now. We can replicate all of our existing schedules using nothing, but 
these training phases. So here's a function called phases. S JDR, which 
does SGD, are using the new training phase api, and so you can see if he 
runs this schedule and here's what it looks like. But he's even done the 
little trick. I have where you're training a little really low learning 
rate just for a little bit and then pop up and do a few cycles and the 
cycles are increasing in length and that's all done in a single in a single 
function. So the new one cycle we can now implement with again a single 
little function, alright, and so, if we run if we fit with that, we get 
this triangle all followed by a little flatter bit and the momentum is a 
cool thing.</p>

<p>The momentum has a momentum, decay right and then here we've 
got a fixed momentum at the end. So it's doing the momentum and the 
learning rate at the same time. So something that I haven't tried yet, but 
I think would be really interesting - is to use he's calling a differential 
learning rates. We've changed the name now to discriminative learning rates 
to use oops. Yes, what fix it? Just scream inactive learning race. So a 
combination of discriminative learning rates and one cycle - no one's tried 
yet so that would be really interesting. There's actually a the only paper 
I've come across, which has discriminative learning rates is called uses, 
something called Lars lar s, and it was used to Train imagenet with very, 
very large batch sizes by basically looking at the ratio between the 
gradient and the mean at H. Layer and using that to change the learning 
rate of each layer automatically and they found that they could use much 
larger batch sizes. That's the only other place. I've seen this kind of 
approach used, but there's lots of interesting things. You could try with 
combining discriminative learning rates and different interesting 
schedules, so you can now write your own LR finder of different types 
specifically because there's now this stop div parameter, which basically 
means that it'll, you know, use whatever schedule you asked for, but when 
the loss Gets too bad at all stop training, so here's one with no learning 
rate versus loss and you can see it stops itself automatically.</p>

<p>One useful 
thing that's been added. Is the linear parameter to the plot function? If 
you use linear schedule, rather than an exponential schedule and your 
learning rate finder, which is a good idea, if you've kind of fine-tuned 
into roughly the right area, then you can use linear to find exactly the 
right area. And then you probably want to plot it. With a linear scale, so 
that's why you can also pass linear to plot now as well. You can change the 
optimizer HBase and that's more important than you might imagine, because 
actually, the current state-of-the-art for training on really large batch 
sizes really quickly for imagenet actually starts with rmsprop for the 
first bit, and then they switch to SGD for the second vid, and So that 
could be something interesting to experiment more with is like because at 
least one paper has now shown that that can work well and again, it's 
something that isn't well appreciated as yet, and then the bit I find most 
interesting is you can change your data And why would we want to change our 
data? Because you remember from lessons 1 and 2? You could use small images 
at the start and later bigger images later, and the theory is. The theory 
is that you could use that to kind of train the first bit more quickly with 
smaller images and remember, if you like, have half the height and half the 
width and you've got a quarter of the activations, basically every layer. 
So it can be a lot faster and it might even generalize better.</p>

<p>So you can 
now create a couple of different, for example, it's because he's got 28 and 
then 32 sized images. This is just so far 10, so there's only so much you 
can do and then, if you pass in an array of data in this data list 
parameter when you call fit pop shared it'll use a different data set for 
each face. So that's really cool because we can use that now like we could 
use that in our dorm bench entries and see what happens when we actually 
increase the size with very little code. So what happens when we do that? 
Well, the answer is here in Dorn bench: training on imagenet, and you can 
see here that Google is one this with half an hour on a cluster of TP use. 
The best non cluster of TPU result is fast: AI plus students under three 
hours, beating out Intel on 128 computers. Where else we ran on a single 
computer, we also beat Google running on a TPU, so using </p>

<h3>3. <a href="https://youtu.be/-lx2shfA-5s?t=13m45s">00:13:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Tiramisu: combining Mean Shitft Clustering and Approximate Nearest Neighbors</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>This approach we've shown the fastest GPU result. The fastest single 
machine result the fastest, publicly available infrastructure result. These 
TPU pods, you can't use unless your Google and the cost is tiny like this 
Intel, one cost them $ 1,200 worth of compute. They haven't even written it 
here, but that's what you get a few user. Sorry, 128 computers in parallel, 
each one with 36 cause each one with 140 gig compared to our single AWS 
instance. So this is, you know, a kind of a breakthrough in in what we can 
do like the idea that we can train imagenet on a single, publicly available 
machine and this $ 72. By the way it was actually $ 25 because we used a 
spot instance. So one of our students, Andrew Shaw, built this whole system 
to allow us to throw a whole bunch of spod instance. Experiments up and run 
them simultaneously and pretty much automatically, but dawn binge doesn't 
quote the actual number we used. So it's actually 25 bucks, not 72 bucks. 
So this data list idea is super important and helpful, and so our sci-fi 10 
results also now up there officially - and you might remember, the previous 
best was a bit over an hour and the trick here was using one cycle. 
Basically, so all this stuff, that's in silver, has training phase. Api is 
really all the stuff that we used to get these top results and really cool 
another fastai student, who goes by the name here. Bkj, has has taken that 
and done his own version.</p>

<p>He took a resonant 18 and added the concat 
polling that you might remember that we learnt about on top and used leslie 
cycles once leslie smith's one cycle and so he's got on the leaderboard. So 
all the top three first day i students, which is wonderful and same for 
cost, the top three and you can see paper space, so brett ran this on paper 
space and got the the cheapest result. Just ahead of dkj been his name is, 
i believe, okay, so so i think you can see like a lot of the kind of 
interesting opportunities at the moment for the training stuff more quickly 
and cheaply, you're all about kind of the learning rate annealing and size. 
Annealing and like training with different parameters at different times, 
and I still think we buddies scratching the surface - I think we can go a 
lot faster and a lot cheaper and that's really helpful for people. You know 
in resource constrained environments, which is basically everybody except 
Google. Maybe Facebook architectures interesting as well, though, and one 
of the things we looked at last week was just like creating a simpler 
architecture which is basically state of the art. You know like the really 
basic kind of dark net architecture, but there's a piece of architecture. 
We we haven't talked about which is necessary to understand the inception 
network. The inception network is actually pretty interesting because they 
use some tricks to to actually make things more efficient and we're not 
currently using these tricks, and I kind of feel that maybe we should try. 
It, and so this is the the most interesting most successful inception 
network is their inception resident to network and most of the blocks in 
that looks something like this and it looks a lot like a standard ResNet 
block in that there's an identity connection here and then There's a conv 
confirmation is so a one by one.</p>

<p>Conf is simply saying for each grid cell. 
In your input, you've got a basically it's a vector, write a 1x1 by number 
of filters. Tensor is basically a vector right so for each grid cell. In 
your input, you're just doing a dot product with that tensor right and 
then, of course, it's going to be one of those vectors for each of the 
hundred and ninety two activations we're creating soon, basically do 192 
dot products with grid cell 1, 1 and then 192 with good 0, 1, 2 or 1, 3 and 
so forth, and so you'll end up with something which has got the same grid 
size as the input and 192 channels in the output. So that's a really good 
way to you know: either reduce the dimensionality or increase the 
dimensionality of an input without changing the grid size. That's normally 
what we use 1x1 cons for. So here we've got a 1x1 conf and then we've got 
another one-by-one conf and then they add it together and then there's a 
third path, and this third path is not added this third path - it's not 
actually explicitly mentioned, but it's concatenated right and so actually 
there Is a form of ResNet which is basically identical to resonate, but we 
don't do plus we do concat right and that's called a dense net. Alright. So 
it's just a resonate where we do concat instead of plus and that's an 
interesting approach, because then the kind of the identity path is 
literally being copied right. So you kind of get that that that flow 
through all the way through and so as we'll see.</p>

<p>Next week, that tends to 
be good for, like segmentation and stuff, like that, where you really want 
to kind of keep the original pixels and the first layer of pixels and the 
second layer of pixels and touched so concatenate. Another than adding 
branches is, is a very useful thing to do, and so here we're concatenate in 
this branch and this this branch is doing something interesting, which is 
it's doing. First of all, the 1x1 con and then a 1 by 7 and then a seven by 
one. So what's going on there so what's going on, there is basically what 
we really want to do is do a seven by seven conch. The reason we want to do 
a seven by seven con is that if you've got multiple paths, each of which 
has different kernel sizes, then it's able to look at. You know different 
amounts of the image, and so like the original inception network had like a 
1x1 or 3x3 a 5x5 seven by seven kind of getting concatenated in together. 
Something like that, and so, if we can have a seven by seven filter, then 
we get to kind of look at a lot of the image at once and create a really 
rich representation and so actually the stem of the inception network. That 
is the the first few layers of the inception network actually also used. 
You know this kind of seven by seven cond, because you start out with this 
224 by 224 by three, and you want to turn it into something: that's like 
112 by 112 by 64. So, by using a 7x7 Conniff, you can get a lot of 
information in each one of those outputs to get those 64 filters.</p>

<p>But the 
problem is that 7x7 conv is a lot of work. You've got 49 kernel, values, 2 
x 49 inputs for every input pixel across every channel, so the compute is 
crazy. </p>

<h3>4. <a href="https://youtu.be/-lx2shfA-5s?t=22m15s">00:22:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Facebook AI Similarity Search (FAISS)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>You know you can kind of get away with it, maybe for the very first layer 
and in fact the very first layer, the very first con was reza net - is a 
seven by seven con. But i'm not so for inception for inception. They don't 
do a seven by seven comma. Instead, they do a one by seven, followed by a 
seven by one and so to explain the basic idea of the inception networks or 
all the different versions of it. That you have a number of separate paths 
which have different convolution widths in this case conceptually. The idea 
is, this is a one-by-one convolution with, and this is going to be a seven 
convolution with and so they're looking at different amounts of data, and 
then we combine them together, but we we don't want to have a seven by 
seven plunge throughout the network, Because it's just too computationally 
expensive, but if you think about it, if we've got some input coming in 
right and we have some big filter that we want and it's it's too big to 
deal with and what could we do right? So let's say, let's just to make it a 
little bit less drawing that's two five by five. What we can do is to 
create two filters, one which is 1 by 5 1, which is 5 by 1 or 7 or whatever 
on line. So we take our activations, the previous layer and we put it 
through the 1 by 5. We take the activations out of that and put it through 
the 5 by 1 and something comes out. The other end.</p>

<p>Now what comes out the 
other end? Well, rather than thinking of it as first of all, we take the 
activations, then we put it through the 5 by 1. Then we put it through the 
5, but then we put it through the 1 by 5, 1 by 5, then the 5 by 1. What if, 
instead, we think of these 2 operations together and say what is a 5 by 1 
dot production of one by five dot product do together right and effectively 
right? You could take a 1 by 5 and a 5 by 1, and the outer product of that 
is going to give you a 5 by 5 right now that you can't create any possible 
5 by 5 matrix by taking that product right. But there's a lot of 5 by 5 
matrices that you can create, and so the basic idea here is: you know when 
you think about the order of operations and I'm going to go into the detail 
of this if you're interested in more of the theory here, You should check 
out Rachel's numerical linear algebra course, which is basically a whole 
course about this stuff, but conceptually the idea is that very often the 
the computation you want to do is actually more simple than an entire 5x5 
convolution, very often that the term we use In linear algebra is that 
there's some lower rank approximation, in other words, that the 1 by 5 and 
the 5 by 1, combined together that 5 by 5 matrix, is nearly as good as the 
5 by 5 matrix.</p>

<p>You really, ideally would have computed if you were able to, 
and so this is very often the case in practice right just because the 
nature of kind of the real world is that the real world tends not to be. Is 
you know it tends to have more structure? You know than kind of randomness, 
so the cool thing is, if we replace our seven by if we replace our seven by 
seven conf, where the one by seven and a seven by one right, then this has 
basically for each cell. It's got 14 by input channel by output, Channel 
dot products to do, whereas this one has 49 to do okay. So it's just going 
to be a lot faster and we have to hope that it's going to be nearly as 
good. It's certainly capturing as much widths of information by definition. 
So if you're in student learning more about this specifically in a deep 
learning area, you can google for factored convolutions. The idea was come 
up with three or four years ago. Now it's probably been around for long 
river. That was when I first saw it and yeah it turned out to work really 
well and the inception network uses it quite widely. They actually use it 
in their in their stem. It's it's interesting! Actually, we've talked 
before about how we tend to kind of add-on. We tend to say like this: it's 
main like backbone. You know like when we have ResNet 34, for example, we 
kind of say: oh, this is main backbone, which is all of the convolutions, 
and then we've talked about how we can add on to it a custom head right, 
and that tends to be like a mac.</p>

<p>Spalling layer and a fully connected 
layers and whether the you know it's actually kind of better to talk about 
the the backbone is containing kind of two pieces. One is the stem and then 
the other is kind of the main backbone. And the reason is that the thing 
that's coming in remember: it's only got three </p>

<h3>5. <a href="https://youtu.be/-lx2shfA-5s?t=28m15s">00:28:15</a></h3>

<ul style="list-style-type: square;">

<li><b> The BiLSTM Hegemony</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Channels, and so we want some sequence of operations, it's going to expand 
that out into something richer generally, something like 64 channels and so 
in ResNet. The stem is just super simple. It's a seven by seven cons 
straight to one followed by a Strad to Emax port yeah. I think that's it. 
If memory serves correctly an inception, they have a much more complex stem 
with multiple paths getting combined and cabin aided, including factoid 
comms, as one by seven and seven by one and now I'm kind of interested in 
what would happen if you stopped like a resident standard, Resonate on top 
of an inception instead, for instance like I think that would be a really 
interesting thing to try, because, like an inception, stem is kind of quite 
a carefully engineered thing, and this thing of like how do you take your 
three channel input and turn It into something richer seems really 
important, and all of that work seems to have got thrown away for ResNet. 
We like ResNet, it works really well, but what if we put, you know or a 
dent in it? What if we put the dense net backbone on top of an inception 
stem or what, if we replaced the seven by seven cons with a 1 by 7 and 7 by 
1, factored conf? You know standard business, I don't know, there's lots of 
things we could try and I think it'd be really interesting. So there's some 
more thoughts about potential research directions. Ok, so that was kind of 
my little bunch of random stuff section.</p>

<p>Moving a little bit closer to the 
actual main topic of this, which is what I used image enhancement, I'm 
going to talk about a new paper briefly, because it's it really connects 
what I just discussed with what we're going to discuss next and the new 
paper. Well, it's not that new. Is it no it's a year old, it's a paper on 
progressive dance which came from Nvidia, and the progressive Ganz paper is 
really neat. It basically sorry Rachel. Yes, we have a question. One-By-One 
Kampf is usually called a network within a network. In the literature, what 
is the intuition of such a name know? Networking network is more than just 
a one by one time. It's part of it. I am, and we don't. I don't think 
there's any particular reason to look at that said, I'm aware of okay, so 
the the progressive can basically takes this idea of actually gradually 
increasing the image size. It's the only other direction, I'm aware of 
where people have actually gradually increase the image size, and it kind 
of surprises me, because this paper is actually very popular and very well 
known and very well liked. And yet people haven't taken the basic idea of 
gradually increasing the image size and use it anywhere else, which shows 
you, the general level of creativity. You can expect to find in the deep 
learning research community.</p>

<p>Perhaps so they start with four by four, like 
they really go back start with a four by four again, like literally 
they're, trying to create like replicate four by four pixel and then eight 
by eight and so here's the 8 by 8 pixels. This is the celeb. A data set so 
we're trying to recreate pictures of celebrities, and then they go 65, 16 
and then 32 and then 64 and then 128 and then 256 and one of the really 
nifty things they do is that as they increase size, they also add more 
layers To the network right, which kind of makes sense right, because if 
you're doing a more of a resin, Ettie type thing, you know then you're 
spitting out something which hopefully makes sense at each grid cell size. 
And so you should be able to kind of layer stuff. On top - and they do 
another nifty thing where they kind of add a skip connection when they do 
that and they gradually change the linear, interpolation parameter that 
moves it more and more away from the old 4x4 Network and towards the new 
8x8 Network. And then, once this totally moved it across they throw away 
that extra connection. So it's it the details, don't matter too much, but 
it it uses the basic ideas. We've talked about gradually increasing the 
image size, it's kind of skip, connections and stuff. But it's a great 
paper to study because a you know: it's like one of these rare things where 
they've like good engineers, actually built something that just works in a 
really sensible way. Now it's not surprising.</p>

<p>This actually comes from 
Nvidia themselves right so in video, don't do a lot of papers and it's 
interesting that when they do they build something, that's so thoroughly 
practical and sensible, and so I think it's a great paper to study. You 
know if you want to kind of like put together lots of the different things 
we've learned. You know, and there aren't many re-implementation of this. 
So like it's an interesting thing, you know to project and you maybe you 
could build on and find something else. So here's what happens next, we 
eventually go up to 102 4 by 1 or 2, 4 and you'll see that the images are 
not only getting higher resolution but they're getting better, and so, when 
I prove 1 or 2 4 by 184. I'm going to see if you can guess which one of the 
next page is fake, they're, all fake. That's the next stage right you go up 
up up up up up up and then BOOM. Okay, so like dance and stuff they're 
getting crazy - and some of you may have seen this during the week yeah, so 
this video just came out and it's a speech by Barack Obama and let's check 
it out, so my Jordan Peele. This is a dangerous time. Moving forward, we 
need to be more vigilant with what we trust from you nourish. It's time we 
need to rely on trusted news sources. They sound basic, but how we before 
so as you can </p>

<h3>6. <a href="https://youtu.be/-lx2shfA-5s?t=35m">00:35:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Implementing the BiLSTM, and Grammar as a Foreign Language (research)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>See they've used this kind of technology to literally move Obama's face in 
the way that Jordan peels face was moving and, like you, basically have all 
the techniques you need now to do that. So is that a good idea? So this is 
the bit where we talk about what's most important, which is like now that 
we can like do all this stuff. What should we be doing, and how do we think 
about that and the Tod our version is, I actually don't know recently. A 
lot of you saw the founders of this, the spacy prodigy folks founders of 
explosion, a I had to to talk and Matthew and Ennis. I went to dinner with 
them afterwards and we basically spent the entire evening talking debating 
arguing about you know what does it mean? They're companies like ours, of 
building tools that are democratizing access to tools that can be used in 
harmful ways, and you know they're incredibly thoughtful people and we, I 
wouldn't say we didn't agree. We just couldn't. We just couldn't come to a 
conclusion ourselves. So I'm just going to lay out some of the questions 
and point to some of the research and when I say research, most of the 
actual literature review and putting this together was done by Rachel. So 
thanks Rachel. Let me start by saying the models we build are often pretty 
shitty in ways which are not immediately apparent, and you won't know how 
shitty they are unless the people that are building them with you, a range 
of people and the people that are using them with You or a range of people 
so, for example, a couple of wonderful research is Chemnitz at Stanford and 
rest joy.</p>

<p>Is she oh she's at Microsoft? Now she wasn't Stanford, okay, joy! 
Is it from PhD from MIT, so joy and Timna did this really interesting 
research where they looked at some basically off-the-shelf face, recognizes 
one from face plus plus, which is a huge Chinese company, IBM's and 
Microsoft's, and they looked for a range of different face types? I'm, 
generally speaking, you know the Microsoft one in particular was incredibly 
accurate and last the face type happened to be dark-skinned. When suddenly 
it went, you know 25 times worse, you know, got it wrong. Nearly half the 
time and for somebody to a big company like this, to release a product 
that, for like a very, very large percentage of the world, basically 
doesn't work, is more than a technical failure right. It's a really deep 
failure of understanding. What kind of team needs to be used to create such 
a technology and to test such a technology, or even an understanding of who 
your customers are yeah? Some of your customers have dark skin, yes, 
Rachel. I was also gon na add that the classifier is all did worse on women 
than on men, shocking, yeah yeah as funny like actually Rachel tweeted 
about something like this the other day, and some some guy was like. What's 
this all about, you know like what are you saying that we, like you know, 
don't you know about like people, people made cards for a long time.</p>

<p>You 
saying you need women to make cars and Rachel pointed out like well. 
Actually, yes, for most of the history of car safety, women in cars have 
been far far more at risk of death than men in cars, because the men 
created male looking feeling sized crash-test dummies, and so car safety 
was literally not tested on women, size bodies. So the fact you know, like 
you know, shitty product management with a total failure of diversity and 
understanding is not new to our field, and I would say that was comparing 
impacts of similar strength, men and women. Mm-Hmm yeah. I don't know why, 
like whenever you say something nice on Twitter like Rachel, has to say 
this because anytime, you say something like some Twitter there's, like 10 
people, who'll be like all. You have to compare all these other things is, 
if, like we didn't know that so yeah I mean yeah other things. You know our 
very best. Most famous systems do like Microsoft's face recognizer or 
Google's language translator. You turn she is a doctor. He is a nurse into 
Turkish, and quite correctly, both the pronouns become. Oh, because there's 
no gendered pronouns in Turkish so go the other direction. I'll be a 
doctor. I don't know how to say that one for Christmas and what does it get 
turned into? He is a doctor. She is a nurse so, like we've got these kind 
of like biases, built into tools that we're all using every day and again 
people they go.</p>

<p>It's just showing us what's in the world and well: okay, 
there's lots of problems with that basic assertion, but, as you know, 
machine learning, algorithms, love to generalize right and so because they 
love to generalize. This is one of the cool things about you guys knowing 
the technical details now, because they love to generalize when you see 
something like 60 % of people cooking our in the pictures, they used to 
build this model, and then you actually run the model on a Separate set of 
pictures, then 84 percent of the people they choose as cooking women rather 
than the correct 67 percent, but which is like a really understandable 
thing for an algorithm to do as it took a biased input and created a more 
biased output. Because you know for this particular loss function, you know 
that's kind of where it ended up, and this is a really common kind of a 
really common kind of model. Amplification, okay, so this stuff matters 
right. It matters in ways more than just you know, awkward translations or, 
like you know, black people's photos not being classified correctly or you 
know, maybe there's some there's some wins too, as well like you know, 
horrifying surveillance everywhere and maybe won't work on black people 
right. But yes or it'll be even worse because it's horrifying surveillance 
and it's flat-out racist and wrong.</p>

<p>Okay, that, sir, but but let's go 
deeper right like what hat like that, the four always say about human 
failings, humans such generally, you know that there's there's a long 
history of civilization and societies, creating kind of layers of human 
judgment which avoid hopefully the most horrible Things happening, and 
sometimes companies which love technology think let's throw away the humans 
and replace them with technology like Facebook did right. So, let's let two 
or three years ago, a couple years ago, Facebook literally got rid of their 
human editors like this is in the news at the time and they were replaced 
with algorithms and so now, as algorithms, that put all the stuff on your 
on your Newsfeed and human editors right at the loop. What happened next 
many things happen next, one of which was a massive horrifying genocide. 
Menma babies getting torn out of their mothers are right under fires, mass 
rape, murder and an entire people exiled from their homeland. Okay, I'm not 
gon na say that was because Facebook did this, but what I will say is that 
when the leaders of this horrifying project are interviewed, they regularly 
talk about how everything they learnt about the disgusting animal behaviors 
of Rangers that need to be thrown off. The earth they learnt from Facebook 
right because the algorithms just want to feed you more stuff.</p>

<p>That gets 
you clicking, and so, if you get told these people that don't look like you 
and you don't know a bad people and here's what the story's about the bad 
people and then you start clicking on them and then they feed you more of 
those things And next thing you know you have this like extraordinary cycle 
and people have been studying this right. So, for example, some we've been 
told a few times. People click on our fastai videos and then the next thing 
recommended to them is like conspiracy, theory, videos from Alex Jones, and 
then you know continues there because you know humans, click on things that 
shocked us and surprise us and horrify us right and so at So many levels 
you know this decision has had extraordinary consequences which we're only 
beginning to understand, and again this is not to say this particular 
consequence is because of this one thing, but to say it's entirely 
unrelated would be clearly ignoring all of the evidence and information 
That we have right so this is really kind of the key takeaway is to think 
like what are you building and how could it be used right? So lots and lots 
of effort now being put into face detection, including in our course right. 
We've been spending a lot of time, thinking about how to recognize stuff 
and where and there's lots of good reasons to want. </p>

<h3>7. <a href="https://youtu.be/-lx2shfA-5s?t=45m30s">00:45:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Reminder on how RNN’s work from Lesson #5 (Part 1)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>To be good at that, you know for improving crop yields and agriculture for 
improving diagnostic and treatment, planning and medicine for improving 
your logo, sorting, robot system, whatever right, but it's also being 
widely used in in surveillance and propaganda and disinformation. And you 
know again, it's like the question is like well. What do I do about that? I 
don't exactly know right, but it's just definitely at least important to be 
thinking about it. Talking about it, and sometimes you can do really good 
things. For example, meetup comm did something which I would put in the 
category of really good thing, which is they recognized early, a potential 
problem, which is that more men who are tending to go to their meet us and 
that was causing their collaborative filtering systems, which you're All 
familiar building now to recommend more technical content to men, and that 
was causing more men to go to more technical content, which was causing the 
recommendation systems to suggest more technical content to men right and 
this kind of runaway feedback. Loop is extremely common. When we interface 
the algorithm and the human together, so what it made up, do they 
intentionally made the decision to recommend more technical content to 
women right? Not because of some. You know highfalutin idea about how the 
world should be, but just because </p>

<h3>8. <a href="https://youtu.be/-lx2shfA-5s?t=47m20s">00:47:20</a></h3>

<ul style="list-style-type: square;">

<li><b> Why Attentional Models use “such” a simple architecture</b></li>

<li><b>&amp; “Tacotron: a Fully End-To-End Text-To-Speech Synthesis Model” (research)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>That makes sense right that the runaway feedback loop was a bug right. 
There are women that want to go to tech meetups, but when you turn up for a 
tech make up and it's all men and you don't go, and then it recommends more 
to men and so on and so forth right. So so I made up made us a really 
strong product management decision here, which was too not do what the 
algorithm said to do. Unfortunately, this is rare, most of these runaway 
feedback loops, for example, in predictive policing, where algorithms tell 
policemen where to go, which very often is more black neighborhoods, which 
end up crawling with more policemen, which leads to more arrests, which has 
assistance job more policemen to go. To more black, neighborhoods and so 
forth, so this problem of algorithmic bias is now very widespread and, as 
algorithms become more and more widely used for specific policy decisions, 
judicial decisions, day-to-day decisions about just who to give what offer 
to this just keeps becoming a bigger problem. Right and so, and some of 
them are really things that the people involved in the product management 
decision should have seen at the very start, didn't make sense and were 
unreasonable under any definition of the term, for example this stuff that 
I'd gone pointed out. These were questions that were used to decide, which 
was the sentencing guidelines. This software is used for both pretrial so 
who it was required to post bail.</p>

<p>So these are people that haven't even 
been convicted, as well as for sentencing and for who gets parole, and this 
was upheld by the Wisconsin Supreme Court last year. Despite all the flaws, 
okay, so whether you have to stay in jail because you can't pay the bail 
and how long your sentences for and how long you stay in jail for depends 
on what your father did. Whether your parents stayed married, who your 
friends are and where you live right now turns out these algorithms are 
actually terribly terribly bad, so some recent analysis showed that 
they're, basically worse than chance, but even the the company's building 
them were confident on these were statistically accurate. Correlations does 
anybody imagine there's a world where it makes sense to decide like what 
happens to you, based on what your dad did. You know so a lot of this 
stuff. You </p>

<h3>9. <a href="https://youtu.be/-lx2shfA-5s?t=50m15s">00:50:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Continuing on Spelling_bee_RNN notebook (Attention Model), from Lesson 12</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Know at the basic level is obviously unreasonable and a lot of it just 
fails in these ways that you can see empirically that these kind of runaway 
feedback loops must have happened, and these over generalizations must have 
happened. You know, for example, these are the kind of cross tabs that 
anybody working in these fields in any field is using. Algorithm should be 
preparing right, so prediction of likelihood of reoffending for black 
versus white defendants, like we can just calculate this very simply of the 
people that were labeled high-risk but didn't reopen. There were twenty 
three point: five percent white, but about twice that african-american. 
Where else, those that were labeled law risk but did riaf end, was like 
half the white people and only twenty percent of the african-american right 
so like this is the kind of stuff. We're least you know if you're taking 
the technologies we've been talking about and putting the production in 
some kind of in any way right or building a an API for other people or 
providing training for people or or whatever right. Then at least make sure 
that the that what you're doing can be tracked in a way that people know if 
some things you know people know what's going on - that's at least they're 
informed. Okay, I think it's a mistake, in my opinion, to assume that 
people are our evil. You know and and trying to break society right like I 
think I would. I prefer to start with an assumption of like okay.</p>

<p>If people 
are doing dumb stuff, it's because they don't all right, so you know at 
least make sure that they have this information, and I find very few ml 
practitioners thinking about what is the information they should be 
presenting in their interface. You know and then often I'll talk to data 
scientists, who will kind of say, like oh the stuff, I'm working one 
doesn't have a societal impact. It's like really like, like a number of 
people who think that what they're doing is entirely pointless come on. You 
know. Otherwise, you think people are paying you to do it for a reason. 
That's going to impact people in some way. Okay, so think about what that 
is. The other thing I know is a lot of people involved here are hiring 
people, and so, if you're hiring people, you know, I guess you're all very 
familiar with the first day. I philosophy now, which is the basic premise 
that - and I think it comes back to this idea - that I don't think about 
people on the whole were evil. I think they need to be informed and have 
tools right. So we're trying to have us give as many people the tools as 
possible that they need and particularly we're trying to put those tools in 
the hand of a more the hands of a more diverse range of people. So if 
you're involved in hiring decisions, perhaps you can keep this kind of 
philosophy in mind as well, that if you're, if you're, not just hiring a 
wider range of people, but also promoting a wider range of people and 
providing like really appropriate Career Management for a Wider range of 
people well, apart from anything else, your company will do better.</p>

<p>It 
actually turns out that more diverse teams are more creative and tend to 
solve problems more quickly and better than most diverse teams, but also 
you know you might avoid these kind of awful screw-ups, which you know that 
one level are bad for the world and another Level, if you ever get found 
out, they can also destroy your company. Also, they can destroy you or at 
least make you look pretty bad in history. A couple of examples - one is, 
you know, going right back to the Second World War IBM basically provided 
all of the infrastructure necessary to track the Holocaust, so they had a 
these are the forms that they used and they say had different code. For you 
know, Jews were Asian gypsies for 12 death and the gas chambers were six 
and they all went on these punch cards. You can go and look at these punch 
cards and museums now, and this has actually been reviewed by a Swiss judge 
who said that IBM's technical assistance facilitated the task of the Nazis 
and the commission of the crimes against humanity. And you know it's 
interesting to read back the history. You know from these times to see like 
what was going through the minds of people at IBM at that time, and you 
know what what was clearly going through. The minds was like the 
opportunity to show technical superiority, the opportunity that I test out 
their you know their new systems, it's it's it's you know and the course 
the extraordinary amount of money that they were making um you know and 
when, when you do something which At some point down, the line turns out to 
be a problem, even if you were told to do it that can turn out to be a 
problem for you.</p>

<p>Personally, for example, you will remember the diesel 
emissions scandal in VW know. Who is the one guy that went to jail? It was 
the engineer right just doing his job okay. So if all of this stuff about 
actually you know not sucking up the world, isn't enough to convince you, 
they can up your life too right. So if you, if you do something that turns 
out to cause problems, even though somebody told you to do it, you can 
absolutely be held criminally responsible and you're. Certainly like look 
at the, but there's no Cogan. You know, I think a lot of people now know 
the name Aleksandr Cogan. He was the guy that handed over the Cambridge 
analytic er data he's a Cambridge academic, now, a very famous Cambridge 
academic, the world over for doing his part to destroy the foundations of 
democracy. Right so you know this is probably not how we want to go down in 
history all right. So let's have a break before we do read short question 
on a different topic. Yes, in one of your tweets, you said, drop out is 
patented. I think this is about wavenet patent from Google. What does it 
mean? Can you please share more insight on this subject? Does it mean that 
will help to pay to use drop out in the future? Yeah? Okay, good question: 
let's talk about that after the break and so let's come back at 7:40. The 
question before the break was about patents.</p>

<p>What does it mean? So so I 
guess the reasons coming up was because I wrote a tweet this week, which I 
think was like three words and said: drop out is patented. The patent 
holders is Geoffrey Hinton. So what isn't that great inventions all of our 
patents right? And so you know my answer is no. You know. Patents have gone 
wildly crazy. The amount of things that are patentable that we talk about 
every week would be dozens like it's so easy to come up with a little 
tweak, and then you know if you turn that into a patent, to stop everybody 
from using that little tweak for the next 14 years - and you end up with a 
situation we have now where everything is patented in 50 different ways. 
And so then you get these patent trolls, who have made a very, very good 
business out of looking better, basically buying lots of shitty little 
patents and then suing anybody who accidentally turned out. Did that thing 
you know like putting rounded corners on buttons. You know so who was it? 
Oh, this there's Apple </p>

<h3>10. <a href="https://youtu.be/-lx2shfA-5s?t=58m40s">00:58:40</a></h3>

<ul style="list-style-type: square;">

<li><b> Building the Attention Layer and the ‘attention_wrapper.py’ walk-through</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Suits Samsung or something I don't remember so yeah, so what does it mean 
for us that a lot of stuff is patented in deep learning? I don't know it's 
like one theory like a lot of the one of the main people doing. This is 
Google and people from Google who replied to this patent tend to assume 
that on Google's doing it because they wanted to have it defensively. So if 
somebody Sue's them they'll be like don't sue us we'll see you back, 
because we have all these patents. The problem is that, as far as I know, 
they haven't signed, what's called a defensive patent pledge. So, 
basically, you can sign a legally binding document that says our patent 
portfolio will only be used in defense and not offense, and even if you 
believe, all the management of Google would never turn into a patent. Troll 
you've got to remember that. You know management changes right and like to 
give a specific example. I know the you know the somewhat recent CFO of 
Google, you know has a much more. You know kind of aggressive stance 
towards the PNL and I don't know - maybe maybe she might decide that they 
should start monetizing their patents or maybe the you know the group that 
made that patent might get spun off and then sold to another company. That 
might end up in private equity hands and decide to monetize the patents or 
whatever like so, I think it's a problem.</p>

<p>There has been a big shift 
legally recently away from software patents actually having any legal 
standing. So it's possible that these will all end up thrown out of court, 
but you know the reality. Is that anything, but a big company is unlikely 
to have the financial ability to defend themselves against one of these 
huge patent trolls. So I think it's a problem. I don't know like it you you 
can't you can't avoid using patented stuff. If you write code like most, I 
wouldn't be surprised if most lines of code you write, have patents on 
them. So actually, funnily enough, the best thing to do is not to study the 
patents, because if you do and you infringe knowingly, then it's the 
penalties are worse. So the best thing to do is to like put your hands in 
your ears, sing, a song. You know and get back to work so that thing about 
it said about dropouts patented. Forget I said that you, don't you don't 
know that he's given that bit? Okay, this is super fun artistic style, 
we're gon na kind of go a bit retro here, because this is actually the kind 
of original artistic style paper and there's been a lot of updates to it. A 
lot of different approaches - and I actually think kind of in many ways the 
original is the best we're going to look at some of the newer approaches as 
well that I actually think the original is a terrific way to do it, even 
with everything that's gone Since, let's just jump to the code, so this is 
the style transfer, no four.</p>

<p>So the idea here is that we want to take a 
photo. We've got to take a photo of this bird and we want to create a 
painting that looks like van Gogh. Painted the picture of the bird thank 
off then go quite a bit of the stuff that I'm doing by the way uses an 
image net. You don't have to download the whole of image net for any of the 
things I'm doing, there's an image net sample on file start fast. Today I 
slash data which has like I don't know a couple of gig, and it should be 
plenty good enough for everything. We're doing if you want to get really 
great results, you can grab image net. You can download it from cab all the 
I'm Carol. It's the localization. Competition actually contains all of the 
classification data as well all right so, and you know if you've got room. 
It's good to have a copy of imagenet because it comes in handy all the 
time. So I just grab the bird out of my imagenet folder and there is my 
Buddha and so what I'm going to do is I'm going to start with this picture 
and I'm going to try and make it more and more. Like a picture of this bird 
painted by Van Gogh and the way I do, that is actually very simple, you're 
all familiar with it, we will create a loss function which we'll call f, 
yeah and the loss function is going to take as input a picture and Spit out 
as output a value and the value will be lower.</p>

<p>If the image looks more like 
a the bird photo painted by Van Gogh having written that loss function, we 
will then use the pytorch, gradient and optimizers gradient times the 
learning rate and we're not going to update any weights. We're going to 
update the pixels of the input image to make it a little bit more like a 
picture which would be a bird painted by Van Gogh and we'll stick it 
through the loss function again to get more gradients, and do it again and 
again, and That's it so it's like identical to how we solve every problem, 
like you know, I'm a one-trick pony right. This is my only trick. Ok, tap, 
create a loss. Function, use it to get some gradients x, learning rates to 
update something always before we've updated weights. In a model, but today 
we're not going to do that, they're going to update that pixels in the 
input, but it's no different at all all right, we're just taking the 
gradient with respect to the input rather than respect to the weights. 
Okay, that's it so we're nearly done. Let's do a couple more things. Let's 
mention here that there's going to be two more inputs to our loss function. 
One is the picture of the bird birds. Look like this. Okay and the second 
is an artwork by Van Gogh. They look like this, oh and, of course, go okay, 
so, and by having those as inputs as well. That means we all be able to 
rerun the function later to make it look like you know, a bird painted by 
money or a jumbo jet painted by van Gogh or whatever right.</p>

<p>So those are 
going to be the three three inputs, and so initially as we discussed input 
here, this is going to be the first time I've ever found the rainbow pen 
useful, there's going to be that there's also: okay, some random noise. 
Okay. So we start with some random noise use the loss function, get the 
gradients, make it a little bit more like a bird painted by Van Gogh and so 
forth. Okay, so the only outstanding question which you know - I guess we 
can talk about briefly - is how we calculate how much our image looks like 
a bird, this bird painted by Van Gogh. Okay, so let's split it into two 
parts: let's put it into a part, called the content loss and that's going 
to return a function, a value, that's lower. If it looks more like the 
bird, not just any bird, the specific bird that we have coming in okay and 
then, let's also create something called the style loss and that's going to 
be a lower number. If the the image is more like van goths style. Okay, so 
there's one way to do the content loss, which is very simple. We could look 
at the pixels of the output, compare them to the pixels of the bird and to 
a mean square error addemup. So if we did that, I ran this for a while. 
Eventually, our image would turn into an image of the bird. You should try 
it right. You should try this as an exercise. Try to use the optimizer 
implied torch to start with a random image and turn it into another image 
by using mean squared error, pixel loss.</p>

<p>Okay, not terribly exciting, but 
that would be step one. The problem is, even if we already had our style 
loss, function, working beautifully and then presumably what we're going to 
do is we're going to add these two together right and then one of them will 
multiplied by some lambda so like adjust, some number we'll pick to Adjust 
how much style versus how much content right so assuming we had a style 
loss or we picked some sensible lambda. If we use two pixel wise content 
loss, then anything that makes it look more like Van Gogh and less like the 
exact photo. The exact background, the exact contrast lighting everything 
will decrease. The content was, which is not what we want right. We wanted 
to look like the bird, but not in the same way right. It's still gon na 
have the same two eyes in the same place and be the same kind of shape and 
so forth, but not the same representation. So what we're going to do is 
this is going to shop here we're going to use a neural network, all right, 
they're, going to use a neural network. I totally meant that to be black 
and a came out green, it's always a black box ever mind and we're going to 
use the vgg neural network, because that's what I used last year - and I 
didn't have time to see if other things worked, so you can Try that 
yourself during the week and the vgg network is something which takes in an 
input and sticks it through a number of layers and I'm just going to treat 
these as just the convolutional layers.</p>

<p>There's obviously value there. And 
if it's a bgg with batch norm which most are today, then it's it's also got 
better on men, there's some X pulling and so forth, but that's fine. What 
we could do is we could take one of these convolutional activations and 
then, rather than comparing the pixels of this bird, we could instead 
compare the vgg layer, five activations of this, to the vgg layer, five 
activations of our original birth or layer, six or layer. Seven or 
whatever, so why might that be more interesting? Well, for one thing: it 
wouldn't be the same bird right. It wouldn't be exactly the same because 
we're not checking the pixels we're checking some later set of activations, 
and so what are those latest sets of activations contained right? Well, 
assuming that's after some max pooling. They contain a small agree right. 
So it's less specific about where things are and rather than containing 
pixel color values they're more like semantic things like, is this kind of 
like an eyeball or is this kind of furry, or is this kind of bright or is 
this kind of reflective, or is this Laying flat whatever right so, we would 
hope that there's some level kind of semantic features through those layers 
where, if we get something a picture that that matches those activations, 
then any picture that matches those activations looks like the bird. But 
it's not the same representation of the bird.</p>

<p>So that's what we're going to 
do! That's what our content loss is going to be, and people generally call 
this a perceptual loss right which, because, like it's, really important in 
deep learning that you always create a new name for every obvious thing. 
You do right. So if you compare two activations together, you're doing a 
perceptual was okay. So so that's it! A Content. Loss is going to be a 
perceptual loss and then we'll do the style loss later. So, let's start by 
trying to create a bird that initially is random noise and we're going to 
use perceptual loss to create something that is bird-like. But it's not 
this better. Okay. So, let's start by saying they don't do 28 by 288, like 
we've, because pretty good to do one bird there's going to be no GPU memory 
problems right. So I was actually disappointed that I realized that I 
picked a rather small important image. It'd be fun to try this with 
something much bigger to create a really grand scale piece. The other thing 
to remember is: if you are like production izing, this you could like do a 
whole batch at a time. So people sometimes complain about this. This 
approach, gaddy's, is the lead author, the gaddy's style transfer, 
approaches being slow and I don't agree, it's low. It takes a few seconds 
and you can do a whole batch in a few seconds anyway.</p>

<p>So we're going to 
stick it through some trance, as per usual, transforms for vgg, 16 model, 
and so remember the transform class has a dunder call method, so we can 
treat it as if it's a function right. So if you pass an image into that, 
then we get the transformed image right so like try not to treat the fastai 
and ply torch infrastructure as a black box, because, like it's all 
designed to be like really easy to use in a decoupled way, all Right so 
this idea of that transforms are just callable, x' ie things that you can 
do with parentheses comes from pipe torch and we totally plagiarized the 
idea so with with torch vision or with fastai. You basically you're 
transforms are just color balls and the whole pipeline of transforms is 
just a callable. So now we have something of 3 by 2, 88 by 2 88, because 
pytorch likes the channel to be first and as you can see, it's been turned 
into a square for us, it's being normalized to 0 1 or that normal stuff. 
Ok, now we're creating a random image. Ok and here's something I discovered 
trying to turn this into a picture of anything. It's actually really hard. 
I found it very difficult to actually get an optimizer to get reasonable 
gradients that went anywhere and just as I thought, </p>

<h3>11. <a href="https://youtu.be/-lx2shfA-5s?t=1h15m40s">01:15:40</a></h3>

<ul style="list-style-type: square;">

<li><b> Impressive student’s experiment with different mathematical technique on Style Transfer</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>I was going to run out of time for this class and really embarrassed 
myself. I realized the key issue. Is that pictures? Don't look like this? 
They have more smoothness, so I turned this into this by just kind of 
blurring it a little bit. I used a median filter that basically it's like a 
like a median pooling, effectively right and as soon as I change it from 
this to this, it immediately started training really well, ok, so it's like 
a number of little tweaks. You have to do to get these things to work is 
kind of insane, but there's that here is a little booty alright, so we 
start with a random image, which is at least somewhat smooth. Okay and I 
found that my bird image had a standard deviation of pixels, that was about 
half of this, so I mean about half of this mean, so I divided it by two 
just trying to make it a little bit easier for it to match. I don't know if 
it matters turn that into a variable, because this image remember we're 
going to be modifying those pixels with an optimization algorithm. So 
anything that involved in the loss function needs to be a variable, and 
specifically it requires a gradient because we're actually updating the 
image. Okay, all right, so we now have a mini batch of one three channels: 
288 by 288, random noise, we're going to use for no particular reason the 
thirty-seventh layer of vgg. If you print out the vgg Network, you can just 
type in their member score, vgg and prints it out.</p>

<p>You'll see that this is 
a you know, kind of mid to late stage layer. So we can just grab the first 
37 layers and turn it into a sequential model, and so now we've got a 
subset of heg that will spit out some mid layer activations, and so that's 
that's what the models going to be. So we can take our actual bird image 
right and we want to create a mini batch of one. So remember if you slice 
in numpy with none, also known as NP, you axis it </p>

<h3>12. <a href="https://youtu.be/-lx2shfA-5s?t=1h18m">01:18:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Translate English into French, with Pytorch</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Introduces a new unit axis in that point, so in here I want to create an 
axis of size, one to say this is a mini batch of size, one alright, so 
slicing with none, just like I did here, has sliced with none to get this 
one's one Unit axis at the front, okay, sis, so then we turn that into a 
variable and this one doesn't need to write big, be updated. So it's we use 
DV to say you don't need gradients for this guy, and so that's going to 
give us our our target. Activations, okay, so we've basically taken our 
bird image turn it into a variable stuck it through our model to grab the 
thirty seventh layer. Activations and that's our target right is that we 
want our content loss to be this set of activations here. So then, we're 
going to create an optimizer, we'll go back to the details of this in a 
moment, but we're going to create an optimizer and we're going to step a 
bunch of times going 0. The gradients call some loss function, loss top 
backward. No, so that's the high-level version and I'm going to come back 
to the details in a moment, but the key thing is that the loss function, 
we're passing in that randomly generated image, the optimization image or 
actually the variable of it right. So we passed that to our loss function, 
and so it's going to update this using the loss function and the loss 
function is the mean squared error loss, comparing our current optimization 
image passed through our vgg to get the intermediate activations and 
comparing it to our target.</p>

<p>Activations, okay, just like we discussed okay 
and we'll run that a bunch of times and we'll print it out, and we have our 
bird but not the representation of the boat. Okay. So there it is so a 
couple of new details here. One is we had optimizer lb-ft, yes, anybody 
who's done. I don't know exactly what courses they're in, but certain parts 
of math and computer science courses comes into deep learning discovers we 
use all this stuff like Adam and the SGD, and always assume that nobody in 
the field knows the first thing about computer science and immediately 
Says, oh of any of you guys tried using the FDS, there's basically a long 
history of a totally different kind of algorithm for optimization that we 
don't use to train neural networks and of course the answer is actually the 
people who have spent decades studying neural networks. Do know a thing or 
two about computer science and it turns out these techniques on the whole. 
Don't work very well, but it's actually going to work well for this, and 
it's a good opportunity to talk about an interesting algorithm for those of 
you that haven't studied this type of optimization algorithm at school. So 
BFGS is one of the names Broyden favor. I can't remember anyway, initials 
are for different people at the L stands for limited memory, so it's really 
just quote: VFDs limited memory BFGS and it's an optimizer so as an 
optimizer. That means that there's some loss function and it's going to use 
some gradients.</p>

<p>To I mean not all optimizes use gradients, but all the ones 
we use do here's gradients to find a direction to go and try to make the 
loss function, go lower and lower by adjusting some parameters. Yeah this 
just an optimizer, but it's an interesting kind of optimizer, because it 
does a bit more work than the ones we're used to on each step and so 
specifically, okay, Facebook, okay, so the way it works is. It starts the 
same way that we used to, which is we just kind of pick somewhere to get 
started, and in this case we've picked like a random image, as we saw and 
as per usual, we we calculate the gradient, but we then don't just take a 
Step, but what we actually do is as well as finding the gradient. We also 
try to find the second derivative, so the direct second derivative says how 
fast is the gradient change, so the gradient is how fast of the function 
change. The second derivative is how fast as a gradient change. In other 
words, how curvy is it right and the basic idea is that if you know that 
it's like not very curvy, then you can probably jump further. But if it's 
very curvy, then you probably don't want to jump as far and so in in higher 
dimensions. The gradients called the Jacobian and the second derivative is 
called the Hessian you'll, see those words all the time that that's what 
they mean. Okay, again, mathematicians have to invent your words for 
everything as well they're, just like deep learning researchers.</p>

<p>So it may 
be a bit more snooty, so with BFGS we're going to try and calculate the 
second derivative and then we're going to use that to figure out kind of 
what direction to go and and how far to go all right. So it's less of a 
kind of a wild jump into the unknown. Now the problem is that actually 
calculating the hessian. The second derivative is almost certainly not a 
good idea, because in each possible direction that you can add for each 
direction that you're measuring the gradient in you also have to calculate 
the hessian in every direction. It gets ridiculously big so, rather than 
actually calculating it, we take a few steps and we basically look at how 
much the gradients changing as we do each step and we approximate the 
Hessian using that little function. Right and again, this seems like a 
really obvious thing to do, but nobody thought of it until someone. Well, 
surprisingly, a long time later, keeping track of every single step you 
take takes a lot of memory, so duh don't keep track of every step. You take 
just keep the last ten or twenty and the second bit there. That's the L to 
the l-bfgs, so a limited memory BFGS means keep the last ten or 20 
gradients use that to approximate the amount of curvature and then use the 
curvature in gradient. To estimate what direction to travel and how far, 
and so that's normally not a good idea in deep learning for a number of 
reasons, you know it's obviously more work to do than a kind of an atom or 
an SGD update, number Z, more memory memory is Much more of a big issue 
when you've got a GPU to store it on an hundreds of millions of weights. 
But, more importantly, the mini-batches super bumpy, so figuring out like 
curvature, decide exactly how far to travel is kind of polishing.</p>

<p>Turds, as 
we say, is that an American expression, or just an Australian Australian 
thing, a bit English there to do in a certain sense, yeah, obviously yeah, 
oh yeah, yeah polishing, turns you get the idea and also, interestingly, 
actually take using the second derivative information. It turns out is like 
a magnet for saddle points, so there's some interesting theoretical 
results. That basically say it's actually sends you towards nasty flat 
areas of the function. If you use second derivative information, so 
normally not a good idea, but in this case we're not optimising weights, 
we're optimizing pixels. So all the rules, change and actually turns our 
BFGS does make sense and because it does more work each time you know it's 
a kind of a different kind of optimizer. The API is a little bit different 
in plight. Watch as you can see here, when you say optimizer dot step, you 
actually pass in the loss function, okay and so my log. So my loss function 
is to call step with a particular loss function, which is my activation 
loss right and, as you can see you don't so you don't inside the loop, you 
don't say step step-step right, but rather it looks like this. So it's a 
little bit different and you're. Welcome to try and rewrite this to use. 
Sgd it'll still work it'll just take a bit longer.</p>

<p>I haven't tried it with 
SGD I'd, be interested to know how much longer it takes okay, so you can 
see the loss function going down the mean squared error between the you 
know, activations at layer 37 of our vgg model for our optimized image 
versus the target. Activations and remember the target activations were the 
vgg applied to our Albert, so make sense right so we've, okay, so we've now 
got a Content loss. Now one thing I'll say about this content loss. Is we 
don't know which layer it's going to work best, so it'd be nice if we were 
able to experiment a little bit more and the way it is here is annoying. 
Maybe we even want to use multiple layers? Okay, so, rather than like 
lopping off all of the layers are for the one we want. Wouldn't it be nice 
if we could somehow like grab the activations of a few layers as it 
calculates now. We already know one way to do that back when we did SSD. We 
actually wrote our own network, which had a number of outputs. Remember 
like the different convolutional layers. We spat out a different like icon 
thing, but I don't really want to go and like add that to the torch vision, 
ResNet model, especially not if, like later on, I want to try you know, 
then I want to try the torch vision, vgg model, and then I want to try an S 
and at a model I don't to go into all of them and like change their outputs 
right, besides, which I'd like to easily be able to turn certain 
activations on and off with demand.</p>

<p>So we briefly touched before this idea 
that pytorch has these fantastic things called hooks. You can have forward 
Hawks that, let you plug anything you like into the forward path of a 
calculation or a backward walk. So that's you plug anything. You like into 
the backward pass, so we're going to create the world's simplest forward 
hook, and this is one of these things that, like almost nobody knows about 
so like almost any code you find on the internet that implements style 
transfer will have all kinds of horrible Hacks, rather than using forward 
walks, but with four books, it's really easy so to create a forward hook, 
you just create a class right and the class has to have something called 
hook: function: okay and your hook function is going to receive the module 
that you've hooked. It's going to receive the input for the forward pass 
and it's going to receive the target, and then you do whatever the hell you 
like. So what I'm going to do is I'm just going to store the output of this 
module in some attribute? That's it all! Right so this can actually be 
called anything you like, but hook function seems to be the standard, 
because you can see what happens here in the constructor. Is I store inside 
some attribute? The result of this is going to be the layer that I'm gon na 
hook. You go module register forward hook and pass in the function that you 
want to be called when this module, when it's when it's forward method is 
called. So when it's forward method is called, it will call self dot hook 
function which will store the output in an attribute cord features.</p>

<p>Okay, 
so now what </p>

<h3>13. <a href="https://youtu.be/-lx2shfA-5s?t=1h31m20s">01:31:20</a></h3>

<ul style="list-style-type: square;">

<li><b> Translate English into French: using Keras to prepare the data</b></li>

<li><b>Note: Pytorch latest version now supports Broadcasting</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>We can do is we can create a vgg as before right and let's set it to not 
trainable. So we don't waste time and memory, calculating gradients for it 
and let's go through and find out, let's find all of the max pool layers. 
Alright. So, let's go through all of the children of this module and if 
it's a max pool layer, let's spit out index minus one. So that's going to 
give me the layer before the map, sport and so in general. The layer before 
and that's pool or the layer before us dry to cons is a very interesting 
layer. But because it's like it's the most, you know complete 
representation. We have at that grid cell size back because the very next 
layer is changing the grid. Okay, so that seems to me like a good place to 
grab the for content loss from, is you know the best most semantic, most 
interesting content we have at that grid size. So that's why I'm going to 
pick those indexes so Helia. Those are the indexes of the last layer before 
each max poor in vgg, so I'm going to grab this one here, 22, just no 
particular reason just to try something else. So I'm going to say sorry 
this one here 32, so I'm going to say, block ends. 3, that's maybe 32, so 
children, vgg indexed to block ends 3, will give me the 30 second layer of 
vgg as a as a module right and then, if I call the save features, 
constructor, it's going to go self cork equals 30. Second layer of the GG 
register forward hook hook, function, ok, so now, every time I do a forward 
pass on this bjg model.</p>

<p>It's going to store the 30 second layers. Output 
inside sf dot features. So we can now say see here. I'm calling my vgg 
Network, but I'm not strong it anywhere. I'm not saying you know: 
activations equals vgg of my image, I'm calling it throwing away the answer 
and then grabbing the features that we stored in our SF in our safe 
features, object right. So that way, this is now going to contain one. So 
like I've done. If this is a forward plus now that's how you do a forward 
pass and ply torch, you don't say dot forward, you just use it as a 
callable and using as a callable on an NN dot module automatically calls 
forward. That's how plat watch modules, what okay? So we call it as a koala 
ball that ends up calling our forward hook, that forward hook, stores the 
activations in SF dot features, and so now we have our target variable just 
like before, but in a much more flexible way. These are the same four lines 
of code. We had earlier I've just stuck them into a function, okay, and so 
it's just giving me my popped up, my random image to optimize and an 
optimizer to optimize that image. This is exactly the same code as before, 
so that gives me these, and so now I can go ahead and do exactly the same 
thing right. But now I'm going to use a different loss, function, 
activation lost number two which doesn't say out, equals mV GG again. The 
calls MV chuchita to a forward pass throws away the results and grabs. Sf 
top features.</p>

<p>Okay - and so that's now, my 30 second layer activations, 
which I can then do my MSA loss on you - might have noticed the last time 
the last loss function in this one, both multiplied by a thousand. Why are 
they multiplied by a thousand again? This was like all the things that were 
trying to get this lesson to not work correctly. I didn't used to have the 
a thousand yeah. It wasn't training by lunchtime today. Nothing was working 
after days of trying to get this thing to work and finally kind of just 
randomly noticed like gosh. The last functions like the numbers are really 
low. You know like 10 e neg 7 and I just kind of thought: well what, if 
they weren't so low, so I multiplied them by a thousand and instead of 
working. So why did it not work because we're doing single precision, 
floating point right and single precision floating point ain't, that 
precise and particularly once you're kind of getting gradients that are 
kind of small and then you're multiplying around the learning rate can be 
kind of small, and You end up with a small number and if it's so small, 
they could get rounded to zero and that's what was happening and my model 
wasn't ready. Okay, so I'm sure there are better ways: I'm multiplying by a 
thousand whatever it works. Fine, like it doesn't matter what you multiply 
a loss function by, because all you care about is its is its direction. 
It's relative size right and, interestingly, like this is actually 
something similar we do for when we were training imagenet, we were using 
half precision floating point, because the Volta tents, of course require 
that and it's actually a standard practice.</p>

<p>If you want to get the half 
precision floating point to Train, you actually have to multiply the loss 
function by a scaling factor and we were using a thousand and twenty four 
or five twelve, and I think fast. Ai is now the first library that has all 
of the tricks necessary to train in half precision floating point built-in. 
So if you now, if you have a lucky enough to have a Volta or you can pay 
for a p3, if you've got a learner object, you can just say learned: half 
and it'll now just magically train correctly position floating point that 
built into the model. Data objects as well, it's all automatic and pretty 
sure, no other library does that okay. So this is just doing the same thing 
on a slightly earlier layer and you can see that the bird looks you know 
the later layer. You know doesn't look very bird-like at all, but you can 
kind of tell it's bird slightly earlier layer, more bird-like right and 
hopefully that makes sense to you that earlier layers are getting closer to 
the they're getting closer to the pixels. You know it's a it's! A smaller 
grid size well, there's this little more grid cells. Each cell is smaller, 
smaller receptive field, less complex semantic features, so the earlier we 
get the more it's going to look like a bit and in fact the paper has a nice 
picture of that, showing various different layers and kind of zooming into 
this house.</p>

<p>They're trying to make this house look like this picture, and 
you can see that later on, it's pretty messy and earlier on. It looks like 
this okay, so this is just doing what we just did and I will say, like one 
of the things I've noticed in our study group is anytime. I say to somebody 
to answer a question. Yes, if we're anytime, I say, read: </p>

<h3>14. <a href="https://youtu.be/-lx2shfA-5s?t=1h38m50s">01:38:50</a></h3>

<ul style="list-style-type: square;">

<li><b> Writing and running the ‘Train &amp; Test’ code with Pytorch</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>The paper there's a thing in the paper that tells you the answer to that 
question. There's always this shock. Look: okay, read the paper me the 
paper, but seriously the papers have like they've done these experiments 
and drawn the pictures like there's all this stuff in the papers like it 
doesn't mean you have to read every part of the paper right, but at least 
look at The pictures right so check out the gaddy's paper. It's got nice 
pictures. Okay, so they've done the experiment for us. They basically did 
this experiment, okay, but it looks like they didn't go as deep. They just 
got some earlier ones. Okay, the next thing we need to do is to create 
style loss all right, so we've already got the loss, which is how much like 
the bird is it now we need how like this painting style? Is it and we're 
going to do nearly the same thing? Okay, we're going to grab the 
activations of somewhere now. The problem is that the activations of some 
layer - let's say it - was a five by five layer way. Of course, there are 
no five by five layers up to 24 by 24 bit corporate head five by five. I, 
whatever nine to say, totally unrealistic sizes but never mind so there, 
here's some activations and we could get these activations both per hour, 
the image we're optimizing and for our van Gogh. Thank you and let's look 
at our van Gogh painting there. It is night I downloaded this from 
Wikipedia and I was wondering what is taking so long to load.</p>

<p>It turns out 
that the the Wikipedia version I downloaded was thirty thousand by thirty 
thousand pixels. Okay, that's pretty cool they've got this like serious 
gallery quality archive stuff. There I didn't know it existed, so don't try 
and run a neuron in on that totally killed my Jupiter notebook, okay, so 
yeah. So we can do that for our van Gogh image and we can do that for our 
optimizer image. And then we can compare the two and we would end up 
creating an image that looks. You know, content like the painting, but it's 
not the painting, that's not what we want. We want something with the same 
style, but it's not the painting, it doesn't have the contrary. So we 
actually want to throw away all of the spatial information right, we're not 
trying to create something that looks that has a moon here and stars here 
and because it's a church here and whatever right, we don't want any of 
that. So how do we throw away all the spatial information? What we do is 
let's grab so there are like, in this case they're, like nineteen faces on 
this right, like nineteen slices. So let's grab this top slice. Okay, let's 
grab that top slice. So that's going to be a five by five matrix: okay and 
now, let's flatten it so now, we've got a twenty five long vector now, in 
one stroke, we've thrown away the you know the bulk of the spatial 
information by flattening it all right.</p>

<p>Now, let's grab a second slice, 
alright, so another another Channel and do the same thing: okay, so here's 
channel one flattened here's channel two flattened and they've both got $ 
25 and now, let's take the dot product which we can do with at in dump. I 
and so the nine the dot products going to give us one number. What's that 
number? What is it telling us? Well, assuming this is kind of somewhere 
around the middle activation. You know the activations are somewhere around 
the middle layer of the vgg network. We might expect some of these 
activations to be like how textured is the brushstroke and some of them to 
be like how bright is this area? And some of them to be like? Is this part 
of a house or a part of a circular thing or other parts to be? You know how 
dark is this part of the painting and so this a dot product? Remember it's 
basically a correlation right. If, if, if this element - and this 
</p>

<h3>15. <a href="https://youtu.be/-lx2shfA-5s?t=1h44m">01:44:00</a></h3>

<ul style="list-style-type: square;">

<li><b> NLP Programming Tutorial, by Graham Neubig (NAIST)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Element are both highly positive or both highly negative. It gives us a big 
result right we're also, if they're the opposite gives a small result. If 
they're both close to zero, it gives no result, so it's basically a dot 
product as a measure of how similar these two things are right, and so, if 
the activations of channel 1 and channel 2, you know how similar that it 
basically says. Let's give an example: let's say this: first, one was like 
how textured are the brushstrokes, and this one here that say was like how 
kind of diagonally oriented are the brushstrokes right and if and if both 
of these were high together and both of these were high Together then, it's 
basically saying, oh anywhere, that there's more textured brush strokes. 
They tend to be diagonal right or another. Interesting one is what would be 
the dot product of c1 with c1, so that would be basically the the two 
normal that the sum of the squares of that channel, which, in other words, 
is basically just on average. How so, let's go back. I screwed this up. 
Channel 1 might be texture and channel, two might be diagonal, and this one 
here would be cell 1, comma 1, and this cell here would be like cell, say, 
4, comma, 2, and so sorry, what I was should have been saying is: if these 
are both high At the same time - and these are both high at the same time, 
then it's saying grid cells would have texture tend to also have diagonal. 
So sorry I drew that all wrong. The idea was right.</p>

<p>It has drew it all 
wrong so yeah, so this number is going to be high when grid cells that have 
texture also have diagonal and when they don't they don't so that's C: 1 
dot product C 2, where else C 1 dot product C 1 right is Basically, as we 
said like the two norm, effectively squared or the sum of the squares of C 
1 sum over, I of c1 squared - and this is basically saying how, in how many 
grid cells is the textured channel active and how active is it so? In other 
words, see one dot product see, one tells us how much textured painting is 
going on and see two dot product see two tells us how much diagonal paint 
strokes is going on, and you know maybe see three is you know? Is it 
bright, colors so see three dot product see three would be you know. How 
often do we have bright colored cells? So what we could do, then is we 
could create a 25 by 25 matrix containing every one channel 1 channel 2 
channel 3 channel 1 channel 2 channel 3, so you're not channel man. It's 
been a long day. 19. There are 19 channels, 19 by 19, okay, channel 1 
channel 2 channel 3 channel 19 channel 1 channel 2 channel 3 did it do it 
channel 19, okay, and so this would be the dot product of channel 1 or 
channel 1. This would be the drug product of channel 2 with channel 2 and 
so forth. After flattening yeah and like we discussed mathematicians, have 
to give everything a name, so this particular matrix where you flatten 
something L out and then do the doctrine.</p>

<p>All the dot products is called a 
gram matrix </p>

<h3>16. <a href="https://youtu.be/-lx2shfA-5s?t=1h48m25s">01:48:25</a></h3>

<ul style="list-style-type: square;">

<li><b> Question: “Could we translate Chinese to English with that technique ?”</b></li>

<li><b>&amp; new technique: Neural Machine Translation of Rare Words with Subword Units (Research)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>And I'll tell you a secret like most deep learning: practitioners either 
don't know or don't remember all these things like what is a grand matrix 
if they ever did study at university, they probably forgot it because they 
had a big night afterwards and the way it works In practice is, like you 
realize, oh, I could create a kind of non spatial representation of how the 
channels correlate with each other and then, when I write up the paper, I 
have to go and ask around and say like does this thing have a name and 
Somebody be right, isn't it the gram matrix and you go and look it up that 
it is right, so don't think like you have to go and study all of math. 
First, you use your intuition and common sense, and then you worry about 
what the math is called later. Normally sometimes it works the other way 
not with me, because I can do a mess okay, so this is called the gram 
matrix and, of course, if you're, a real mathematician is very important 
that you, you say this as if you already always knew it was a Gram matrix 
and you kind of just go: oh yes, we just calculate the gram matrix, that's 
really important, so the grand matrix then is this kind of map of the 
diagonal is perhaps the most interesting right. The diagonal is like, which 
channels are the most active and then the off diagonal is like which 
channels tend to appear together and overall.</p>

<p>If two pictures have the same 
style right, then we're expecting that some layer of activations they will 
have similar Gramm matrices, because if we found the level of activations 
that capture a lot of stuff about like paint, strokes and colors and stuff, 
then the diagonal alone might Even be enough and like that's another 
interesting homework assignment, if somebody wants to take, it, is try 
doing gaddy's style transfer, not using the ground matrix, but just using 
the diagonal of the gray matrix, and that would be like a single line of 
code to change that. I haven't seen it tried and I don't know if it would 
work at all, but it might work fine, Christine Christine. Okay, yes, 
Christine you've tried it and it works most of the time except when you 
have funny pictures where you need two styles to appear in the same spot. 
So it sounds like grass in one half and like a crowd in one half, and you 
need the two styles. Ah cool you're still gon na take your homework. Is 
that okay, Christine says she'll, do it for you? Okay? So let's do that. So 
here's our painting I've tried to resize the painting, so it's the same 
size as my bird picture. So that's all this is just doing so. I make the 
yeah so there it is. It doesn't matter too much which bit I use as long as 
it's got. Lots of a nice style in it, I grab my optimizer and my random 
image just like before and this time I call save features for all of my 
block ends and that's going to give me an array of save features, objects, 
one for each module.</p>

<p>That appears the layer before IMAX Paul so now, 
because because this time I want to play around with different activation 
layer styles or more specifically, I want to let you play around with it. 
Okay, so now I've got a whole array of them. So now I call my vgg module on 
my my image again: yeah, I'm not going to use that yeah. Okay, ignore that 
line.i style image; sorry style images, my van Gogh painting, so I take my 
style image. Put it through my transformations to create my transform style 
image. I turn that into a variable put it through the forward pass of my 
vgg module, and now I can go through all of my save features, objects and 
grab each set of features and notice. I call clone right because I don't 
I've caused like later on. If I call my vgg object again, it's going to 
replace those contents, I haven't quite thought about whether this is 
necessary. If you take it away and it's not that's fine but us as being 
careful. So here's now an array of the activations at every block and 
player. So here you can see all of those shapes and you can see like being 
able to like whip up a list. Comprehension really quickly. It's really 
important in your Jupiter fiddling around because you really want to be 
able to like immediately see you know. Here's my channel sixty four one, 
four, six, five, twelve and you can see here the grid size having as we 
would expect because all of these appear just before M export. So so do a 
Graham MSC loss.</p>

<p>It's going to be the MSE loss on the ground. Matrix of the 
input versus the gram matrix of the target and the ground matrix is just 
the matrix multiply of X with X, transpose where X is simply equal to my 
input where I've flattened the batch and channel axes all down together, 
and I already got one Image so you can kind of ignore the batch part right, 
it's basically channel and then everything else which in this case is the 
height and width, is the other dimension, because there's now it'll be 
channel by width and then, as we discussed, we can then just do The matrix 
multiply of that by its transpose and just to normalize it we'll divide 
that </p>

<h3>17. <a href="https://youtu.be/-lx2shfA-5s?t=1h54m45s">01:54:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Leaving Translation aside and moving to Image Segmentation,</b></li>

<li><b>with the “The 100 layers Tiramisu: Fully Convolutional DenseNets” (research)</b></li>

<li><b>and “Densely Connected Convolutional Networks” (research)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>By the number of elements, it would actually be more elegant if I had said 
/ import, dot, Nam, elements that were in the same thing: okay and then 
again, this kind of keep me tiny numbers. So I multiply it by a big number 
to make it something more sensible. Okay, so that's basically my loss 
right. So now my style loss is to take my image to optimize, throw it 
through vgg forward, pass grab an array of the features in all of the safe 
features objects and then call my Graham MSC loss on every one of those 
layers. Okay and that's going to give me an array, and then I just add them 
up now you could add them up with different weightings. You could add up 
subsets whatever right in this case, I'm just grabbing all of them pass 
that into my optimizer as before, and here we have a random image in the 
style of Van Gogh, which i think is kind of cool and again gaddy's has done 
it For us here is different layers of random image in the style of Van 
Gogh, and so that the first one as you can see, the activations are simple, 
geometric things not very interesting at all. The later eight layers are 
much more interesting, so we kind of have a suspicion that we probably want 
to use later layers largely for our style laughs. If we wanted to look good 
all right, I added this. What was it this save features, dot close, which 
just calls remember, I stored the hook here and so Hawk. Don't remove, gets 
rid of it and it's a good idea to get rid of it, because otherwise, you 
know you can potentially just keep using memory all right.</p>

<p>So, at the end I 
go through each of my so features. Objection was it so style transfer is 
adding the two together with some weight, so there's not much to show grab 
my optimizer grab my image and now my combined loss is the MSE loss at one 
particular layer. My style loss set all of my layers sum up. The stay 
losses add them to the content loss, the content lost. So I'm scaling 
actually the style loss a scaled already by 1e6, and this one is one two, 
three, four, five, six, okay, so actually they're they're, both the scaled. 
Exactly the same, add them together and again you could try weighting the 
different style losses or you could maybe remove some of them whatever. So 
this is the simplest possible version, train that and like holy, it 
actually looks good. So I think that's yeah. I think that's pretty awesome, 
you know again, you know the main takeaway here is: if you want to solve 
something with a neural network, all you got to do is set up a loss, 
function and then optimize something all right and the loss function is 
something which A lower number is something that you're happier with 
because then, when you optimize it, it's going to make that number as low 
as you can and it'll do. What's your, what did its do? Okay? So here we 
can't what we didn't come up with. Gettys came up with a loss function that 
does a good job of being a smaller number.</p>

<p>When it looks like the thing we 
want it to look like, and it looks like the style of the thing we want to 
be in the style of that's all. We had to do right, we like what it actually 
comes to it. You know, apart from implementing gram, MSE loss, which was 
like six lines of code. If that that's our loss function, pass it to our 
optimizer, wait about five seconds and we're done and remember. We could do 
a batch of these at a time, so we could wait five seconds and sixty-four of 
these will be done yeah. So I think that's really interesting and and since 
this paper came out, you know it's really inspired a lot of interesting 
work to me, though, most of the interesting work hasn't happened yet 
because to me the interesting work is the work where you combine human 
creativity with These kinds of tools - you know - and you know I haven't 
seen much in the way of tools that you can download or use where you know 
that the artist is in control and can kind of do things interactively it's 
interesting talking to the guys at the Google Magento project, which is 
kind of their creative value project, all of the stuff they're doing with 
music, is specifically about this. It's building tools that musicians can 
use to perform in real time, and so you'll see much more of that on the 
music space thanks to magenta.</p>

<p>If you go to their website, there's all 
kinds of things where you can like press the buttons to like actually 
change the drum beats or melodies or keys or whatever, and you can 
definitely see like Adobe an Nvidia, it's kind of starting to release. You 
know little prototypes and started you through this spit. You know this 
kind of like creative AI explosion hasn't happened yet I think we have 
pretty much all the technology we need, but no one's like put it together 
into a thing and said. Like look at the thing I built and look at the stuff 
that people built with my thing, you know so that's just a huge area of 
opportunity. So the paper that I mentioned at the start of class in passing 
the one where we can add Captain America's shield to arbitrary paintings, 
basically used this technique right that the trick was, though some minor 
tweaks to make the kind of the pasted Captain America shield blending 
Nicely right but like that, would that papers only a couple of days old so 
like that would be a really interesting project to try, because you you can 
use all this code. You know it really does leverage this approach and then 
you could start by you know making the content image be like the painting 
with the shield and then the style image could be the painting without the 
shield and like that would be a good start. And then you could kind of see 
what specific problems they try to solve and there's painting in this paper 
to make it better.</p>

<p>But you know you could you could have a start on it 
right now? Okay, so let's make a quick start on the next bit, which is yes, 
Rachel, say two questions earlier. There are a number of people that 
expressed interest in your thoughts on pyro and probabilistic programming 
yeah. So you know tensor flows now, but this particular tends to flow 
probability or something there's yeah, there's a bunch of probabilistic 
programming frameworks out there. I I think they're intriguing, you know, 
but as yet unproven in the sense that, like I haven't, seen anything done 
with any probabilistic programming system, which hasn't been done better 
without them. The basic premise is that it allows you to create more of a 
moral of how you think the world works and then like plug in the 
parameters. So back when I used to work in management consulting 20 years 
ago, we used to do a lot of stuff where we would use a spreadsheet and then 
we would have these Monte Carlo simulation and plugins at risk, and one 
crystal ball. I don't know if there still exists decades later, but 
basically they would let you like change a spreadsheet cell. To say this is 
not a specific value, but it actually represents a distribution of values. 
With this mean and the standard deviation or it's got this distribution, 
and then you would like hit a button and the spreadsheet would recalculate 
a thousand times pulling random numbers from those distributions and show 
you like the distribution of your outcome.</p>

<p>That might be some. You know 
profit or market share or whatever, and we used them all the time back. 
There apparently think that feel that a spreadsheets a more obvious place 
to do that kind of work, because you can kind of see it all much more. 
Naturally, but I don't know we'll see at this stage - I I hope it turns out 
to be useful, because I find it very appealing and it kind of appeals to, 
as I say, the kind of work I used to do. A lot of there's actually whole 
practices around this stuff. They used to call systems dynamics which 
really was built on top of this kind of stuff, but no it's it's not quite 
gone anywhere. Hey then there is a question about pre training for generic 
style transfer. Yes, I don't think you can pre train for a generic style, 
but you can pre train for a generic photo for a particular style which is 
where we're going to get to. Although it may end up being a homework, I 
haven't decided, but I'm going to do all the pieces and one more question 
is: please ask him to talk about multi-gpu, oh yeah, I haven't had a slide 
about that. It's events actually we're about to about to get it. So, yes, 
okay! So before we do just another interesting picture from the Geddes 
paper, they've got a few more just didn't fit in mice in my slide here, but 
different convolutional layers for the style, different style to content 
ratios and here's the different images.</p>

<p>Obviously this isn't then go 
anymore. There's a different combination, so you can see like if you just 
do like wall style. You don't see any image. If you do all you know lots of 
content that you use low enough convolutional layer, it looks okay, but the 
backgrounds kind of dumb. So you kind of want somewhere around here - or 
here I guess anyway, so you can play around with it and experiment, but 
also use the paper to help guide you. Actually. I think I might work on the 
math now and we'll talk about multi, GPU and and super resolution next 
week, because I think that this is from the the paper and like one of the 
things I really do want you to do after we talk about a Paper is to read 
the paper and then ask questions on the forum. Anything that's not clear, 
but there's kind of like a key part of this paper, which I wanted to talk 
about and discuss how to interpret it. So we're going to be the paper says: 
we're going to be given an input image X and this little thing means it's 
whatnot. It means it's a vector Rachel, but this one's a matrix. I guess it 
could mean either yeah. I don't know - maybe it's anyway. So normally small 
letter, bulk means vector or a small letter with Dubey on top means vector 
they can both mean vector and normally big letter means matrix or small 
letter with two doobies on top means matrix. In this case, our image is a 
matrix. We are going to basically treat it as a vector, so maybe we're just 
getting ahead of ourselves.</p>

<p>So we've got an input image X and it can be 
encoded in a particular layer of the CNN by the filter responses. So the 
activations, your responses are activations right. So hopefully, that's 
something you all understand, that's basically what a CNN does is it 
produces layers of activations Alea has a bunch of filters right, which 
produce a number of channels right and so this year says that layer number 
L has capital our filters and again this Capital does not mean matrix, so I 
don't know math notation is so inconsistent, so capital NL distinct filters 
that layer, L, which means it has that also that many feature Mouse right 
so make sure you can see that this letter is the same as list letter. 
That's you've got to be very careful to read the letters and recognize it's 
like snap. You know that's the same letter as that. Okay, so obviously NL 
feature maps are in our filters, filters create and our feature maps or 
channels h1 is of size M. Okay. So I can see this is where the this is, 
where the unrolling is happening. Each map is of size M little L right. So 
this is, like you know, m square bracket, L in numpy, notation, it's the 
elf layer, so m for the elf layer and the the size is height times: width; 
okay, so we flattened it out. So the responses at that layer, L, can be 
stored in a matrix, F and now the old grows at the top for some reason. So 
this it's not f, ^ else.</p>

<p>It's just another indexing we're just moving 
around fun, and this thing here where we say it's an element of R. This is 
a special I mean in the real numbers n times M. This is saying that the 
dimensions of this is n by M right. So this is really important, like you, 
don't move on it's just like with pytorch, making sure that you understand 
the rank and size of your dimensions. First same with math right, you did. 
These are the bits where you stop and think. Why is it n by M right? Okay, 
so n is a number of filters. M is height by width right. So do you remember 
that thing where we did viewer batch x, channel, comma, minus 1 right here 
that is okay, so try to map the code to the math, so f is f is X. If I was 
nicer to you, I would have used the same letters as the paper, but I was 
too busy getting this damn thing working to do that carefully, so you can 
go back and rename it as capital F, okay, and this is why we moved the L to 
the top is because we're now going to have some more indexing right so 
like, where else your numpy or apply torch. We index things by square 
brackets and then lots of things with commas between the approach in math 
is to like surround your letter by little letters all around it: okay and 
just throw them up there everywhere. So here F, L is the elf layer of F and 
then I J is the activation of the I filter at position, J of layer, yeah 
right, so position J is cut up to size M, which is up to size height by 
width. This is the kind of thing that be easy to get confused.</p>

<p>Like often 
you'd see an IJ and assume that's like indexing into a position of an image 
like height by width, but it's totally not. Is it okay, it's indexing into 
channel by flattened image right, and it even tells you it's the I filter 
the earth channel in the J's position in the flattened out image in layer. 
L right so you you're, not gon na, be able to get any further in the paper 
unless you know, unless you understand what F is okay, so that's. Why, like 
these are the bits where you stop and make sure you're comfortable all 
right. So now the content lost, so I'm not going to spend much time on, but 
basically we're going to just check out the values of the activations 
versus the predictions squared right. So there's our content, loss, okay 
and the style loss will be much the same thing. But using the grande matrix 
G, okay - and I really wanted to show you this ones, I think it's super. 
This is sometimes I really like things you can do in math note and they're 
things that you can also generally do in J and APL, which is this kind of 
this implicit loop going on here right. What this is saying is there's a 
whole bunch of values of I and a whole bunch of values of J and I'm going 
to define G for all of them and there's a whole bunch of values of L as 
well. I'm going to define G for all of those as well right and so for all 
of my G at every L of every I at every J. It's going to be equal to 
something, and you can see that something has an i and a J and an L right 
so matching these, and it also has a K and that's part of the sum so what's 
going on here.</p>

<p>Well, it's saying that my grand matre -- ks in layer, l for 
the eighth channel well there's out channels anymore in the eighth position 
in one axis and the j DH position and another axis is equal to my F matrix, 
so my flattened out matrix for the Eighth channel in that layer versus the 
jafe channel and the same layer, and then I'm going to sum over I'm going 
to say see. This K in this case are the same letter right. So we're going 
to take the k, DH position and multiply them together and then add them all 
up all right. So that's exactly what we just did before when we calculated 
our own matrix right. So like this, there's a lot going on because of some 
like to me very neat. Notation right, which is there are three implicit 
loops or going on at the same time, plus one explicit loop in the Sun. And 
then they all work together to create this grand matrix for every layer. 
That's so let's go back and see if you can match this yeah. So so, oh 
that's kind of happening all at once, which i think is pretty great. Okay, 
so that's it so next week we're going to be looking at a very similar 
approach, basically doing style transfer all over again, but in a way where 
we were actually going to train a neural network to do it for us rather 
than having to do the Optimization and we'll also see that you can do the 
same thing to do super resolution and we're also going to go back and 
revisit some of that SSD stuff, as well as doing some some segmentation.</p>

<p>So 
if you're, if you've forgotten SSD, might be worth doing a little bit of 
revision this week, all right thanks, everybody see you next week, [ 
Applause, ], </p>






  </body>
</html>
