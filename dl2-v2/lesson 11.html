<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 11: Neural Translation</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://course.fast.ai/">fast.ai: Deep Learning Part 2 (v2) (2018)</a></p>
  <h1><a href="http://course.fast.ai/lessons/lesson11.html">Lesson 11: Neural Translation</a></h1>
  <h2>Outline</h2>
<p>Today we’re going to learn to translate French into English! To do so, we’ll learn how to add attention to an LSTM in order to build a sequence to sequence (seq2seq) model. But before we do, we’ll do a review of some key RNN foundations, since a solid understanding of those will be critical to understanding the rest of this lesson.</p>

<p>A seq2seq model is one where both the input and the output are sequences, and can be of difference lengths. Translation is a good example of a seq2seq task. Because each translated word can correspond to one or more words that could be anywhere in the source sentence, we learn an attention mechanism to figure out which words to focus on at each time step. We’ll also learn about some other tricks to improve seq2seq results, including teacher forcing and bidirectional models.</p>

<p>We finish the lesson by discussing the amazing DeVISE paper, which shows how we can bridge the divide between text and images, using them both in the same model!</p>

  <h2>Video Timelines and Transcript</h2>

<h3>1. <a href="https://youtu.be/bZmJvmxfH6I?t=30s">00:00:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Tips on using notebooks and reading research papers</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>What a start pointing out a couple of the many cool things that happen this 
week. One thing that I'm really excited about is we briefly talked about 
how Leslie Smith has a new paper out and it basically the paper goes text 
is previous to key papers. Are cyclic or learning rates and super 
convergence and built on them with a number of experiments to show how how 
you can achieve super convergence and so super convergence lets you train 
models five times faster than previous, like kind of stepwise approaches, 
it's not 5 times faster Than CLR, but it's faster than CLR as well, and the 
key is that super convergence lets you get up to like massively high 
learning rates by somewhere between 1 and 3, which is quite amazing, and so 
the interesting thing about super convergence is that it. You actually 
train at those very high learning rates for quite a large percentage of 
your ybox, and during that time the loss doesn't really improve very much. 
But the trick is like it's doing a lot of searching through the space to 
find really generalizable areas. It seems so those we kind of had a lot of 
what we needed in fastai to achieve this, but we're missing a couple of 
bits, and so silver Google has done an amazing job of fleshing out the 
pieces that we're missing and then confirming that he Has actually achieved 
super convergence on training on say, 5 10. So I think this is the first 
time that this has been done, that I've heard of outside of Leslie Smith 
himself, and so he's got a great blog post up now on one cycle, which is 
what Leslie Smith called this approach, and this is actually it turns Out 
what one cycle looks like it's: a single cyclic or learning rate, but the 
key difference here is that the the going up be it is the same length as 
they're going down right, so you go up like really slowly and then at the 
end, for like A tenth of the time you, you, then, have this little bit 
where you go down even further and it's interesting.</p>

<p>Obviously this is a 
very easy thing to show very easy thing to explain. Sylvia has added at a 
fastai under the temporarily. It's it's called use. Clr beta by the time 
you watch this on the video it'll probably be called one cycle - something 
like that. But you can use this right now. So that's one key piece to 
getting these massively high learning rates and he shows a number of 
experiments when you do that. A second key piece is that, as you do this to 
the learning rate, you do this to the momentum right. So when the loading 
rates low, it's fine to have a high momentum, but then, when the learning 
rate gets up really high, your momentum needs to be quite a bit lower. So 
this is also part of our. What he's added to the library is this cyclic? 
All momentum, and so with </p>

<h3>2. <a href="https://youtu.be/bZmJvmxfH6I?t=3m15s">00:03:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Follow-up on lesson 10 and more word-to-image searches</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>With with these two things you can train for about the fifth of the number 
of epochs with stepwise learning rate schedule, then you can drop your 
weight decay down by about two orders of magnitude. You can often remove 
most or all of your drop out, and so you end up with something that's 
trained faster and generalizes better, and it actually turns out that 
silver got quite a bit better accuracy than Leslie Smith's paper. His guess 
I was pleased to see is because our data augmentation defaults are better 
than less visas. I hope that's true so check that out another cool thing I 
just as I say, there's been so many cool things this week, I'm just going 
to pick two Hamill who seen who's a works at github. I just really like 
this there's a fairly new project called our cube flow, which is basically 
10250 for kubernetes Hamill wrote a very nice article about magical 
sequence, to sequence, models, building data products on that and using 
kubernetes to kind of put that in production and so Forth he said that the 
Google cou flow team created a demo based on what he wrote earlier this 
year directly based on the skills on moon, fastai, and I will be presenting 
this technique at KD. Dk t DS one of the top academic conferences, so I 
wanted to share this as a motivation for folks to blog, which i think is a 
great point. You know I don't nobody who goes out and writes a blog things 
that you know. Probably you know.</p>

<p>None of us really think our blog is 
actually going to be very good, probably nobody's gon na read it you know, 
and then, when people actually do like it and read it, it's like with great 
surprise you just got over. That's actually something people were 
interested to read. So here is the the tool where you can summarize github 
issues using this tool, which is now hosted by Google on their Cooper org 
domain. So I think that's a great story of getting you know if Emeril 
didn't put his work out there, none of this would have happened and yeah 
you can check out his post. That made it all happen as well. So talking of 
the magic of sequence to sequence, models, let's build one, so we're going 
to be specifically working on machine translation. So machine translation 
is a something that's been around for a long time, but specifically we're 
going to look at an approach called neural translation, which is using 
neural networks for translation, and they didn't know that wasn't really a 
thing in any kind of meaningful way. Until a couple of years ago, and so 
thanks to Chris Manning from Stanford for the next three slides 2015 Chris 
pointed out that neural machine translation kind of first appeared properly 
and it was pretty crappy compared to the statistical machine. Translation 
approaches that use kind of classic.</p>

<p>Like feature engineering and standard 
MLP kind of approaches of lots of stemming and fiddling around this work 
frequencies and engrams and lots of stuff by a year later, it was better 
than everything else. This is on a metric called blue, we're not going to 
discuss the metric, because it's not a very good metric and it's not very 
interesting, but it's what everybody uses, so that was blue as of when 
Chris did this slide. As of now, it's about up here, it's about 30, so 
we're kind of seeing machine translation starting down the path that we 
saw, starting computer vision, object classification in 2012. I guess which 
is you know, we kind of just surpassed the state of the art and now we're 
zipping past it at a great rate. It's very unlikely that anybody watching 
this is actually gon na build a machine translation model because you can 
go to </p>

<h3>3. <a href="https://youtu.be/bZmJvmxfH6I?t=7m30s">00:07:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Linear algebra cheat sheet for deep learning (student’s post on Medium)</b></li>

<li><b>&amp; Zero-Shot Learning by Convex Combination of Semantinc Embeddings (arXiv)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Translate, Google calm and use theirs and it works quite well. So why are 
we learning about machine translation? Well, the reason we're learning 
about machine translation is that the general idea of taking some kind of 
input like a sentence in french and transforming it into some other kind of 
output with arbitrary length such as a sentence in english, is a really 
useful thing to do. For example, the thing that we just saw that ham all 
that github did tex github issues and turns them into summaries. Other 
examples is taking videos and turning them into descriptions or taking a 
well. I don't know I mean, like you know, basically anything where you're 
spitting out kind of an arbitrary sized output. Very often that's a 
sentence, so maybe taking a CT, scan and spitting out a radiology report. 
You know this is where you can use sequence to sequence, learning. So the 
important thing about a neural machine translation - these are more slides 
from Chris and and generally six a sequence of sequence. Models is that you 
know there's no fussing around with heuristics and Haacke. You know feature 
engineering, whatever it's end-to-end training we're able to build these 
distributed representations which are shared by lots of kind of concepts 
within a single network. We're able to use long term State in the air and 
ENSO use a lot more context than kind of Engram type approaches and in the 
end, the text we're generating uses an iron in as well.</p>

<p>So we can build 
something. That's more fluid we're going to use a bi-directional LS TM, 
with attention well, actually were going to use a bi-directional giu with 
attention, but basically the same thing. So you already know about 
bi-directional: recurrent neural networks and attention we're going to add 
on top today. These general ideas, you can use or lots of other things as 
well as Chris points out on this slide. So let's jump into the code which 
is in the translate notebook, funnily enough and so we're </p>

<h3>4. <a href="https://youtu.be/bZmJvmxfH6I?t=10m">00:10:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Systematic evaluation of CNN advances on ImageNet (arXiv)</b></li>

<li><b>ELU better than RELU, learning rate annealing, different color transformations,</b></li>

<li><b>Max pooling vs Average pooling, learning rate &amp; batch size, design patterns.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Going to try to translate French into English, and so the basic idea is 
that we're going to try and make this look as much like a standard, neural 
network approach as possible. So we're going to need three things. You will 
remember the three things: data, a suitable architecture and a suitable 
loss function once you've got these three things you run fit and all things 
going. Well, you end up with something that solves your problem: okay, so 
data! You know we generally need XY pairs. Okay, because we need something 
which we can feed it into the loss function and say I took my x-value, 
which was my French sentence and the loss function says it was meant to 
generate this English sentence. And then you had your predictions, which 
you would then compare and see how good it is. Okay. So therefore we need 
lots of these tuples of french sentences with their equivalent English 
sentence. That's called a parallel corpus. Obviously, this is harder to 
find than a corpus for a language model because for a language model we 
just need text in some language which you can basically all for any living 
language of which the people that use that language, like use computers. 
There will be a few gigabytes at least of text floating around the 
internet, for you to grab okay, so building a language model is only 
challenging corpus wise or you know, ancient languages. One of our students 
is trying to do a Sanskrit one.</p>

<p>For example, at the moment, but that's very 
rarely a problem for translation. There are actually some pretty good. 
Parallel corpus is available for European languages. The European 
Parliament basically has every sentence in every European language. 
Anything that goes through the UN is translated to lots of languages. For 
French to English, we have a particularly nice thing, which is pretty much 
any semi official Canadian web site. I will have a French version and an 
English version, and so this chap kursk, Ellison birch did a cool thing, 
which is basically to try to transform French URLs into English URLs by 
like replacing. If I am and hoping that that retrieves the equivalent 
document and then did that for lots and lots of web sites and into that 
creating a huge corpus based on millions of web pages, so a French to 
English, we have this particularly nice resource. So we're going to start 
out by talking about how to create the data, then we'll look at the 
architecture and then we'll look at the loss function and so for bounding 
boxes. All of the interesting stuff was in the loss function, but for New 
York translation. All of the interesting stuff is going to be in the 
architecture. Okay, so let's zip through this pretty quickly and one of the 
things I want you to think about particularly, is what are the 
relationships, the similarities in terms of the tasks we're doing and how 
we do it between language, modeling versus euro translation, okay.</p>

<p>So the 
basic approach here is that we're going to take a sentence, so this case 
this example is English to German and this slides through Mon Steven 
emeriti, we steal everything we can from Steven. We start with some 
sentence in English, and the first step is to do basically the exact same 
thing: we do in a language model which is to chuck it through an R in it 
now with our language model. Actually, that's not even think that language 
model. Let's start even easier: the classification model, okay, so 
something that turns some. This sentence into positive or negative 
sentiment. We had a a decoder, you know something which basically took the 
RNN output and from our paper we grabbed three things. We took a max pull 
over. All of the time steps we took a mean port over there, all the time 
steps and we took the value of the iron in at the last time, step stack all 
those together and put it through a linear layer. Most people don't do that 
in most NLP stuff. This is a like. I think it's something we invented 
people pretty much always use the last time step. So all the stuff we'll be 
talking about today uses the last time step. So we start out by chucking 
this sentence through an R and N, and out of it comes some state right, so 
some state meaning some hidden state, some vector that represents the 
output of an hour that is encoded. That sentence you'll see the word that 
Steven used here was encoder.</p>

<p>We've tended to use the word backbone right 
so like when we've talked about like adding a custom head to an existing 
model. Like you know, the existing pre-trained imagenet model, for example, 
we kind of say that's our backbone, and then we stick on top of it. 
Some-Some head. That does the task. We want, in sequence, to sequence, 
learning they use the word encoder, but it basically it's the same thing. 
It's some piece of a neural network architecture that takes the input and 
turns it into you know some representation which we can then stick a few 
more layers. On top of to grab something out of it, such as we did for the 
classifier, where we stuck a linear layer on top over to turn it into a 
sentiment, positive or negative. So this time, though, we have something 
that's a little bit harder than just kiding sentiment, which is, I want to 
turn this state not into a positive or negative sentiment, but into a 
sequence of tokens where that sequence of tokens is the German. In this 
case, the German sentence that we want, so this is sounding more like the 
language model than the classifier, because the language model had multiple 
tokens for every input word. There was an output word, but the language 
model was also much easier because the the number of tokens in the language 
model output was the same length as the number of tokens in the language 
model input, and not only were there the same length they exactly matched 
Up it's like after word.</p>

<p>One comes word, two afterward two comes word three 
and so forth right, but for translating language you don't necessarily know 
that the word he will be translated as the first word in the output and 
that love will be. The second word in the output I mean in this particular 
case. Unfortunately they are the same, but very often you know the subject. 
Object order will be differential. There will be some extra words inserted 
or some pronouns we'll need to add some gendered article or whatever. Okay, 
so this is the key issue we're gon na have to deal with. Is the fact that 
we have an arbitrary length output where the tokens in the output do not 
correspond to the same order? You know specific tokens in the input okay, 
but the general idea is the same: here's an RNN to encode the input turns 
it into some hidden state, and then this is the new thing. We're going to 
learn is generating a sequence output. So we already know sequence. Two 
plus that's IMDB classifier, we already know sequence, two equal length 
sequence. Where corresponds to the same items: that's the language model, 
for example, but we don't know yet how to do a general-purpose sequence to 
sequence. So that's the new thing today. Very little of this will make 
sense unless you really understand lesson: six, how an RNN works.</p>

<p>Okay. So 
if some of this lesson doesn't make sense to you - and you find yourself 
wondering what does he mean by hidden state, exactly how's - that working 
go back and re-watch lesson six, to give you a very quick review, we learnt 
that and are an N at its Heart is a standard fully connected Network, so 
here's one with one two three four layers right takes an input and puts it 
through four layers, but then at the second layer it can just concatenate 
in the second important third layer concatenate in a third input, but we 
Actually wrote this in Python as just literally a four layer, neural 
network, okay, there's nothing else. We used other than linear layers and 
values. We used the same weight matrix every time an input came in, we used 
the same matrix every time. We went from one of these states to the next 
and that's why there's errors of the same color? And so we can redraw that 
previous thing like this yeah, and so we not only did we redraw it, but we 
took the you know four lines of linear, linear, linear linear code in 
pytorch and we replaced it with a for loop. Okay, so remember we had 
something that did exactly the same thing as this, but it just had four 
lines of code: saying linear, linear, linear layer, and we really literally 
replaced it with a for loop because that's nice to refactor.</p>

<p>So, like 
literally, that refactoring, which doesn't change any of the math any of 
the ideas and if the outputs that refactoring is entirely okay, turning a 
bunch of separate lines in the code into a Python corner, okay, and so 
that's how we can draw it, we could Take the output so that it's not 
outside the loop and put it inside the loop like so right, and if we do 
that, we're now going to generate a separate output for every input. So in 
this case this particular one here, the hidden state gets replaced. Each 
time - and we end up just spitting out the final hidden State, so this one 
is this example: okay, but if, instead we had something that said, you know 
h's dot, append, h and returned H's at the end. That would be this picture 
yeah and so go back and relook at that notebook. If this is unclear, I 
think the main thing to remember is when we say hidden state we're 
referring to a vector. Okay, see you here's the vector right H, equals 
torch, zeroes, okay and hidden. Now, of course, it's a vector for each 
thing in the mini bash, so it's it's a matrix, but I'm generally, when I 
speak about these things, I ignore them in each piece and treat it for just 
a single item. Okay, so it's just a vector of this length. We also learned 
that you can stack these layers on top of each other, so rather than this 
first errand ends bidding our output.</p>

<p>There could just spit out inputs into 
a second area and, if you're thinking at this point, I think I understand 
this, but I'm not quite sure if you're anything like that me. That means 
you don't understand this right and the only way you know and that you 
actually understand it is to go and write this in from scratch in pytorch 
or an umpire okay. And if you can't do that, then you know: okay, you don't 
understand it and you can go back and re-watch the lesson 6 and check out 
the notebook and copy some of the ideas until you can it's really important 
that you can write that from scratch. It's less than a screen of code. 
Okay, so you want to make sure you can create a two layer, iron, okay - and 
this is what it looks like if you unroll it. Okay, so that's the goal is to 
get to a point that we first of all, have these XY pairs of sentences and 
we're going to do French to English. So we're going to start by downloading 
this data set and training a translation model takes a long time. Google's 
translation model has eight layers of RNN stacked on top of each other, 
there's no conceptual difference between eight layers and two layers. It's 
just like if you're google and you have more GPUs GPUs and you know what to 
do with then you're fine doing that. Where else, in our case, it's pretty 
likely that the kind of sequence to sequence models we're building are not 
going to require that level of computation so to keep things simple, let's 
do a cut-down thing where, rather than learning, how to translate French 
into English for any Sentence: let's learn to translate French questions 
into English questions: okay and specifically, questions that start with 
what, where which, when okay, so you can see here, I've got a regex.</p>

<p>It 
looks for things at start of WH and in with a question mark, so I just go 
through the corpus open up each of the two files. Each line is one parallel 
text written together. I grab the English question the French question and 
check whether they match the regular submissions. Okay, dump, that out was 
the pickle, and so they don't have to do it again, and so we now have 
52,000 sentences, and here are some examples with us. Well, sentence pairs, 
and here are some examples of those one nice thing about this - is that 
what who, where clap questions, tend to be fairly short, which is nice, but 
I would say the idea that we could learn from scratch with no previous 
understanding of the idea Of language, let alone of English or a French 
that we could create something that can translate one to the other for any 
arbitrary question, with only 50,000 sentences. It sounds like a ludus 
cursory, difficult thing to ask us to do right, so I will be impressed if 
we can make any progress whatsoever. This is very little data to do a very 
complex exercise correct. So this contains the tuples of French and 
English. You can use this handy idiom just pick them apart into a list of 
English questions and list of french questions, and then we tokenize the 
English question questions and we tokenize the French questions. Okay, so 
remember that just means splitting them up into separate words or word like 
things by default.</p>

<p>The tokenizer that we have here and remember this is a 
wrapper around the Spacey tokenizer, which is a fantastic tokenizer. This 
wrapper by default assumes English okay. So to ask for French, you just add 
an extra parameter. The first time you do this you'll get an error, saying 
that you don't have the space you French model installed and you can Google 
to get the command something Python. Ms BAE see download French or 
something like that to grab the French model. Okay, I don't think any of 
your going to have RAM problems here, because this is not particularly big 
corpus, but I know that some of you we're trying to train new language 
models during the week and we're having RAM problems. If you do it's worth 
knowing what these functions are actually doing so, for example, these ones 
here is processing every sentence across multiple processes as well. The NP 
means and remember you, know Farsi. Our code is designed to be pretty easy 
to read, so you know three or four lines of code, so here's the three lines 
of code to process or MP find out how many CPUs you have divided by two, 
because normally with hyper-threading, they don't actually all work In 
parallel, then, in in parallel run, this process function, so that's going 
to spit out a whole separate Python process for every CPU. You have. If you 
have a lot of cause, that's a lot of Python processes, everyone's going to 
load the whole.</p>

<p>You know all this data in and that can potentially use up 
all your ramped, so you could replace that with just proc all rather than 
pro or MP to use less RAM or you could just pros use less cause. So but you 
know at the moment we were calling this function partition by cause which 
calls partition on a list and asks to split it into a number of equal 
length things according to how many CPUs you have so you could replace that 
you're, not splitting it Into a smaller list and read it on less things: 
</p>

<h3>5. <a href="https://youtu.be/bZmJvmxfH6I?t=27m15s">00:27:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Data Science Bowl 2017 (Cancer Diagnosis) on Kaggle</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Yes, Rachel was an intention layer tried in the language model. Do you 
think it would be a good idea to try and add one we haven't had learned 
about attention yet so, let's ask about things that we have cut to not 
things we haven't. The short answer is no, I haven't tried it properly. 
Yes, you should try it because it might help okay and you know in general, 
there's going to be a lot of things that we covered a day which, if you've 
done some sequence to sequence stuff before you're, want to know about 
something we haven't covered. Yet I'm going to cover all the sequence of 
sequence, things. Ok! So at the end of this, if I haven't covered the thing 
you wanted to know about, please ask me: then: if you ask me before I'll, 
be answering something based on something that I'm about to teach you, ok, 
so having tokenized the english and french, you can See how it gets bit out 
and you can see the tokenization for french is quite different, looking 
because french loves their apostrophes and their hi friends and stuff 
right. So if you try to use in english tokenizer for a french sentence, 
you're going to get it pretty. Crappy, ok, so, like I don't find you need 
to know heaps of NLP ideas to use deep learning for NLP, but just some 
basic stuff, like you know, use the right. Tokenize of your language is 
important, and so some of the students this week in our study group have 
been trying to work, build language models for Chinese instance, which of 
course doesn't really have the concept of a tokenizer.</p>

<p>In the same way, so 
we've been starting to look at it briefly mentioned last week. This google 
thing called sentence: peace which basically splits things into arbitrary 
sub word units, and so, when I say tokenize, if you're, using a language 
that doesn't have spaces in, you should probably be checking out sentence. 
Peace or some other similar sub word unit thing instead and hopefully in 
the next week or two we'll be able to report back with some early results 
of these experiments with Chinese. Okay, so have you tokenized to it, will 
save that disk and then remember. The next step after we create tokens is 
to turn them into numbers and turn them into numbers. We have two steps. 
The first is to get a list of all of the words that appear and then we turn 
every word into the index into that list right. If there are more than 
40,000 words that appear, then let's cut it off there, so it doesn't go too 
crazy and we insert a few extra tokens for beginning of stream, padding end 
of stream and okay. So if we try to look up something that wasn't in the 
40,000 most common, then we use a default dicked to return three, which is 
unknown so now we can go ahead and turn every token into an ID by putting 
it through the string to integer dictionary. We just created, and then at 
the end of that, let's add the number two which is industry and you'll, see 
like this kind. The code you see here is the code.</p>

<p>I write when I'm 
iterating in experimenting right because, like 99 % of the code I write 
when I'm either writing experimenting. It turns out to be totally wrong or 
stupid or embarrass seeing and you don't get to see it right. But like 
there's no point, you know refactoring that and making it beautiful when 
I'm riding it's kind of wanting you to see all the little shortcuts I have 
so like, rather than doing this properly and actually find you know having 
some constant or something for end of Stream marker and using it when I'm 
prototyping, I just do the easy stuff. You know I mean not not so much that 
I end up with broken code. You know, but I I don't you know, try to find. I 
try to find some middle ground between beautiful code, and you know code 
that was just heard him mention that we divide number of CPUs by two, 
because with hyper-threading we don't get a speed-up using all the hyper 
threaded cores. Is this based on practical experience or is there some 
underlying reason why we wouldn't get additional speed-up yeah? It's just 
practical experience and it's like a lot of things kind of seem like this, 
but I definitely noticed with tokenization hyper threading seem to slow 
things down a little bit. Also, if I use all the cause you know like often, 
I want to do something else. At the same time, like generally run some 
interactive notebook, and I don't have any spare room to do that.</p>

<p>It's a 
minor issue: yeah, okay, so now for our English and our French, we can grab 
our list of IDs and when we do that, of course, we need to make sure that 
we also store the vocabulary. There's no point having IDs. If we don't know 
like what the number five represents, there's no point having a number 
five. So that's our vocabulary. It's a string and the reverse mapping 
string to int that we can use to convert more courses in the future. Okay, 
so just to confirm it's working. We can go through each ID, convert the 
into a string and spit that out and there we have our thing back now with 
an industry marker. At the end, English vocab is 17,000. Our French vocab 
is 25,000, so you know there's not too big. You know there's not too 
complex a vocab that we're dealing with, which is nice to know okay, so we 
spent a lot of time on the forums during the week discussing how pointless 
what vectors are and how you should stop getting so excited about them and 
we're Now going to use them, why is that, basically, all the stuff we've 
been learning about using language models and pre-trained proper models 
rather than pre-trained? You know linear single layers, which is what word 
vectors are, I think, applies equally well to sequence, to sequence, but I 
haven't tried it yet yet so Sebastian - and I are you know, starting to 
look at that and slightly distracted by preparing this class at the moment. 
But after this class is done so there's a whole thing for anybody 
interested in creating some genuinely new like highly publishable results. 
The entire area of sequence, to sequence, with pre-trained language models 
hasn't been touched yet, and I strongly believe is going to be just as good 
as as classification style and if you you know, work on this and you get to 
the point where you have something.</p>

<p>That's looking exciting and you want 
help publishing it. You know I'm very happy to help co-author papers. You 
know that on stuff, that's looking good, you know so feel free to reach 
out. If and when you have some interesting results. So at this stage we 
don't have any of that, so we're going to use you know very little fast. I 
I actually and very little in terms of kind of fastai ideas. So we you 
know, all we've got is word vectors anyway, so, let's at least use decent 
word vectors. So word, Tyvek is very old. Where vectors there are better 
word vectors now and fast text is a pretty good source of word vectors. 
There's hundreds of languages available for them your language is likely to 
be represented so to grab them. You can click on this link download word 
vectors for a language that you're interested in install install the fast 
text, Python library, it's not available on pi PI, but here's a handy trick 
if there is a github repo that has like a setup PI inert and a Requirements 
text in it you can just chuck get Plus at the start and then stick that in 
your pip install and it works, like hardly anybody, seems to know this and 
like it never even like. If you go to the fast text repo, they won't tell 
you this they'll say you have to download it and CD into it and blah blah 
blah, but you're done. You can just run that ok, which you can also use for 
the fastai library by the way, if you want to pip install the latest 
version of past pay.</p>

<p>I even totally do this, so you grab the library 
import, it load the model. So here's my English model and here's, my French 
you'll see there's a text version and a binary version. The binary versions 
a bit faster, we're going to use that the text version is also a bit buggy 
and then I'm going to convert it into a standard Python dictionary to make 
it a bit easier to work with. So this is just going to grow through each 
word, with a dictionary, comprehension and save it as a pickle dictionary. 
Ok, so now we've got our pickle dictionary. We can go ahead and look up a 
word, for example, come up and that will return a vector. The length of 
that vector is the dimensionality of this set of word vectors. So in this 
case, we've got three hundred dimensional English and French words: okay,  
</p>

<h3>6. <a href="https://youtu.be/bZmJvmxfH6I?t=36m30s">00:36:30</a></h3>

<ul style="list-style-type: square;">

<li><b> DSB 2017: full preprocessing tutorial, + others.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>For reasons that you'll see in a moment, I also want to find out what the 
mean of my vectors are and the standard deviation of my vectors are so the 
means about 0 and the standard deviation is about point three. So remember 
that often corpuses have a pretty long, tailed distribution of sequence 
length and it's the longest sequences that kind of tend to overwhelm how 
long things take, and you know how much memory is used and stuff like that, 
so I'm going to grab you know in This case, the 99th to 97th percentile of 
the English and French and truncate them to that amount. Originally I was 
using the 90th percentile, so these are poorly named variable, so apologies 
for that, okay, so that's just truncating them so we're nearly there. We've 
got our tokenized numeric alized, English and French data set. We've got 
some word vectors, so now we need to get it ready for play torch, so app a 
torch, expects a data set object, and hopefully, by now you all can tell me 
that a data set object requires two things: a length and an indexer. So I 
started route writing this now. It's like okay, I need a sector sector 
dataset and I started out. Writing it and I thought okay, we're going to 
have to pass it out. X's now wise and store them away, and then my indexer 
is going to need to return a numpy array of the X's at that point in an 
umpire row of the Y's at that point, and oh that's it so then after I wrote 
this, I realized I Haven't really written a sector sector dataset I've just 
written a totally generic data set, so here's like the simplest possible 
data set that works for any pair of arrays.</p>

<p>So it's now poorly named it's 
much more general than a sector sick data set. But that's what I needed it 
for this a function. Remember: we've got V variables, teeth, tensors a4 
arrays. So this basically goes through each of the things you pass it. If 
it's not already in numpy array, it converts it into a numpy array and 
returns back at a pool of all of the things that you passed it which are 
now guaranteed to be mud pirates. So that's a VT 3 very handy little 
functions. Ok, so that's it! That's our data set. So now we need to grab 
our English and French IDs and get a training set and a validation set. And 
so one of the things which is pretty disappointing about a lot of code out 
there on the Internet is that they don't follow us and simple best 
practices. For example, if you go to the Play Torche website, they have an 
example section for sequence, a sequence, translation there example does 
not have a separate validation set. I tried it training according to their 
settings and I tested it with their validation set. It turned out that it 
overfit massively, so this is not just a theoretical problem: the actual 
pipe torch repo has the actual official sequence, the sequence translation 
example, which does not check for overfitting and overfits horribly. Also, 
it fails to use mini batches, so it actually fails to utilize any of the 
efficiency of pytorch whatsoever.</p>

<p>So there's a lot of like even if you find 
code in the official pipe torch repo, don't assume it's any good at all 
right. The other thing you'll notice, is that everybody were when they like 
pretty much every other sequence. The sequence model I've found in Peyer 
torch anywhere on the internet, has clearly copied from that shitty pipe or 
epoch isn't all the same variable names. It has the same problems. It has 
the same mistakes like another example: nearly every pytorch, convolutional 
neural network I found, does not use an adaptive pooling layer, so in other 
words, the final layer is always like average paul, 7. Comma 7 right. So 
they assume that the previous layer is 7 by 7 and if you use any other size 
input, you get an exception and therefore nearly everybody. I've spoken to 
that uses, Piatt torch thinks that there is a fundamental limitation of CN 
NS, that they are tied to the input size, and that has not been true since 
vgg right. So every time we grab a new model and stick it in the first day. 
I repo i have to go in search for paul and add adaptive to the start and 
replace the 7 with a 1, and now it works on any sized object. All right so 
just be careful, you know it's still early days and and believe it or not. 
Even though most of you have only started in the last year, your deep 
learning journey, you know quite a lot more about a lot of the more 
important practical aspects and the vast majority of people that are like 
publishing and writing stuff in official repos and stuff.</p>

<p>So you kind of 
need to have a little more self-confidence than you might expect when it 
comes to reading other people's code. If you find yourself thinking that 
looks odd, it's not necessarily you, it might well be then. Okay, so yeah, 
I would say, like at least 90 % of deep learning code that I start looking 
at turns out to have, like you know, like deathly, serious problems that 
make it completely unusable for anything. And so I kind of been telling 
people that I've been working with recently. You know if, if the repo 
you're looking at doesn't have a section on it saying here's the test we 
did where we got the same results as the paper that suspend to be 
implementing. That almost certainly means they haven't, got the same 
results in the paper. They're implementing they probably haven't even 
checked. Okay, and if you run it, it definitely won't get those results, 
because it's it's hard to get things right. The first time it takes me 12 
goes, you know, probably takes the normal, smarter people than me. Six 
goes, but if they haven't tested it once, it's almost certainly won't work. 
Okay, so there's our sequence sequence data set, let's get the training and 
validation, sets here's an easy way to do that. We have a bunch of random 
numbers. One feature of your data see if they're bigger than 0.1 or not, 
that gives you a list of balls index into your array with that list of 
balls to grab a training set index into that array, with the opposite of 
that list of balls.</p>

<p>To get your validation set, there's a nice easy way, 
there's lots of ways of doing it. I just like to do different ways to you: 
can see a few approaches. Okay, so now we can create our data set with our 
X's in our Y's, French and English. If you want to translate instead, 
English to French switch these two around and you're done. Okay, now we 
need to create data loaders. We can just grab our data loader and pass in 
our data set and match size. We actually have to transpose the arrays, I'm 
not going to go into the details about why we can talk about it during the 
week if you're interested, but have a think about why we might need to 
transpose their orientation. But there's a few more things. I want to do 
one is that since we've already done all the pre-processing, there's no 
point spawning off multiple workers to do like augmentation or whatever, 
because there know what to do so, making them workers equals. One will save 
you some time. We have to tell it what our padding index is. That's 
actually pretty important, because what's going to happen is that we've got 
different length sentences and faster. I I think it's pretty much. The only 
baby that does this past day. I will just automatically stick them together 
and pad the shorter ones to be so that they're all in that equal length 
because remember a tense, has to be rectangular. Okay in the decoder in 
particularly, I actually want my padding to be at the end, not at the start 
like for a classifier.</p>

<p>I want the padding at the start, because I want that 
final token to represent the last word of the movie review, but in the 
decoder as you'll see, it actually is going to work out a bit better to 
have the padding at the end. So I say: prepaired equals false and then, 
finally, since we've got sentences of different lengths coming in and they 
all have to be put together in a mini batch to be the same size by padding, 
we would much prefer that the sentence in a mini batch are Of similar sizes 
already, because otherwise that it's going to be as long as the longest 
sentence and that's going to end up wasting time and memory. Okay, so 
therefore, I'm going to use the sample of tricks that we learnt last time, 
which is the validation, set. We're going to ask it to sort everything by 
length, first, okay and then for the training set, we're going to ask it to 
randomize the order of things, but to roughly make it so that things of 
similar length are about in the same spot. Okay, that's how we got our sort 
sample and a sort, assembler, okay, and then at that point we can create a 
model. Data object. Remember a model data object really does one thing 
which is it says I have a trading set and a validation set and an optional 
test set and sticks them into a single object. We also have a path so that 
it has somewhere to store temporary files models. Stuff like that right, so 
you know we're doing we're not using fastai for very much at all.</p>

<p>In this 
example, just kind of a minimal set to show you like you know how, to you 
know kind of get your model data objects in the end, once you've got a 
model data object, you can then create a learner, and you can then call fit 
okay. So that's kind of like minimal amount of faster, faster. I stuff 
here. This is a standard height watch compatible data set. This is a 
standard plate or compatible data loader behind the scenes, it's actually 
using the class AI version, because I do need to do this automatic padding 
for convenience. So there's a few tweaks in our version that are a bit 
faster, a bit more convenient the faster ice samplers we're using. But you 
know, there's not too much going on here so now, we've got our model data 
object. We can basically tick off number one. Okay, so, as I said, most of 
the work is in the architecture, and so the architecture is going to take 
our sequence of tokens. Okay, it's going to spit them in to a encoder or 
you know, in kind of computer vision, turns what we've been calling a 
backbone. You know something that's going to try and turn this into some 
kind of representation. So that's just going to be an iron in okay, that's 
going to spit out the final hidden state which for each sentence it's just 
a vector single okay, and so that's all going to take that's none of this 
is going to be.</p>

<p>Do that's all going to be using very direct, simple 
techniques that we've already learnt and then we're going to take that and 
we're going to spread it into a different era, tan, which is a decoder and 
that's going to have some new stuff. Because we need something that can go 
through one word at a time: okay and it's got to keep going until it thinks 
it's finished, the sentence it doesn't know how long the sentence is going 
to be ahead of time keeps going until it thinks it's finished. The sentence 
and then it stops and returns a sentence. Okay. So let's start with the 
with the end coder. So in terms of variable naming here, there's basically 
identical variable for encoder and decoder buttes for encoder and decoder, 
the encoder versions. </p>

<h3>7. <a href="https://youtu.be/bZmJvmxfH6I?t=48m30s">00:48:30</a></h3>

<ul style="list-style-type: square;">

<li><b> A non-deep-learning approach to find lung nodules (research)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Have Inc the decoder versions have dead, okay, so for the N coder, here's 
our embeddings and so like. I always try to mention like what the mnemonics 
are. You know, rather than writing things out. You know so in turn 
longhand. So you know just remember: anchors and encoder deckers of decoder 
and visit embedding. The final thing that comes out is out. The r and n in 
this case is a gr. U not an LST M they're nearly the same thing. So don't 
worry about the difference. You could replace it with an LST m and you'll 
get basically the same results to replace it with an LST M, simply type OST 
M okay. So we need to create an embedding layer to take because remember 
what we're being passed is the index of the words into a vocabulary, and we 
want to grab there fast text embedding and then over time. We might want to 
also fine tune to train that embedding n to it so to create an embedding 
we'll call create embedding up here so we'll just say: n n dot. Embedding. 
So it's important that you know now how to set the rows and columns for 
your embedding. So the number of rows has to be equal to your vocabulary 
size, so each vocab riordan has a word vector and the. How big is your 
embedding well in this case it was determined by fast text and the first 
text embedding size 300. So we have to use size 300 as well, otherwise we 
can't start out by using their betting's okay now.</p>

<p>So what we want to do is 
this is initially going to give us a random set of embeddings, and so we're 
going to now go through each one of these and if we find it in fast text, 
we'll replace it with the first text admitting okay. So again, something 
that you should already know is that a pytorch module that is learn about 
has a weight attribute and the weight attribute is a variable and that 
variables have a data attribute, and the data attribute is a tensor. Now 
your notice very often today, I'm saying here, is something you should know 
not so that you think. Oh, I don't know that I'm a bad person right, but so 
that you think okay. This is a concept that you know I haven't learnt yet 
and Jeremy thinks I ought to know about, and so I've got to write that down 
and I'm gon na go home and I'm gon na like Google because, like this is a 
normal pie, Torche attribute in Every single learning platform module this 
is a normal plate or attribute in every single pipe torch variable. And so, 
if you don't know how to graph the weights out of a module or you don't 
know how to grab the tensor out of a variable, it's gon na be hard for you 
to build new things or to debug things or maintain things or whatever. Yes, 
so if I say you ought to know this and you're thinking, I don't know this 
don't run away and hide, go home and learn the thing and, if you're having 
trouble learning the thing, because you can't find documentation about it 
or you don't understand that documentation Or you don't know why Jeremy 
thought it was important.</p>

<p>You know it jump on the forum and say like please 
explain this thing, here's my best understanding of that thing as I have it 
at the moment. Here's the resources. I grew up that helped film you, okay 
and normally. If I respond it's very likely, I will not tell you the 
answer, but I will instead give you something, a problem that you could 
solve that if you solver will solve it for you, because I know that that 
that way, it'll be something you remember. Okay, so again don't be put off. 
If I'm like. Okay, you like go read this link. Try and summarize that thing 
tell us what you think, like I'm trying to be helpful, not unhelpful, and 
if you're still not following just come back and say like I had to look 
honestly that link you sent. I don't know what I knew. That means I 
wouldn't know where to start whatever, like I'll, keep trying to help you 
until you fully understand it. Okay, so so now that we've got our weight 
tensor, we can just go through our vocabulary and we can look at the word 
in our pre-trained vectors and if we find it, we will replace the random 
weights with that pre-trade vector the random weights. Have a standard 
deviation of 1, our pre-trained vectors, it turned out how to standard 
deviation of about 0.3. So again, this is the kind </p>

<h3>8. <a href="https://youtu.be/bZmJvmxfH6I?t=53m">00:53:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Clustering (and why Jeremy wasn’t a fan before)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Of hacky thing I do when I'm prototyping stuff, I just multiply two by 
three. Okay, obviously, by the time you you know see, the video of this 
mean way, we'll have put all this sequence to sequence, stuff into the 
faster library. You won't find horrible paths like that and there sure 
hope, but hack away when you're prototyping some things won't be in fast 
text, in which case we'll just keep track of it, and I've just added this 
print statement here, just so that I can kind of see. What's going like, 
why am I missing stuff? Basically, when you know I'll probably comment it 
out when I actually commit this to github? That's why that's there? Okay, 
so we create those embeddings and so when we actually create the sequence 
to sequence, RNN it'll print out how many or missed - and so remember we 
had like about 30,000 words, so we're not missing too many and interesting 
the things that are missing well, there's our Special toque there's a 
special token for uppercase, not surprising. That's missing. Also, remember 
it's not token Tyvek, it's not token text it's. You know it does words. So 
L, apostrophe and D apostrophe and apostrophe s they're not appearing 
either. So that's interesting. That does suggest that. Maybe we could have 
slightly better embeddings if we try to find some which would being 
tokenized the same way.</p>

<p>We tokenize that's okay, Rachel. Do we just keep 
embedding vectors from training? Why don't we keep all word embeddings in 
case you have new words in the test set? Oh, I mean we're gon na be 
fine-tuning them, you know and so yeah I don't know. I mean it's an 
interesting idea. Maybe that would work. I haven't tried it. I mean, 
obviously you wouldn't so. Can you use that? I asked the question so you 
can also add random embedding to those and the beginning. Just keep them 
random on, but you're gon na have bigs gon na make it an effect in the 
sense that you are going to be using those words yeah yep. I think it's an 
interesting line of inquiry, but I will say this: the vast majority of the 
time when you're kind of doing this in the real world, your vocabulary will 
be bigger than 40-thousand and once your vocab areas, bigger than 
40-thousand using the standard techniques you The embedding layers gets so 
big that it takes up all your memory. It takes up all of the time and the 
backdrop there are tricks to dealing with very large vocab Riis. I don't 
think we'll have time to handle them in this session, but you definitely 
would not want to have all three and a half million fast text vectors in a 
in an embedding layer. So I I wonder so, if you're not touching a word, 
it's not gon na change. Right like you and we are fine-tuning right.</p>

<p>You 
are not it's in GPU Ram and you're gon na remember three and a half million 
times three hundred times the size of a single precision floating point 
vector plus all of the gradients for them. Even if it's not touched like 
they're, without being very careful and adding a lot more code and stuff, 
it is slow and hard and when we wouldn't touch it for now, but as I say, I 
think, there's an interesting path of inquiry. But it's the kind of path of 
inquiry that leads to like multiple academic papers. Not you know something 
that you do on a weekend, but I think would be very interesting for ya. 
Maybe we can look at at some time. You know, and as I say, I have actually 
started doing some stuff around incorporating large for cavalry handing 
handling into fastai. It's not finished, but hopefully by the time we get 
here. This kind of stuff would be possible. Ok, so we create our encoder. 
Embedding add a bit of drop out, ok and then we create our an end. This 
input to the RNN, obviously is the size of the embedding by Bodie by 
definition, number of hidden is whatever we watch, so we set up to 256 for 
now. However many layers we want hands and drop out inside the arent as 
well. Okay, so this is all standard height watch stuff. You could use ur LS 
TM here as well and then finally, we need to turn that into some output 
that we're going to feed to the decoder. So, let's use a linear layer to 
convert the number of hidden into the decoder embedding sites.</p>

<p>Okay, so in 
the forward pass, here's how that's used! We first of all initialize our 
hidden state to a bunch of zeros okay. So we've now had a vector of zeros, 
which we and then we're going to take our input and put it through our 
embedding we're going to put that through drop out. We then pass our 
currently zeros hidden state and our embeddings into our R and N, and it's 
going to spit out the usual stuff that I ran in spit up, which includes the 
final hidden state. We're then going to take that final hidden stake and 
stick it through that linear layer. So we now have something of the right 
size to feature our decoder. Okay, so that's! That's it and again, this is 
like ought to be very familiar and very comfortable. It's like the most 
simple possible iron n. So if it's not go back check out lesson, six make 
sure you can write it from scratch and that you understand what it does. 
But the key thing to know is that it takes our inputs and spits out a 
hidden vector that hopefully will learn to contain all of the information 
about what that sentence says and how it says it, because if it can't do 
that, all right, if we can't Do that, then we can't feed it into a decoder 
and hope it to spit out our sentence in a different language. So that's 
what we want it to learn to do and we're not going to do anything special 
to make it learn to do that. We're just gon na, do you know the three 
things and cross our fingers, because that's what we do all right so that's 
H is.</p>

<p>Is that yes, all right, so it's a hidden stick. I guess Stephen used 
s for state. I used HB hidden, but there you go. You would think the two 
Australians could agree on something like that, but apparently not so. How 
do we now do the new bit right, and so the basic idea of the new bit is the 
same. We're going to do exactly the same thing, but we're going to write 
our own forward. Okay, and so the full loop is going to do exactly what the 
for loop inside pytorch does here, but we're going to do it manually right, 
so we've got to go through the forward and how big is the for loop? It's a 
output sequence length! Well, what is output sequence length? That's 
something that passed to the constructor and it is equal to the length of 
the largest English sentence. Okay, so we're going to do this for loop as 
long as the largest English sentence, because we're translating into 
English right. So we can't possibly be longer than that, at least not in 
this corpus. If we then used it on some different corpus that was longer, 
this is going to fail. So, but you could make this, you know you could 
always pass in a different parameter. Of course, all right, so the basic 
idea is the same: we're going to go through and put it through the in the 
embedding we're going to stick it through the R and n we're going to stick 
it through drop out and we're going to stick it through A linear layer all 
right, so the basic four steps are the same and once we've done that, we're 
then going to append that output to a list fat and then, when we're going 
to finish we're going to stack that list up into a single tensor and 
return. It okay, that's the basic idea: normally a recurrent neural network 
here is our decoder.</p>

<p>Current neural network, recurrent neural network works 
on a whole sequence at a time, but we've got a for loop to go through each 
part of the sequence separately. So we have to add a leading unit access to 
the start. To basically say this is a sequence of length: 1. Okay, so we're 
not really taking advantage of the recurrent net. Much at all. We could 
easily rewrite this with a linear layer. Actually, that would be an 
interesting experiment if you wanted to try it, so we basically take our 
input and we feed it into our embedding, and we add something to the front 
saying treat this as a sequence of length 1 and then we pass that to our M 
we then get the output of that RNN feed it into our dropout and 15:20 me a 
layer, so there's two extra things to know that be aware of. Well, I guess 
it's really one thing. The one thing is what's this: what is the input to 
that? Embedding, okay - and the answer is it's the previous word that we 
translated see how the input here is the previous word here. The input here 
is the previous word here. So the basic idea is, if you're, trying to 
translate if you're about to translate. You know tell me the fourth word of 
the new sentence, but you don't know what the third word you just said was 
that's going to be really hard all right, so we're gon na feed that in at 
each time, step let's make it as easy as possible.</p>

<p>Okay, and so what was 
the previous word at the start for the Rawls month? Okay, so specifically, 
we're gon na start out with a beginning of stream, took it okay, so the 
beginning of stream token is a zero. So, let's start out our decoder with a 
beginning of stream token, which is zero, okay and, of course, we're doing 
a mini batch. So we need batch size number of them. But let's just think 
about one part, so we've got we start out with a zero. We look at that zero 
in our in our embedding matrix to find out what the vector for the 
beginning a stream token. Is we stick a unit axis on the front to say we 
have a single sequence length of beginning of stream token. We stick that 
through our RM, which gets not only the fact that there's a zero is 
beginning of stream, but also the hidden state which, at this point is 
whatever came out of our encoder okay. So this now its job is to try and 
figure out what it's the first word. Okay, what's the first word to 
translate the sentence pop through some drop out go through one linear 
layer in order to convert that into the correct size for our decoder 
embedding matrix? Okay, append that to our list of translated words, and 
now we need to figure out what word that was because we need to feed it to 
the next time step. We need to feed it to the next time step. Okay, so 
remember what we actually output here and and look at use a debugger right, 
pdb, dot set trace, put it here. What is at P now P is a tensor.</p>

<p>How big is 
the tensor so before you look it up in the debugger, try and figure it out 
from first principles and check you're right, so our P is a tensor whose 
length is equal to the number of words in our English vocabulary, and it 
contains the Probability for every one of those words that it is that word 
makes sense right. So then, if we now say at P dot max that looks in its 
tensor to find out, which word has the highest probability: okay and max 
imply Torche returns two things. The first thing is: what is that max 
probability, and the second is: what is the index into the array of that 
mass probability, and so we want that. Second item index number, one, which 
is the word index with the largest thing. Okay, so now that contains the 
word well, the word index into a VOC, a pre of the word. If it's a one 
right, you might remember one was padding, then that means we're done 
right. That means we've reached the end because we finished with a bunch of 
padding okay. If it's not one, let's go back and continue, but now Dec M is 
whatever the highest probability. Word was, but so we keep looping through 
either until we get to the largest length of a sentence or until everything 
in our mini batch is padding and each time we've appended our outputs each 
time, not not the word with the probabilities.</p>

<p>Okay to this list, which we 
stack up into a tensor, and we can now go ahead and feed that to a loss 
function so before we go to a break since we've done one and two: let's do 
three, which is the last function. The last function is categorical: 
cross-entropy loss. Okay, we've got a list of probabilities for each of our 
classes that the classes are all the words in our English vocab and we have 
a target which is which is the correct class ie, which is the correct word 
at this location. There's two tweaks, which is why we need to write our own 
little loss function, but you can see basically it's going to be cross 
interview. That's right and the tweaks are as follows: tweak number one is, 
we might have stopped a little bit early right and so the sequence length 
that we generated may be different to the sequence length of the target, in 
which case we need to add some padding right Height or padding function is 
weird if you have a rank: three cancer </p>

<h3>9. <a href="https://youtu.be/bZmJvmxfH6I?t=1h8m">01:08:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Using Pytorch with GPU for ‘meanshift’ (clustering cont.)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Which we do, we have batch size by sorry, we have sequence length by batch 
size by number of words in the vocab Iraq. Three tensor requires a six 
tupple each pair, and things in that topple is the padding before and the 
padding after that. That dimension all right, so in this case the first 
dimension has no padding. The second dimension has no padding. The third 
dimension has no padding on the left and as much padding is required on the 
right okay. So it's good to know how to use that function. Now that we've 
added any padding that's necessary, the only other thing we need to do is 
cross. Entropy loss expects a rank. Two tensor I mentioned matrix, but 
we've got sequence length by batch size. So, let's just flatten out the 
sequence length and batch size into a that's. What that minus one in view 
does okay, so flatten out that for both of them and now we can go ahead and 
call cross-entropy, that's it! So now we can just use the standard 
approach: here's our sequence of sequence, R and n. That's this one yeah! 
So that is a standard! Pipe watch module! Stick it on the GPU. Hopefully, 
by now you've noticed you, you can call CUDA, but if you call to GPU, then 
it doesn't put it on the GPU. If you don't have one you can also set 
faster. I don't use GPU to false to force it to not use the GPU, and that 
can be super handy for debugging.</p>

<p>We then need something that tells it how 
to handle learning rates, learning rate groups, so there's a thing called 
single model that you can pass it to which treats the whole thing as a 
single learning rate group. So this is like this easiest way to turn a 
height watch module into a fastai model. Here's the model data object we 
created before. We could then just call lerner to turn that into a learner, 
but if we call our NN learner, our NN learner is a learner. It defines 
cross-entropy as the default criteria. This case we're overriding that 
anyway. So that's not what we care about, but it does add in these save 
encoder and load encoder things that can be any sometimes so we could have 
in this case we're really pretty to set learner, but Aaron Lerner also 
works. Ok, so here's how we turn our height watch module into a fast AO 
model into a and once we have a learner, give it our new loss function and 
then we can call LR find and we can call Fitch and it runs through awhile 
and we can Save it, you know the normal learn stuff now works. The remember 
the model attribute of a learner is a standard piped watch model. So we can 
pass that sum X, which we can grab out of our validation set or you could 
use lo, not predict array or whatever you like. If you get some predictions 
and then we can convert those predictions into words by grabbing going dot 
max 1 to grab the index of the highest probability, words to get some 
predictions and then we can go through a few examples and print out the 
French, the correct English and the predicted English for things that are 
not padding - and here we go alright, so amazingly enough, this kind of 
like simplest possible, written largely from scratch, pytorch module and 
only fifty thousand sentences is sometimes capable on a validation set of 
giving you exactly the Right answer: sometimes the right answer in slightly 
different wording and sometimes sentences that really are grammatically 
sensible or even have too many question marks.</p>

<p>So we're we're well on the 
right track. I think you would agree so you know even the simplest possible 
sector, SiC trained for a very small number of epochs without any you know, 
pre-training other than the use of word. Embeddings, surprisingly good. So 
I think you know the message here and we're going to improve this for the 
moment after the break, but I think the message here is even sequence to 
sequence models. You think, as simple of them to possibly work, even with 
less data than you think you could learn from can be surprisingly and in 
certain situations this main, even you know, be enough for your needs, so 
we're going to learn a few tricks after the break, which Will make this 
much better? So let's come back at 750, so one question that came up during 
the break is that some of the tokens that are missing in fast text like had 
a a curly quote rather than a straight quote, for example, and the question 
was: would it help to normalize Punctuation and the answer for this 
particular case is probably yes: the difference between curly quotes and 
straight quotes is rarely semantics. You do have to be very careful, 
though, because, like it may turn out that people using beautiful curly 
quotes like using more formal language and they're. Actually, writing in a 
different way.</p>

<p>So I generally, you know if you're going to do some kind of 
pre-processing like punctuation normalization, you should definitely check 
your results with and without because, like nearly always that kind of 
pre-processing makes things worse, even when I'm sure it won't hello. What 
would be some ways over what I seen these sequence of sequence besides 
through power and weight? Again, let me think about that during the week 
yeah. It's like you, know a wdl STM, which we've been relying on a lot. Has 
so many great I mean it's. It's all drop out: well, not all drop out those 
drop out of many different kinds, there's and then there's the we haven't 
talked about it much, but there's also a kind of a regularization based on 
activations and stuff like that as well and on changes and whatever I just 
haven't seen anybody put in a thing like that amount of work into 
regularization of sequence, to sequence models and I think, there's a huge 
opportunity for somebody to do like the AWD LS TM of sectors ik, which 
might be as simple as dealing all the Ideas from a top UDS, VLS TM and 
using them directly in Sector SEC. That would be pretty easy to try. I 
think, and there's been an interesting paper that actually Steven Rarity's 
added in the last couple of weeks where he used an idea, which I don't know 
if he stole it from me, but it was certainly something I had also recently 
done and talked about on Twitter. Either way, I'm thrilled that he's done 
it, which was to take all of those different AWD, LS, TM, hyper parameters 
and train a bunch of different models.</p>

<p>And then I use a random forest to 
find out with feature importance, which ones actually met at the most and 
then figure out like how to set them yeah. So I think you could totally you 
know, use this approach to figure out. You know, for sequence, the 
sequence, regularization approaches which one's the best and optimize them, 
and that would be amazing yeah. But at the moment I think you know I. I 
don't know that there are additional ideas to sequence, the sequence 
regularization that I can think of beyond. What's in that paper for regular 
language models, stuff and probably all those same approaches would would 
work. Okay, so tricks trick number one go bi-directional: okay, so for 
classification, my approach to bi-directional that I suggested you use is 
take all of your token sequences, spin them around and train a new language 
model and train a new traffic classifier, and I also mentioned the wiki 
Text pre train model, if you replace FWD with vwd in the name, you'll get 
the pre-trained backward model I created for you, okay, so you can use that 
get a set of predictions and then average the predictions just like a 
normal ensemble, okay and that's kind of How we do Baidu for that kind of 
classification there maybe ways to do it end to end, but I haven't quite 
figured them out yet they're, not in fastai yet, and I don't think 
anybody's written a paper about them yet so, if you figure it out, That's 
an interesting line of research, but because we're not doing you know 
massive documents where we have to kind of chunk it into separate bits and 
then pall over them and whatever we can do by do very easily.</p>

<p>In this case, 
which is literally as simple as adding bidirectional equals true to our 
encoder, people tend not to do bi-directional for the decoder. I think, 
partly because it's kind of considered cheating, but I don't know like I 
was just talking to somebody to break about it. There, you know, maybe it 
can work in some situations, although it might need to be more of a 
ensemble approach in the decoder, because you kind of it's a bit less 
obvious anyway, and the ink into the encoder. It's very very simple: 
bi-directional equals true, and we now have with bi-directional equals 
true, rather than just having an RNN which is going this direction. We have 
a second RNN, that's going in this direction and so that second RNN 
literally is visiting them each token in the opposing order. So when we get 
the final here in state, it's here rather than here right, but the hidden 
state is of the same size. So the final result is that we end up with a 
tensor. That's got an extra to long axis right and you know, depending on 
what library you use often that will be then combined with the number of 
layers things. So if you've got two layers and bi-directional that tensor 
dimension is now length, four with pytorch, it kind of depends which bit of 
the process you're looking at as to whether you get a separate result for 
each layer and offer each bi-directional bit and so forth.</p>

<p>You have to cut 
the docs and it will tell you inputs/outputs cancer sizes appropriate for 
the number of layers and whether you have bi-directional equals true. In 
this particular case, you'll basically see all the changes I've had to make 
so, for example, you'll see when I added bi-directional, it was true. My 
linear layer now needs number of hidden times to to reflect the fact that 
we have that second direction in our hidden state now you'll see in in it 
hidden. It's now self dot number of layers times two here. Okay, so you'll 
see there's a few places where there's been an extra two that has to be 
thrown in yes, your net um. Why making any color by the original is 
considered cheating? Well, it's it's not just this cheating! It's like we 
have this loop going on. You know it's not as simple as just kind of having 
two tenses and then like. How do you turn those two separate loops into a 
final result? Hey you know, after talking about it, during the break I've 
kind of gone from like hey everybody knows it doesn't work to like. Oh, 
maybe it kind of could work, but it requires more thought. It's quite 
possible. During the week I realize it's a dumb idea and I was being stupid 
but well think about it. Another question people had: why do you need to 
have an into that look? Why do I have a watch of the loop? Why do you need 
to like have a an end to that look, you have like a range if your range 
yeah, oh, I mean it's because when I start training everything's random, so 
this will probably never be true so later on it'll, pretty much always 
break out.</p>

<p>Eventually, but yeah, it's basically like we're going to go for 
it. It's really important to remember like when you're designing an 
architecture that, when you start the model, knows nothing about anything 
right, so you kind of want to make sure it's doing something at least 
vaguely sensible. Okay, so bi-directional means we had. You know: let's see 
how we go here, we got out to 358 cross entropy loss, okay, with a single 
direction with fire direction, gets down to 351 yeah so that improved it a 
bit. That's good and, as I say the only you know it shouldn't really slow 
things down too much. You know, bidirectional does mean there's a little 
bit more sequential processing. </p>

<h3>10. <a href="https://youtu.be/bZmJvmxfH6I?t=1h22m15s">01:22:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Candidate Generation and LUNA 16 (Kaggle)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Have to happen, but you know it's generally, a good win in the Google 
Translation model of the eight layers. Only the first layer is 
bi-directional because it allows it to do more in parallel. So if you 
create really deep models, you may need to think about which ones are 
bi-directional. Otherwise we have performance issues, okay and so 351. Now, 
let's talk about teacher for C so teacher forcing is I'm going to come back 
to this idea that when the model starts learning, it knows nothing about 
nothing. So when the fertile starts learning it is not going to spit out. 
Ah, at this point, it's going to spit out some random meaningless work 
acts. It doesn't know anything about German or about English or about the 
idea of language or anything. And then it's going to feed it down here as 
an input and be totally unhelpful yeah, and so that means that early 
learning is going to be very, very difficult because it's feeding in an 
input that's stupid into a model. That knows nothing and somehow it says, 
get better right. So that's it's not asking too much. Eventually it gets 
there, but it's definitely not as helpful as we can be. So what if, instead 
of feeding in what? If, instead of feeding in the thing I predicted just 
now right what? If, instead, we see it in the actual correct word, it was 
meant to be right. Now we can't do that at inference time, because, by 
definition we don't know the correct word it has to translate it.</p>

<p>We can't 
require the correct translation in order to do translation right. So the 
way I've set this up is I've got this thing called PR force, which is 
probability of forcing, and if some random number is less than that 
probability, then I'm going to replace my decoder input with the actual 
correct thing right and if we've already gone Too far, and if it's already 
longer than the target sentence, I'm just going to stop well, obviously I 
can't give it the correct thing, so you can see how beautiful pytorches for 
this right because like if you try to do this with some static grafting, 
like Like classic tensorflow well, I tried right, like one of the key 
reasons that we switched to pytorch at this exact point in last year's 
class was because Jeremy tried to implement it. You're forcing in chaos 
intensive flow and went even more insane and he started right and I was 
like it was weeks of getting nowhere, and then I literally on Twitter. I 
think it was Andre, capaci, eyesore announced say it's something about. Oh 
there's this thing called pytorch just came out and it's really cool and 
I've tried it that day by the next day I had teacher flossing would and so 
like. I was like, oh my gosh, you know and like all the stuff of trying to 
debug things, it was suddenly so much easier, and this kind of you know 
dynamic stuff is so much easier. So this is a great example of like hey.</p>

<p>I 
get to use random numbers and, if statements and stuff so yeah, so here's 
the basic idea is at the start of training. Let's set P our force really 
high right so that nearly always that gets the actual correct. You know 
previous word, and so it has a useful input right and then, as I train a 
bit more, let's decrease PR force so that by the end, PF force is zero and 
it has to learn properly, which is fine, because it's now actually feeding 
in sensible Inputs most of the time anyway, so let's now write something 
such that in the training loop, it gradually decreases pyaare force. So how 
do you do that? Well, one approach would be to write our own training loop, 
okay, but let's not do that because we already have a training loop that 
has progress, bars and uses exponential weighted averages to smooth out the 
losses and keeps track of metrics. And you know it does a bunch of things 
which they're not rocket science. </p>

<h3>11. <a href="https://youtu.be/bZmJvmxfH6I?t=1h26m30s">01:26:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Accelerating K-Means on GPU via CUDA (research)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>But they're kind of convenient and they also kind of keep track of you - 
know calling the reset hour and ends at the start of an epoch, to make sure 
that the hidden states set to zeros - and you know little things like that. 
We'd, rather not have to write that in scratch. So what we've tended to 
find is that, as I start to kind of write some new thing and I'm like oh, I 
need to kind of replace some part of the code. I then kind of add some 
little hook so that we can all use that book to make things easier in this 
particular case. There's a book that I've ended up using all the damn time 
now, which is the hook called the stepper. And so if you look at our code 
model, dot, pi </p>

<h3>12. <a href="https://youtu.be/bZmJvmxfH6I?t=1h27m15s">01:27:15</a></h3>

<ul style="list-style-type: square;">

<li><b> ChatBots ! (long section)</b></li>

<li><b>Staring with “memory networks” at Facebook (research)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Is where our fit function lives right and so the fit function and model 
dot. Pi is kind of we've seen it before. I think it's like the lowest level 
thing that doesn't require and learner. It doesn't really require anything 
much at all. This requires a standard, pytorch model and a model data 
object. You just need to know how many epochs are standard, pipe torch, 
optimizer and the standard page watch boss function all right, so you can 
call. I don't. We've hardly ever used it in the class. We normally call 
learn but fit, but learn dot fit calls this. So this is our lowest level 
thing, but we filtered the source code here. Sometimes, we've seen how it 
loops through each epoch and that lives through each thing in our batch and 
calls step or dot, step right and so step or dot step is the thing that's 
responsible for calling the model getting the last finding the last 
function and calling The optimizer, and so by default, step or dot step 
uses a particular class called stepper, which there's a few things you 
don't know where. Basically, it calls the model all right. So the model 
winds up inside M Zero's the gradients cause the loss function, calls 
backwards, does gradient flipping if necessary, and then calls the 
optimizer alright. So you know they're the basic steps that back when we 
looked at kind of pytorch from scratch. We had to do so. The nice thing is, 
we can replace that with something else, rather than replacing the training 
loop right.</p>

<p>So if you inherit from step up and then write your own version 
of step right, you can just copy and paste the contents of step and add 
whatever you like right or if it's something that you're going to do before 
or afterwards, your could even call super Dot step in this case, I rather 
suspect I've been unnecessarily complicated. Here I probably could have 
replaced, commented about all of that and just said super darts, DEP X's, 
comma Y comma epoch, because I think this is an exact copy of everything 
right. But you know, as I say when I'm prototyping, I don't think carefully 
about how to minimize my code. I copied and pasted the contents of the code 
from step and I added a single line to the top, which was to replace PR 
force in my module with something that gradually decreased linearly for the 
first 10 a box and after 10 B box it was zero. Okay, so total hack, but 
good enough to try it out, and so the nice thing. What is that I can now 
you know everything else is the same: I've replaced I've added these three 
lines of code to my module and the only thing I need to do other that's 
differently is when I call fit. Is I pass in my customized step, a class 
okay and so that's going to do teacher forcing, and so we don't have 
bi-directional, so we're just changing one thing at a time. So we should 
compare this to our unidirectional results, which was three point.</p>

<p>Five, 
eight - and this is three point - four: nine okay, so that was an 
improvement, so that's great needed to make sure at least two ten epochs, 
because before that it was cheating by using the teacher forcing so yeah 
okay. So that's good! That's an improvement! So we've got another trick, 
and this next trick is a it's a bigger trick. It's a it's a pretty cool 
trick and it's it's called attention and the basic idea of attention is 
this, which is expecting the entirety of the sentence to be summarized into 
this single hidden vector is asking a lot. You know it has to know what was 
said and how it was said and everything necessary to create the sentence in 
general, and so the idea of attention is basically like. Maybe we're asking 
too much all right, particularly because we could use this form of model 
where we output every step of the loop to not just have a hidden state at 
the end, but to hit a hidden state after every single word and like. Why 
not try and use that information? It's it's like it's already there, but so 
far, we've just been throwing it away, and not only that, but by 
directional we've got every step. We've got to. You know vectors of state 
that we can use. So how could we use this piece of state this piece of 
state, this piece of state, this piece of state and this piece of state, 
rather than just the final state, and so the basic idea is well. Let's say 
I'm doing this word translating this word right now, which of these five 
pieces of state.</p>

<p>Do I want and of course the answer is, if I'm doing well, 
actually, that's bigger more interesting would pick this one. So if I'm 
trying to do loved, then clearly the hidden state I want is this one right, 
because this is the word okay and then for for this preposition of little 
preposition, whatever this little word here? No, it's not a preposition. I 
guess it's part of the boat so for this part of the verb, I probably would 
need like this and this and this to kind of make sure that I've got kind of 
the tense right and know that I actually need this part of the verb. And so 
forth, so depending on which bit I'm translating I'm going to need one or 
more bits of this, of these various hidden States, and in fact you know 
like I probably want some weighting of them so like what I'm doing here, I 
probably mainly want this State right, but I maybe run a little bit of that 
one and a little bit of that one right. So, in other words, for these five 
pieces of hidden state, we want to wait an average right and we wanted 
waited by something that can figure out which bits of the sentence the most 
important right now. So how do we figure out? Something like which bits are 
the symptoms are important right now we created a neural net and we train 
the neural net to figure it out. When do we train that you're on it and to 
end so, let's now train to neural nets? Well, we've actually already kinda 
got a bunch right, we've got an hour and an encoder.</p>

<p>We got an RNN decoder. 
We've got a couple of linear layers. What the hell, let's add, another 
neural net into the mix. Okay and this neural net is going to spit out a 
wait for every one of these things and we got to take the weighted average 
at every step and it's just another set of parameters that we learn all at 
the same time. Okay, and so that's called attention, so the idea is that 
once that attentions been learned, we can see this terrific demo from Chris 
Olerud roncada. Each different word is going to take a weighted average, 
see how they're weighted the weights are different, depending on which word 
is being later all right, and you can see how it's kind of figuring out the 
color. The deepness of the blue is how much weight it's using. You can see 
that each word is basically or here at which word are we transferring from 
so when we say European, we need to know that both of these two parts, you 
only influenced ordering economic. Both these three parts are very 
influenced, including the gender of the the definite article and so forth, 
right so check out this distill. The pub article are, these things are all 
like little Hospital interactive diagrams, basically shows you how weights 
how attention works and what the actual attention looks like, and a trained 
translation model. Okay. So, let's try and implement attention so with 
attention it's basically, this is all identical right and the encoder is 
identical and all of this bit of the decoder is identical.</p>

<p>There's one 
difference which is that we see where this happens here we go. We basically 
are going to take a weighted average and the way that we're going to do the 
weighted average is we create a little neural net which we're going to see 
here and here and then we use softmax because of course, the last thing 
about softmax is That we want to ensure that all of the weights that we're 
using add up to 1 - and we also kind of expect that one of those weights 
should probably be quite a bit higher than the other ones right and so soft 
mattes gives us the guarantee that They add up to 1 and because it's that 
aid of that unit it tends to encourage one of the weights to be higher than 
the other ones all right. So, let's see how this works. So, what's going to 
happen, is we're going to take the last layers? Hidden state and we're 
going to stick it into a linearlayout and then we're going to stick it into 
a nonlinear activation and then we're going to do matrix multiply, and so, 
if you think about it, linear layer, nonlinear activation matrix multiply, 
that's a neural net. It's a neural net with one hidden layer: okay, stick 
it into a softmax okay and then we can use that to weight our air encoder 
outputs, okay. So now, rather than just taking the last encoder output, 
we've got, this is going to be the whole tensor of all of the encoder 
outputs, which I just weight by this little neural net that I created and 
that's basically it right so , okay.</p>

<p>So what I'll do is I'll put on the 
wiki thread couple of papers to check out there was. There was basically 
one amazing paper that really originally introduced this idea of attention, 
and I say amazing because it actually introduced a couple of key things 
which have really changed how people work in this field. They say, area of 
attention has been used, not just for text but for you know things like 
reading text out of pictures or kind of doing various stuff with computer 
vision and stuff like that, and then there's a second paper which actually 
Geoffrey Hinton was involved in Called grammar as a foreign language which 
used this idea of Iranians with attention to basically try to replace rules 
based grammar with an R and n which automatically basically tagged the 
grammatical. You know each word based on this grammar and turned out to do 
it better than any rules based system which today, like actually kind of, 
seems obvious. I think we're now used to the idea that neural nets do lots 
of this stuff better than rules-based systems, but at the time I was 
considered really surprising anyway. One nice thing is that they're kind of 
summary of how attention works is really nice and concise. You know, can 
you please explain a thing again sure so here's the idea, I like that nice 
crisp request. That's very easy to understand.</p>

<p>Okay: let's go back and look 
at our original encoder, so an eridan spits out two things: it spits out a 
list of the the state after every time step, and it also tells you the 
state at the last time step and we used the state at the Last time, step to 
create the input state for our decoder, okay, which is what we see you won, 
but we know that it's actually creating a vector at every time step. So, 
wouldn't it be nice to use them all right, but wouldn't it be likes to use 
the one or ones that's most relevant to translating the word I'm 
translating now. So, wouldn't it be nice to be able to take a weighted 
average of the hidden state at each time step weight it by whatever is the 
appropriate weight right now, which, for example, in this case litter, 
would definitely be time step number two: here's what it's all About 
because that's the word I'm translating so how do we get? How do we get a 
list of weights that is suitable for the word? We're training right now? 
Well, the answer is by training a neural net to figure out the list of 
weights, and so anytime. We want to figure out how to train a little neuron 
that that does and tasks the easiest way normally always to do. That is to 
include it in your module and train it in line with everything else. The 
minimal possible neural net is something that contains two layers and one 
nonlinear activation function. So here is one linear layer, okay, and in 
fact you know.</p>

<p>Instead of a linear layer, we can even just grab a random 
matrix if we don't care about bias right and so here's a random matrix, 
it's just a random tensor wrapped up in a parameter. A parameter. Remember 
is a is just a pytorch variable. It's like identical to a variable that it 
just tells pytorch. I want you to learn the weights for this place. 
Alright. So here we've got a linear layer. Here, we've got a random matrix, 
and so here at this point where we start out our decoder. Let's take that 
final, let's take the current hidden state of the tked of the decoder right 
put that into a linear layer. Right because, like how what's the 
information we use to decide what words we should focus on next? Well, we, 
the only information we have to go on, is what the decoders hidden state is 
now all right. So, let's grab that put it into the linear layer, put it 
through a  non-linearity, put it through one more nonlinear layer. This one 
actually doesn't have a bias in it, so it's actually just a matrix 
multiply, put that into a soft Max and that's it right, that's a little 
neural net. It doesn't do anything. We could it's just a neuron that no 
neuron it's do anything they're. Just linear layers with nonlinear 
activations with random weights right, but it starts to do something if we 
give it a job to do right, and in this case the job we give it to do is to 
say don't just take the final state.</p>

<p>But now, let's use all of the encoder 
states and let's take all of them and multiply them by the output of that 
little neuron it right and so, given that the things in this little neural 
net are learnable weights. Hopefully it's going to learn to wait. Those 
encoder outputs, those encoder hidden States by something useful, all 
right, that's all in neural net ever does is. Is we give it some random 
weights to start with and a job to, do and hope that it learns to do the 
job and the ants, and it turns out that it does all right. So everything 
else in here is identical to what it was before. We've got teacher forcing 
it's not bi-directional, so we can see how this goes right. You can see. 
Actually, I am oh yes here, yeah using bi-directional, that's are using 
teacher forcing so teacher foreseeing had three point: four, nine and so 
now we've got nearly exactly the same thing, but we've got this little 
minimal, neural net figuring out what weightings to give our inputs? Oh 
wow, now it's now down to three point: three. Seven, all right remember 
these things are logs right, so e ^. This is quite a significant change. So 
three point three: seven: let's try it out, not bad right. Where are they 
located? What are their skills? What'd? You do it's still not perfect. Why 
or why not, but it's quite a few of them are correct and again considering 
that we're asking it to learn about the very idea of language for two 
different languages and how to translate them between the two and grammar 
and vocabulary, and we only have 50,000 Sentences and a lot of the words 
only appear once I would say this is actually pretty amazing.</p>

<p>Yes Annette, 
why do we use tongue H? You still free Lu for attention minute. I don't 
quite remember: it's been a while, since I looked at it, you should totally 
try using value and see how it goes. Obviously, then, the key difference is 
that it kind of go in each direction and it's as limited both at the top 
and the bottom. I know very often like in it like, for the gates inside our 
own ends and Ellice Tian Xin Jie, I use fan often works out better, but 
it's been about a year since I actually doctored that specific question. So 
I look at it during the week, but the short answer is, you know you should 
try a different activation function and see if you can get a better result, 
be interested to hear what you find out. So what we can do also is. We can 
actually grab the attentions out of the model right, so I actually added 
this return. Attention equals true here, look see here in my forward like 
forward. You can put anything you like in forward, so I added a return. 
Attention parameter false by default, because obviously the the training 
loop, it doesn't know anything about it. But then I just had something here 
saying: if returned attention, then stick the attentions on as well right 
and the attentions is simply that value a just check it in a list. Okay, so 
we can now call the model with returned attention equals true and get back. 
The probabilities and the attentions betweens, as well as printing out 
these here we can draw pictures at each time step of the attention, and so 
you can see at the start.</p>

<p>The attentions on the first word. Second, word 
third word a couple of different words, and this is just for one particular 
sentence right, but so you can kind of see this is equivalent. This is like 
you know, when you're Chris, solar and and and and Sean kata, you make 
things that look like this when you Jeremy, Howard, the exact same 
information, looks like this, but it's the same thing. Okay, just pretend 
that it's beautiful bit, so you can see basically at each different time 
step. We've got a different, a different attention and it's really 
important when you try to build something like this. Like you, don't really 
know if it's not working right, because if it's not working - and you know 
as per usual, my first twelve attempts that this were broken and and they 
were broken in the sense that it wasn't really learning anything useful and 
so therefore was basically Giving equal attention to everything and 
therefore it wasn't worse, it just wasn't better whenever it wasn't much 
better, and so I'm sure you actually find ways to visualize the thing in a 
way that you know what it ought to look like ahead of time. You don't 
really know if it's working, so it's really important that you try to find 
ways to kind of check your intermediate steps in your outputs. Yes, you 
know, so we won't ask you: what is the last function for the attention on 
your network? No, no! No! Loss function to determine your network right, 
it's trained in to end, so it's just like it's just sitting here inside out 
decoder, look right, so the loss function for the decoder loop is that this 
result contains it's exactly the same as before.</p>

<p>Just the outputs, the 
probabilities of the words right so, like the loss function, it's it's the 
same. Loss function right, so so how come the? How come the little little 
mini? Neural nets? Learning something well because, in order to make the 
outputs better and better it, it would be great if it made the weights of 
these little weighted-average better and better right. So part of creating 
our output is to please do a good job of finding a good set of weights, and 
if it doesn't do a good job of finding good set of weights, then the loss 
function won't improve from that bit. So, like end-to-end learning means 
like you throw in, you know everything that you can into one loss function 
and the gradients of these of all the different parameters. Point in a 
direction that says basically hey. You know if you had put more weight over 
there, it would have been better and thanks to the magic of the train rule 
it then no, it's like oh well. It would have put more weight over there if 
you would like change the parameter in this matrix. Multiply a little bit 
over there all right, and so that's that's the magic of end-to-end 
learning. So it's a very understandable question of like how did this 
little mini in your network, but you've got to realize. There's nothing 
particularly about this code that says: hey this particular bits.</p>

<p>A 
separate little mini neural network anymore than the grooc is a separate 
little neural network, or this linear layers is a little function like it's 
all ends up pushed into one output, which is a bunch of probabilities which 
ends up in one lost function. That returns a single number that says this 
either was or wasn't a good translation right, and so thanks to the magic 
of the chain rule, we then back propagate little updates to all the 
parameters to make them a little bit better okay. So this is a a big, weird 
counterintuitive idea and it's totally ok if it's a bit mind-bending right 
and it's the bit where even back to lesson 1. You know it's like how did 
this? How did we make it find dogs versus cats? So we didn't. You know all 
we did was we said this is our data. This is our architecture. This is our 
loss function, please back propagate into the weights to make them better 
and after you've made them better, a while it'll start finding cats from 
dogs right. That's just in this case we haven't used somebody else's like 
convolutional network architecture. We have said here's like a custom 
architecture, which we hope is going to be particularly good at this 
problem right and even without this custom architecture, it was ok all 
right, but then, when we kind of made it in a way that made more sense 
short, we Think it ought to do it worked even better, but at no point did 
we kind of do anything different other than say: here's, a data, here's an 
architecture, here's a loss, function, go and find the parameters, please 
pay ins and it did it because that's what your Net to do okay, so that is 
sequence, the sequence planning and you know if you want to encode an image 
into you, know using CNN backbone of some kind and then pass that into a 
decoder which is like a our an N with attention, and you make Your 
y-values, the actual, correct caption speech of those images.</p>

<p>You will end 
up with an image caption generator if you do the same thing with videos and 
captions you'll end up with a video caption generator. If you do the same 
thing with 3d CT scans and radiology reports, you'll end up with a 
radiology report generator if you do the same thing with github issues, and 
you know people's chosen summaries of them you'll get a github issue, 
summary generator, which you know it Sector, sick, I agree, you know 
they're magical, but they they work. You know, and I don't feel like people 
have begun to scratch the surface of how to use sector SEC models in their 
own domains. So that's like not being a github person, it would never have 
occurred to me that, like oh, it would be kind of cool to start with some. 
You know issue and automatically create a summary, but now I'm like huh, of 
course. Next time I go to github, I want to see a summary written there for 
me. I don't want to write my own damn commit message through that. You know 
why should I write my own, like summary, of the code review when I finished 
adding comments, just lots of lines, it should do that for me as well. Now, 
I'm thinking like Oh github so behind it could be doing this stuff. So what 
are the things in your industry? You know that you could like start with a 
sequence in generate something from it. I can't begin to imagine right so 
again, it's kind of like it's a fairly new area. The tools for it are not 
easy to use.</p>

<p>They're not even built into fastai, yet, as you can see, 
hopefully there will be you know soon and, like you know, I don't think 
anybody knows what the opportunities are. Okay, so I've got good news, bad 
news, the bad news is, we have 20 minutes to cover a topic which in last 
year's course took a whole lesson. The good news is that when I went to 
rewrite this using fastai Empire torch, I ended up with almost no code, so 
all of the stuff that made it hard last year is basically gone now, so 
we're going to do something bringing together for the first Time our two 
little worlds, we focused on text and images and we're going to try and 
bring them together, and so this idea came up really in a paper by this 
extraordinary deep learning, practitioner and researcher named Andrea, 
Framm and Andrea was Google at the time and Her basic crazy idea was to 
say, like you know, words can have a distributed representation, a space 
which, particularly at that time was, you know, really was just word 
vectors right and images can be represented in a space I mean like in the 
end, if we have Like a fully connected layer, they kind of ended up as like 
a vector representation. Could we merge the two? Could we either? It could 
be somehow encourage the vector space that the images end up with be the 
same vector space that the words are in and if we could do that, what would 
that mean? What could we do with that right? So, though, what could we do 
with that? Covers things like? Well? What, if I'm wrong, you know what, if 
I'm predicting, that this image is a beagle, and I predict jumbo jacked, 
you know, and you nets model predicts Corgi. The normal loss function says 
that your net and Jeremy's models are equally good, ie they're both wrong 
right. But what if we could somehow actually say, though, you know what 
like Corky's closer </p>

<h3>13. <a href="https://youtu.be/bZmJvmxfH6I?t=1h57m30s">01:57:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Recurrent Entity Networks: an exciting area of research in Memory Networks</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>To beagle than it is to jumbo jets, so units models better than Jeremy's, 
and we should be able to do that right because in in in in word, vector 
space, vehicle and Corgi are pretty close together, but jumbo jet, not so 
much okay. So it would give us a nice situation where, hopefully, our 
inferences would be like wrong insane aways if they're wrong. It would also 
allow us to search for things that aren't in our you know an image net 
since set ID. You know like a category in image net like like dog and cat. 
Why did I have to train a whole new model to find dog vs. cats when we 
already have something that found Corky's and Paddy's right? Why can't, I 
just say, find me dogs? Well, if I had trained it in in word vector space, 
I totally could right because, like that there there's our word vector, I 
can find things with the right image, vector and so forth. So we'll look at 
some cool things we can do with it in a moment, but first of all, let's 
train a model where this model is not learning a category, a one hotted 
coded ID where </p>

<h3>14. <a href="https://youtu.be/bZmJvmxfH6I?t=1h58m45s">01:58:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Concept of “Attention” and “Attentional Models”</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Every category is equally far from every other category, let's instead 
trainer model where we're finding the dependent variable, which is a word 
vector, so what word vector? Well, obviously, the word vector for the word 
you want right. So if it's Corgi, let's train it to create a word vector, 
that's that's the Corgi work that don't, if it's a jumbo jet, let's train 
it with a dependent variable that says this is the word vector for a jumbo 
jet. Okay. So, as I said, it's now shockingly easy, okay, so let's grab the 
fart. The fasttech word vectors again load the men. We only need English 
this time right and so here's an example of the word vector for King, but 
it's just a 300 numbers. So, for example, you know little J, Jeremy and big 
J Jeremy have a correlation to point six. I don't like bananas at all. This 
is good banana and Jeremy point. One four right so like words that you 
would expect to be correlated are correlated in words. That should be as 
far away from each other as possible. Unfortunately, they're still slightly 
correlated, but not so much right. So let's now grab all of the imagenet 
classes because we actually want to know you know which one's Corgi and 
which ones jumbo jet. So we've got: we've got a list of all of those up on 
files, doc, bastard AI. We can grab them and that's also grab a list of all 
of the nouns in English, which I've made available here as well.</p>

<p>Okay, so 
here are the names of each of the thousand imagenet classes, and here are 
all of the nouns in English, according to work net, which is a popular 
thing for kind of representing what words are or not. So we can now go 
ahead and load that list of nouns and sorry load. The list of imagenet 
classes turn that into a dictionary. So these are the class IDs for the 
1000 images that are in the competition data set they're at a thousand 
okay. So here's an example and 0-100 Allah is attentively is a kind of 
fish. Let's do the same thing for all those word net nouns and you can see. 
Actually, it turns out that image net is using word net class names, so 
that makes it nice and easy to map between the two and word net. You know 
most basic thing is an entity and then that if it's an abstraction and a 
physical entity could be an object and so forth right. So these are our two 
worlds: we've got the image net thousand and we got the 80 mm which are in 
wordnet. So we want to map the two together and which is as simple as 
creating a couple of dictionaries to map them based on the on the scene set 
ID or the web net ID, and it turns out that 49 thousand four hundred and 
sixty nine, let's see Since set oh okay, so what I need to do now is grab 
the 80 mm nouns in word net and try and look them up in fast text.</p>

<p>Okay, 
and so I've managed to look up 49 thousand of them in fast text all right. 
So I've now got a dictionary that goes from sin set ID, which is what 
wordnet calls them to word vectors. Okay. So that's what this dictionary! 
Yes, sin sin set to work vector and I've also got the same thing 
specifically for the 1000 word net classes. So save them away. That's fine! 
Now I grab all of the image net which you can actually download from cable. 
Now, if you look at the cackle image net, localization competition that 
contains the entirety of the image net classifications as well, it's got a 
validation set of 28,650 items in it, and so i can basically just grab for 
every image in image net. I can grab using that sin set to work vector grab 
it's its word net. Sorry, it's a fast text word vector and I can now stick 
that into this image. Vectors array stack that all up into a single matrix 
and save that away, and so now what I've got is something for every image. 
Net image they've also got. The fast text word vector that it's associated 
with like just by looking up. You know the the sin set ID going to word net 
then going to first text and grabbing, though the the word vector, okay and 
so here's a cool trick. I can now create a model data object, which is 
specifically it's an image.</p>

<p>Classifier data object and I've got this in 
court from names, an array I'm not sure if we've used it before, but we can 
pass it a list of file names, and so these are all of the file names in an 
image net and we can just pass It an array of our dependent variables - and 
so this is all of the fast text, word vectors right and then I can pass in 
the validation indexes, which in this case is just all of the last IDs. I 
need to make sure that they're the same as image: net users, otherwise I'll 
be cheating, okay and then I pass in continuous equals, true, which means 
this puts a lie again to this image. Classifier data is now really an 
image. Regressive data so continuous equals true means, don't one-hot 
encode my outputs, but treat them just as continuous values. So now I've 
got a model data object that contains all of my file names and for every 
file name, a continuous array representing the word vector for that. So I 
have an X, I have a Y, so I have data now. I need an architecture and the 
last function that once I've got that I should be done. So let's create an 
architecture, and so we can roll revise this next week, but basically we 
can use the tricks we've learnt so far, but it's actually incredibly simple 
fast. Ai has a ComNet builder, which is what, when you say, you know, come 
flow, no dot pre-trained. It calls this and you basically say: okay, what 
architecture do you want, so we're going to use ResNet 50? How many classes 
do you want in this case it's not really classes, it's. How many outputs do 
you want, which is the length of the fast text? Word vector? That's 300. 
Obviously, it's not model class classification, it's not classification at 
all. Is it regression? Yes, it is regression okay and then you can just say 
all right.</p>

<p>What fully connected layers do you want? So I'm just going to 
add one fully connected layer, hidden layer of length, a thousand and 
twenty four, why a thousand and twenty four well I've got the the last 
layer of resin 850. Is I think, it's a thousand twenty four long? The final 
output I need is three hundred long. I obviously need my penultimate layer 
to be longer than three hundred. Otherwise it's not enough information, so 
I kind of just picked something a bit bigger. Maybe different numbers would 
be better, but this worked for me. How much dropout do you want? I found 
that the default dropout. I was consistently under fitting, so I just 
decreased the dropout from 0.5 to 0.2, and so this is now a convolutional 
neural network. That does does not have any softmax or anything like that, 
because it's regression, it's just a linear layer at the end and yeah. 
That's basically it that that's that's my model, so I can create a common 
floater from that model. Give it an optimization function. So now all I 
need I've got data. I've got an architecture right, so the architecture, 
because I said I've got this many three hundred outputs. It knows that 
there are three hundred outputs, because that's the size of this array 
right so now. All I need is a loss function. Now the default loss function 
for regression is l1 loss, so the absolute differences - that's not bad 
right, but unfortunately, in really high dimensional spaces.</p>

<p>Anybody whose 
could have studied the bit of machine learning probably knows this in 
really high dimensional spaces. In this case it's three hundred dimensional 
and basically, everything is on the outside okay and when everything's on 
the outside distance. Is it's not meaningless, but it's it's a little bit 
awkward like things you know things. Can things tend to be close together 
or fire doesn't really mean much in these really high dimensional spaces, 
but everything's on the edge right? What does mean something, though, is 
that if one things on the edge over here and one things on the edge over 
here, you can form an angle between those vectors and the angle is 
meaningful right, and so that's why we use a cosine similarity when we're. 
Basically, looking for like how close or far apart are things in high 
dimensional spaces right and if you haven't seen cosine similarity before 
it's basically the same as you're fitting in distance, but it's normalized 
to be basically a unit norm. That's so it basically divided by the length. 
So we don't care about the length of the vector we only care about its 
angle. Okay, so there's a there's, a bunch of like stuff that you could 
easily learn in a couple of hours. But but if you haven't seen it before 
it's a bit mysterious for now just know that loss functions in high 
dimensional spaces, where you're trying to find similarity, you you care 
about angle and you don't care about distance.</p>

<p>Okay, if you didn't use this 
custom loss function, it would still work. I tried it. It's just a little 
bit less good. Okay, so we've got an architecture. We've got data. We've 
got a loss function. Therefore, we're done okay, so we can go ahead and fit 
now. I'm training on all of imagenet right, that's going to take a long 
time, so pre-compute equals true. As your friend okay, you remember, 
precompute equals true. That's that thing we learnt ages ago that caches 
the output with the final convolutional layer that and just trains the 
fully connected bit yeah and like even with pre compute, equals true. It 
takes like three minutes to train an epoch on all of imagenet, so I trained 
it for a while. I trained it for a while longer. So it's like an hour's 
worth of training right, but it's pretty cool that you know with fastai. We 
can train, you know a new custom head, basically on all of imagenet for 48 
bucks. You know in an arrow, so okay, and so at the end of all that we can 
now say all right grab the 1000 images all right and, let's predict, on our 
whole validation, set right and let's just take a look at a few pictures. 
Okay, so here's a look at you know a few pictures and because the 
validation set is ordered, you know they're, all all the stuff is the same 
type as in the same place. I don't know what this thing is and what we can 
now do is we can now use nearest neighbors search all right, so nearest 
neighbors search means. You know.</p>

<p>Here's one 300 dimensional vector here's 
a whole lot of other three-dimensional vectors which things is it closest 
to Wow, and normally that takes a very long time, because you have to look 
through every 300 dimensional vector caplets at distance and find out how 
far away it is. Okay, but there's an amazing, almost unknown library quote 
NMS Lib that that does that incredibly fast, like almost nobody's heard of 
it, some of you may have tried other nearest neighbors libraries. I 
guarantee this is faster than what you're using. I can tell you that, 
because it's been benched much like by people who do this stuff for a 
living, this is by far the fastest on every possible dimension right. So 
this is basically a super fast way. We basically look here. This is angular 
distance right, so we want to create an index on angular distance and we're 
going to do it on all of our imagenet word vectors right, adding a whole 
batch create the index, and now I can query a bunch of vectors all at once. 
Get there ten nearest neighbors users, Bodi threading, it's it's absolutely 
fantastic, this library, okay, you can install it from pip, it just works 
and it tells you how far away they are and their indexes right, and so we 
can now go through and print out. The top three, so it turns out that bird 
actually is a limpkin okay, so here are there. This is the top three for 
each one. Interestingly, this one doesn't say it's a limpkin and I looked 
it up. It's the fourth one.</p>

<p>I don't know much about birds but like 
everything else here is brown with white spots. That's not so I don't know 
if that's actually a limb tune or if there's the mislabel, but I sure, as 
hell doesn't look like the other birds. So you know I thought that was 
pretty interesting, that yeah it's kind of saying like. I don't think it's 
that now. This is not a particularly hard thing to do, because it's only a 
thousand imagenet classes. It is not doing anything new right, but what? If 
we now bring in the entirety of wordnet - and we now say which of those 
forty-five thousand things as a closest to exactly the same right, so it's 
now searching all of wordnet right. So now like. Let's do something a bit 
different, which is take all of our predictions right so basically take our 
whole validation, set of images and create a KNN index of the image 
representations, because remember it's predicting things that are meant to 
be word vectors and now, let's grab the Fast text, vector for boat and 
boat, is not an image net concept right and yet I can now find all of the 
images in my predicted word vectors in my validation set that are closest 
to the word boat and it works. Even though, though, it's not something that 
was ever trained on what if we now take engine's vector and boats vector 
and take their average and what, if we now look in our nearest neighbors 
for that these are boats with engines, I mean yes, this is this is 
Actually, a boat with an engine it just happens to have wings on as well 
right by the way sail is not an image net thing boat is not an image net 
thing. Here's the average of two things that are not image net things, and 
yet, with one exception, it's bound me to sailboats.</p>

<p>Well, okay, let's do 
something else: crazy, let's open up an image in the validation set. Here 
it is hey. I don't know what it is. Let's call predict array on that image 
to get you know, it's kind of like word victor, like thing and let's do a 
nearest neighbors search on all the other images and here's all the other 
images of whatever. That is that, so you can see this is like crazy. We've 
trained a thing on all of imagenet in an hour using like a custom head that 
required basically like two lines of code and these things like run in like 
300 milliseconds to do these searches like I. I actually taught this basic 
idea last year as well, but it was in chaos and it was just like pages and 
pages and pages of code and everything took a long time and it's 
complicated and back then I kind of said yeah. I can't begin to think all 
the stuff you could do with this, as I don't think anybody's really thought 
deeply about this yet, but I think it's fascinating and so go back and read 
the devise paper because, like Andrea, had a whole bunch of other thoughts 
and Now that it's so easy to do, hopefully people will will dig into this 
now, because I think it's crazy and amazing all right thanks. Everybody see 
you next week, [ Applause, ] </p>






  </body>
</html>
