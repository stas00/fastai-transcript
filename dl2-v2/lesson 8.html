<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 8: Object Detection</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://course.fast.ai/">fast.ai: Deep Learning Part 2 (v2) (2018)</a></p>
  <h1><a href="http://course.fast.ai/lessons/lesson8.html">Lesson 8: Object Detection</a></h1>
  <h2>Outline</h2>
<p>Welcome to Cutting Edge Deep Learning for Coders, part 2 of fast.ai‚Äôs deep learning course. This part covers lessons 8 to 14, and assumes you have already completed lessons 1 to 7 from part 1. If you‚Äôre already a deep learning practitioner, feel free to try starting with this part‚Äîif you find that you‚Äôre not familiar with the topics discussed, you can always return to part 1. In particular, we assume that you‚Äôre familiar with:</p>

<p>    The Pytorch and fastai libraries</p>

<p>    CNNs and RNNs</p>

<p>    Modern techniques for training and regularizing neural networks.</p>

<p>Lesson 8 starts with a quick recap of what we‚Äôve learned so far, and introduces the new focus of this part of the course: cutting edge research. We talk about how to read papers, and what you‚Äôll need to build your own deep learning box to run your experiments. Even if you‚Äôve never read an academic paper before, we‚Äôll show you how to do so in a way that you don‚Äôt get overwhelmed by the notation and writing style. Another difference in this part is that we‚Äôll be digging deeply into the source code of the fastai and Pytorch libraries: in this lesson we‚Äôll show you how to quickly navigate and build an understanding of the code. And we‚Äôll see how to use python‚Äôs debugger to deepen your understand of what‚Äôs going on, as well as to fix bugs.</p>

<p>The main topic of this lesson is object detection, which means getting a model to draw a box around every key object in an image, and label each one correctly. You may be surprised to discover that we can use transfer learning from an Imagenet classifier that was never even trained to do detection! There are two main tasks: find and localize the objects, and classify them; we‚Äôll use a single model to do both these at the same time. Such multi-task learning generally works better than creating different models for each task‚Äîwhich many people find rather counter-intuitive. To create this custom network whilst leveraging a pre-trained model, we‚Äôll use fastai‚Äôs flexible custom head architecture.</p>

  <h2>Video Timelines and Transcript</h2>


<h3>1. <a href="https://youtu.be/cRjPVN3oo4s?t=0">00:00:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Intro and review of Part 1</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Okay, well welcome to part two of deep learning for coders part. One was 
practical, deep learning for coders. That too, is not impractical, deep 
learning for coders, but it is a little different as well discuss. This is 
probably a really dumb idea that last year I started like not starting 
part, two with part two lesson, one but part two lesson: 8, because it's 
kind of out of the same sequence. So I've done that again, but sometimes 
I'll probably forget and call things lesson, one so clap to lesson one in 
part, two less than eight or the same thing. If you ever make that mistake, 
so we're going to be talking about object. Detection today, which refers to 
not just blending out what a picture is a picture of, but also where abouts 
the thing is, but in general, the idea of each lesson in this part is not 
so much because I particularly want you to care about, say, object, 
rotation, But rather because I'm trying to pick topics which allow me to 
teach you some foundational skills that you haven't got yet right. So, for 
example, object detection is going to be all about creating much richer 
convolutional network structures which have kind of a lot more interesting 
stuff. Going on and a lot more stuff going on in the first day, I library 
that we have to customize to get there so like at the end of these seven 
weeks. I can't possibly cover the hundreds of interesting things that 
people are doing with deep learning.</p>

<p>Right now, but the good news is that 
all of those hundreds of things are you'll see once you later read the 
papers like minor tweaks on a reasonably small number of concepts, and so 
we covered a bunch of those concepts in part one and we're going to Go a 
lot deeper into those concepts and build on them to get just in deeper 
concepts in part two. So in terms of what we covered in part, one there's a 
few key takeaways we'll go through each of these takeaways into it. One is 
the idea - and you might have seen recently Young Buck Owens - been 
promoting the idea that we don't call this deep learning but differentiable 
programming and the idea is that you've noticed all the stuff we did in 
part. One was really about setting up a differentiable function and a loss 
function that describes how good the parameters are and then pressing go 
and it kind of makes it work. You know - and so this is kind of I think 
it's quite a good way of thinking about it - differentiable programming, 
this idea that if you can configure a loss function that does but you know 
describes you have scores how good something is it doing your task and You 
have a reasonably flexible neural network architecture, you're kind of 
done. Okay, so that's one key way of thinking about this. This example here 
comes from playground, tensorflow dot org, which is a cool website where 
you can play interactively with creating your own little differentiable 
functions manually.</p>

<p>The second thing, then, we learnt is about transfer 
learning and it's basically that transfer learning is like the most 
important single thing to be able to do to use deep wound effectively. 
Nearly all courses nearly all papers, nearly everything in deep learning, 
education and research focuses on starting with random words, which is 
ridiculous because you almost never would want to or need to do, that. You 
would only want to or need to do that if nobody had ever trained and model 
on a vaguely similar set of data with an even remotely connected kind of 
problem to solve. As what you're doing now, you know which almost so this 
is, where kind of the faster library and the stuff we talk about in this 
class, is vastly different too any other library, or course, is that it's 
all focused on transfer learning, and it turns out that You do a lot of 
things quite differently, so the basic idea of transfer learning is here's 
a network that does thing a remove the last layer or so replace it with a 
few random layers at the end, fine-tune those layers to do things be taking 
advantage of The features at the original network word and then optionally, 
find you in the whole thing into end and you've now got something which 
probably uses orders of magnitude less data. And if you start with random 
weights, it's probably a lot more accurate and probably trained a lot 
faster. You know we didn't talk a hell of a lot about architecture, design 
in part one and that's because kind of architecture design is getting less 
and less interesting.</p>

<p>There's a pretty small range of architectures that 
generally works pretty well. Quite a lot of the time we've been focusing on 
using CNN's for generally fixed size, somehow ordered data era tins for 
sequences at some state fiddling around a tiny bit with activation 
functions. I soft maps if you've got a single, categorical outcome or 
sigmoid if you've got multiple outcomes and so forth. Some of the 
architecture, design we'll be doing in this part, gets kind of more 
interesting, particularly this first session about object detection. But 
you know, on the whole, I think we probably spend less time talking about 
architecture design than most courses or papers, because it's it's not. 
It's you know, it's generally, not too happy okay. So the third thing is we 
looked at was out of word overfitting, and so did. The general idea that I 
tried to explain is is the way I like to build. A model is to of all create 
something. That's definitely terribly over parameterised will massively 
overfit for sure trainer and make sure it does over the it factors. At that 
point, you know: okay, I've got a model that is capable of reflecting the 
training set, and then it's as simple as doing these things to then reduce 
that overfitting and if you can't start, if you don't start with 
something's overfitting, then you you're kind of Lost right, so you start 
with something else: overfitting and then to make it over fit less.</p>

<p>You can 
add more data, you can add more data augmentation, you can do things like 
more batch norm, layers or dense Nets, or you know various things that can 
handle those a few nice data. You can add regularization like weight, decay 
and drop out, and then finally, this is often the thing people do first, 
but this should be. The thing you do last is reduce the complexity of your 
architecture, have less layers or less activations. We talked quite a bit 
about betting's both for NLP and the general idea of any kind of 
categorical data as being something you can now model with neural nets. And 
it's been interesting to see how, since part one came out at which 
</p>

<h3>2. <a href="https://youtu.be/cRjPVN3oo4s?t=8m">00:08:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Moving to Python 3</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Point there were almost no examples of kind of papers or blogs or anything 
about using kind of tabular data at categorical data in deep learning. 
Suddenly, it's kind of taken off and it's it's kind of everywhere, so this 
is becoming a more and more popular approach. It's it's still little enough 
known that when I say to people like, oh, you know, we use neural nets for 
time series and tabular data analysis is often like wait really, but it's 
definitely not such a far-out idea, yeah and there's more and more 
resources available, including Casual competition, recent capital, 
competition, winning approaches using this technique; okay, so the part one 
you know, which kind of particularly had those five messages really was all 
about, introducing you to best practices in deep learning, and so it's like 
trying to show you techniques which were mature Enough that they definitely 
work reasonably reliably for practical, real world problems and that I had 
researched and tuned enough over quite a long period of time that I could 
kind of say: okay, here's, the sequence of steps and architectures and 
whatever that, if you use this you'll, Almost certainly get pretty good 
results and then had kind of put that into the first day. I bring into a 
way that you could do that pretty quickly and easily, so that's kind of 
what practical, deep learning pakodas was designed to do so.</p>

<p>This part two 
is cutting edge, deep learning for coders, and what that means is, I often 
don't know the exact best parameters, architecture, details and so forth to 
solve Euler problem. We don't necessarily know if it's going to solve a 
problem well enough to be practically useful. It almost certainly won't be 
integrated well enough in too fastai or any library that you can just press 
a few buttons. It'll start working, it's it's all about stuff which I'm not 
going to teach it unless I'm very confident. </p>

<h3>3. <a href="https://youtu.be/cRjPVN3oo4s?t=10m30s">00:10:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Moving to Tensorflow and TF Dev Summit videos</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>That you know there is now or will be soon, very practically useful 
technique. So, like I don't kind of take stuff which just appeared - and I 
don't know enough about it to kind of know like what's a trajectory gon na 
be so if I'm teaching it in this course and saying, like you know, this is, 
you know either works well In the research literature now and it's gon na 
be well worth learning about or we're pretty close to being there. It's got 
to take a lot of freaking, often and experimenting to get it to work on 
your particular problem, because we don't know you know the details. Well 
enough to know how to kind of make it work for every dataset or every 
example, so it's kind of exciting to be working at this point. It means 
that, rather than fastai and pytorch being obscure black boxes, which you 
just know these recipes for you're, going to learn the details of them. 
Well enough that you can customize them exactly the way you want that you 
can debug them, that you can read the source code or to see what's 
happening, and so, if you're, not pretty confident of you know 
object-oriented Python and stuff like that, then that's something you Don't 
want to focus on studying during this course, because we, we assume that 
I'm not going to spending time on that, but I will be trying to introduce 
you to some some tools that I think are particularly helpful.</p>

<p>Like the 
Python debugger like how to use your editor to kind of jump through the 
code stuff like that, and in fact in general, there'll, be a lot more 
detailed. Specific code, walkthroughs coding, technique, discussions and 
stuff like that, as well as more detailed walkthroughs of papers and stuff, 
and so anytime. We cover one of these things if you notice something where 
you're like you know, this is assuming some knowledge that I don't have. 
That's fine. You know it just means like that's something you could ask 
from the forum and say: hey you know, Jeremy kind of was talking about 
whatever static methods in Python. I don't really know what a static method 
is or like he was using it here, because only some resources like you know 
these are kind of things at all: they're, not rocket science. It's just 
just because you don't happen to have come across it yet doesn't mean it 
ha. It's just something you learn. I will mention that, as I cover these 
research level topics and develop these courses, I often refer to code that 
academics have put up. You know to go along with it papers or kind of 
example, code that somebody else is written on github. I nearly always find 
that there's some massive critical flaw so be careful of like taking code 
from you, know, online resources and just assuming that, if it doesn't work 
for you that you've made a mistake or something you know this kind of like 
research level code.</p>

<p>It's it's just good enough that they were able to run 
their particular experiments. You know every second Tuesday, so you should 
you know you should be ready to kind of do some debugging. So on that, 
that's, since I just wanted to remind you about something from our old 
course wiki that we sometimes talk about, which is like people often ask 
what should I do after the less and like had away? How do I know if I've 
got it right and we basically have this thing called how to use the 
provided notebooks and the idea is this part, don't open up the notebook, 
and I know I said this in part - one as well, but I'll say it again And go 
shift and a shift and a shift enter until a bag appears and then go to the 
forums and say it notebooks broken right. The idea of the notebook is to 
kind of be like a little crutch to help you get through step. The idea is 
that you start with like an empty notebook and think like okay, I now want 
to complete this process right and, and that might be initially require you 
op tabbing, to to the notebook and reading it figuring it out what it says, 
but whatever you Do don't copy and paste it your notebook type it out 
yourself right back, so try to make sure you can repeat the process and as 
you're typing it out and you'd be thinking like well. What am i typing? Why 
don't i? Okay? So if you can get to the point where you can, you know solve 
an object, detection problem yourself in a new empty notebook, even if it's 
using the exact same data set, we used in the course that's a great sign 
that you're getting it right and that'll. Take a while, but the idea is 
that by practicing you know the second time you try to do it.</p>

<p>The third 
time you try to do it. Your check, the notebook glass! Yes right and if 
there's anything in the notebook where you think, if you think I don't know 
what it's doing, I hope to teach you enough techniques in this course in 
this past that you'll know how to experiment to find out what it's doing 
right. So you shouldn't have to ask that, but you may well want to ask like 
why is it doing that you know that's the conceptual bit and that's 
something which you may need to go to the forums and say, like you know, 
before this tip jeremy had done This after this tip jeremy, had done that 
this is bit in the middle way. Who does this other thing? I don't quite 
know why you know so, then you can try and say like here my policies as to 
why like try and work through it as much as possible and that way you'll 
both be helping yourself and other people will help. You fill in the gaps 
right if you wish, and you have the financial resources now is a good time 
to build a deep learning box for yourself, when I say a good time, I don't 
mean a good time in the history of the pricing of GPUs. Gpus are currently 
by far the most expensive they've ever been. As I say this because of the 
cryptocurrency mining boom, I mean it's a good time in your study cycle. I 
mean the fact is, if you're paying somewhere between 60 cents and 90 cents 
an hour for doing your deep learning on a cloud provider, particularly if 
you're still on a k80 like an Amazon, p2 or google collab.</p>

<p>Actually I 
haven't come across it now. Let's you train on a kad for free, but those 
are very slow jeez. You know you can buy one. It's gon na be like three 
times faster for maybe six hundred seven hundred dollars. You need a box to 
put it in, of course, but you know the example in the bottom right here 
from the forum was something that somebody put together in last year's 
cost. So, like a year ago, they were able to put together a decent box 
referred over five hundred dollars. Generally speaking, your problem, I 
created a veneer forum thread where you can talk about. You know, options 
and parts and ask questions and so forth, afford it right now that gtx 
980ti is almost certainly what you want in terms of the best price 
performance MIPS if you can't afford it at ten. Seventy is fine. If you 
can't afford that, you should probably be looking for a secondhand 980 or a 
second Amazonia, something like that. If you can afford to spend more 
money, it's worth getting a second GPU, so you can do what I do, which is 
to have one GPU training and another GPU which I'm running an interactive 
Cupid in the clock session. Ram is very useful. Try and get 32 gig if you 
can Ram, is not terribly expensive. A lot of people find that their vendor 
or person to buy one of these business classes they on CPUs. That's a total 
waste of time. You can get one of them into our eye.</p>

<p>Five or I seven 
consumers to CPUs far far cheaper, but actually a lot of them are faster, 
often you're here, CPU speed doesn't matter, that's if you're doing 
computer vision. That's definitely not true. It's very common now with 
these, like 1080 TRS and so forth, to find that the speed of the data 
augmentation is actually this mo bit. That's happening on the CPU, so it's 
worth getting a decency to you, your again, your GPU. If it's running 
quickly, but the hard drives got fast enough to give it data, then that's a 
waste as well. So if you can afford an nvme drive, they're super super 
fast. You don't have to get a big one. You can just get a little one. You 
just copy your current set of data onto and have some big raid array that 
sits there for the rest of your data when you're not using it. There's a 
slightly arcane thing about PCI lanes, which is basically like they're kind 
of the size of the highway. That connects your GPU to your computer and a 
lot of people claim that you need to have a 16 lanes to feed your GPU. It 
actually turns out, based on some analysis, that I've seen recently that 
that's not true, you need you need eight lanes GPU. So again, so like 
hopefully help you save some money on your motherboard if you've never 
heard of PCI lanes before trust me by the end of putting together this bus, 
you'll be sick of hearing about, you can buy all the parts and put it 
together yourself.</p>

<p>It's not that hard can be a useful learning experience. 
It can also be kind of frustrating and annoying, so you can always go to 
like central computers and they'll put it together, for you there's lots of 
online and vendors. That will do the same thing now generally like make 
sure it turns on and runs properly generally, not much of a markup. So it's 
not a bad idea. We're going to be doing a lot of reading papers, basically, 
each week we'll be implementing a paper or a few papers, and if you haven't 
looked at papers before they look. Something like on the left. That thing 
on the left is an extract from the paper that implements atom. You may also 
have seen atom as a single excel formula on the spreadsheet that are in 
there the same thing. Okay, the difference is in academic papers. People 
love to use Greek letters, they also hate to refactor. So you'll often see 
like like a page long formula where </p>

<h3>4. <a href="https://youtu.be/cRjPVN3oo4s?t=22m10s">00:22:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Moving to PyTorch</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>When you actually look at it carefully, you'll realize like the same and a 
sub equation of his eight times. You know they didn't know if they didn't 
think to say about it like that T equal like this, that equation announced 
one. I don't know why. This is a thing, but I guess all this is to say like 
once: you've read and understood a paper. You then go back to it and you 
look at it. You just like wow. How did they make such a simple feeling so 
complicated, like Adam Wright, is like momentum and momentum on this 
momentum on the gradient and momentum on the squared already? That's it 
right and and speak long things and the other reason it's a big long thing 
is because they have things like this, where they have like theorems and 
calories and stuff, where they're kind of saying, like here's or our 
theoretical reasoning behind why this ought to Work or whatever, and for 
whatever reason, you know a lot of conferences and journals, don't like to 
accept papers that don't have a lot of this theoretical justification. 
Geoffrey Hinton's talked about this a bit. How particularly you know a 
decade or two ago, when no conferences would really accept any neural 
network papers. Then there was this like one, abstract, theoretical result 
that came out where suddenly they could show this.</p>

<p>You know I don't like 
practically unimportant, but theoretically interesting thing and then 
suddenly they could then start submitting things to journals because they 
have this like theoretical justification. So it's kind of yeah academic 
papers are a bit weird, but in the end it's the way that the research 
community communicates their their findings. And so we need to learn to 
read them. But something that could be a great thing to do is to take a 
paper put in the effort to understand it and then write a blog where you 
explain it in you know: code in normal, English and lots of people who do 
that. You know end up getting quite a following end up getting some pretty 
great job offers and so forth, because you know it's such a useful skill to 
show like okay, I can I can understand these papers, I can implement code, 
I can explain them in English. One thing I will mention is it's very hard 
to read or understand something which you can't localize, which means, if 
you don't know the names of the Greek letters like it sounds weird, but 
it's actually very difficult to understand. Remember take in a formula that 
appears again and again that's got like squiggle right. You need to know 
that that squiggle is called Delta or that screw or the Sigma or whatever. 
So, like just spending some time learning the names of the Greek letters is 
like sounds like a strange thing to do.</p>

<p>But suddenly you don't look at 
these things anymore and go like squiggle iovers people beat us other weird 
squiggle looks like a. Why thing right, they've! All got notes, okay, so 
now that we're kind of at the cutting edge stage, a lot of the stuff we'll 
be learning, this class is stuff that almost nobody else knows about. So 
that's a great opportunity for you to be kind of like it's the first person 
to create an understandable and generalizable code, library that implements 
it or the first person to write a blog post explains it in clear English 
for the first person to try applying it To this slightly different area, 
but it's obviously going to work just as well or so so when we say 
cutting-edge research, that doesn't mean you have to come up with like the 
next batch norm or the next atom or the next five liter convoluted 
convolution. It can mean, like okay, take this thing that was used for 
translation and apply it to this very similar other parallel and LP task, 
or take this thing that was tested on skin lesions and trusted on this data 
set of this other clarifications. That kind of stuff is super great 
learning, experience and incredibly useful, because the vast majority of 
the world that knows nothing about this whole field. It just looks like 
magic. You know you're, like a I've for the first time shown greater than 
90 percent accuracy at you know finding this kind of lesion in this kind of 
data . So you know when I say here, experiment in your area of expertise. 
You know one of the things we particularly look for in this class is to 
kind of bring in people who are pretty good at something else.</p>

<p>You know 
pretty good at meteorology or pretty good at denovo drug design or pretty 
good at goat, dairy farming or </p>

<h3>5. <a href="https://youtu.be/cRjPVN3oo4s?t=27m30s">00:27:30</a></h3>

<ul style="list-style-type: square;">

<li><b> From Part 1 ‚Äúbest practices‚Äù to Part 2 ‚Äúnew directions‚Äù</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Whatever you know examples people we had in the class, so probably the 
thing you can do the best would be to take that thing, you're already 
pretty good at and that on these new skills right because otherwise, if you 
try to go into some different domain, you're Gon na have to figure out how 
do I get data for that domain? Problems are solved in that domain. Where 
else often it'll seem pretty trivial to you to take this technique apply to 
this data set that you've already got sitting on your hard drive, but 
that's often going to be that super interesting thing. You know for the 
rest of the world to see like. Oh, that's interesting, you know when you 
apply it to meteorology data and you use this eridan or whatever, and 
allows you to forecast over larger areas or a longer time periods. So 
communicating what you're doing is super helpful. We've we've talked about 
that before, but I know something that a lot of people in the forum's ask 
people who have already written is that, when somebody's written, a blog, 
often people on the forum will be like. How did you get up the guts to do 
that or what did you? What up the process you got to before you decided to 
start publishing something or whatever, and the answer is always the same. 
It's always just you know. I was sure I wasn't good enough to do it. I felt 
terrified and intimidated of doing it, but I wrote it and posted it anyway, 
but you just like never a time.</p>

<p>I think any of us actually feel like we're 
not total frauds and imposters, but we know more about what we're doing 
then us of six months ago right and there's somebody else in the world who 
knows as much as you did six months ago. So if you act something now that 
would have helped you at six months ago, you're helping some people and 
honestly, if you wait another six months, then the you of 12 months ago, 
probably won't even understand that any more offices to advanced it know so 
Mike. It's great to communicate wherever you're up to in a way that you 
think could be helpful to the person you were before. You knew that thing, 
um and, of course, something that the forum's have been useful for is 
getting feedback about drafts. You know, and if you post a draft of 
something that you're thinking of releasing and the folks here, can point 
out things that they find clear or they think need some Corrections 
whatever. So the kind of overarching theme with part two I've described as 
generative models, but unfortunately, then Rachel asked me this afternoon 
exactly what I meant by generative models, and I realize I don't really 
know so. What I really mean is, in part one that the output of our neural 
networks was generally like. Like a number, you know our category, where 
else the outputs, a lot of the stuff in part, two are going to be like a 
whole lot of things.</p>

<p>You know like the top left and bottom right location 
of every object in an image along with what the object is or a complete 
picture, with the class of every single pixel in that picture or an 
enhanced super resolution. Version of the input image or the entire 
original input, paragraph translated into fish - or you know it's kind of 
like often it just requires some different ways of thinking about things 
and some kind of different, architectures and and so forth, and so that's 
kind of like. I guess the main theme of the kind of techniques we'll be 
looking at the vast majority, possibly </p>

<h3>6. <a href="https://youtu.be/cRjPVN3oo4s?t=31m40s">00:31:40</a></h3>

<ul style="list-style-type: square;">

<li><b> Time to build your own box</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>All of the data we'll be looking at will be either text or image data the 
it would be fairly trivial to do most of these things with audio as well. 
It's just not something. I've spent much time on myself, yet somebody asked 
on the forum about like welcoming doing more stuff with time-series and 
tabular data, and my answer was like I've already taught you everything I 
know about that, and I'm not sure there's much else to say, particularly if 
you Check out the machine learning course, which goes into a lot of that in 
a lot more detail, so I don't feel that there's more stuff to tell you, I 
think, that's a super important area, but I think we're done we're done 
with that, we'll be looking at Some larger data sets both in terms of the 
number of objects in the data search and the size of each of those objects. 
For those of you that are working with limited computational resources, 
please don't let that put you off feel free to replace it with something 
small at the simpler. In fact when I was designing this course, so did 
quite a lot of it in Australia. When I went to visit my mom and my mom 
decided to book a nice holiday house for us with fast Wi-Fi and we turned 
up to the holiday house with fast Wi-Fi, and indeed it did have Wi-Fi that 
was fast, but the Wi-Fi was not the internet.</p>

<p>So I caught up the agent - 
and I said, like I, found thee a DSL router and it's got an ADSL thing 
plugged in and I followed the cable down and the other end of the cable has 
nothing to plug into so she called the she called the People, you know 
renting the house ona and called me back the next day and she said actually 
the tambaran is quite quickly. I actually point leo: has known internet 
wait what, and so the good old Australian government had decided to replace 
ADSL in point Leo with a new National Broadband Network and therefore they 
had disconnected ADSL that had not yet connected and after brought that 
down. So we had fast Wi-Fi which we could use to Skype chat from one side 
of the house to the other, but I had no internet. Luckily, I did have a 
news surface book fifteen-inch, which has a gtx 1070 in it, and so I wrote 
a live German of this course entirely on my laptop, which means I had to 
practice with relatively small resources. I mean not tiny, like 16 gig ram 
and six gig GP here, so I can definitely you know III, definitely, and it 
was all in Windows. So I can tell you that most of this you know much one 
where this course works well on Windows on a laptop, so you can always use 
smaller batch sizes. You could use a cut-down version of the data set 
whatever, but if you have the resources, you'll get better results. If you 
can use the bigger data sets when they're available.</p>

<p>Okay, now it's a good 
time, I think, to take a somewhat early break, so we can fix the floor so 
the forum's chill down okay, so it was it okay, let's come back at 7:25. So 
let's start talking about object, detection and so here is an example. 
Object, detection and so hopefully, you'll see two main differences from 
what we're used to when it comes to classification. The first is that we 
have multiple things that were classifying, which is not unheard of. We did 
that in the planets satellite data, for example. But what is kind of 
unheard of is that, as well as saying what we see, we've also got: what's 
called bounding boxes around. What we see a bounding box has a very 
specific definition, which is it's a box all right. It's a rectangle and 
the rectangle has the object. </p>

<h3>7. <a href="https://youtu.be/cRjPVN3oo4s?t=36m20s">00:36:20</a></h3>

<ul style="list-style-type: square;">

<li><b> Time to start reading papers</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Entirely fitting within it, but it's no bigger than it has to be okay. 
You'll see this bounding box is perhaps for the horse at least slightly 
imperfect, in that this looks like there's a bit of tail here, so it 
probably should be a bit wider and maybe there's leaving a little bit of 
hoof here. Maybe there should be a bit longer so, like the bounding box, 
this won't be perfect, but they're generally pretty good in most data, so 
our job will be to take data that has been labeled in this way and on data 
that was unlabeled to generate their classes Of the objects and each one of 
those their bounding losses, one thing I'll note to start with is that 
labeling, this kind of data is generally more expensive, it's generally 
quicker to say horse person, person horse car dog jumbo jet than it is to 
say you know, If there's a whole like horse race, going on to label the 
exact location of every rider and a very horse, and then of course, it also 
depends like what classes do you want to label? You know if you want to 
label everything fence, post or whatever? So, generally always just like in 
like imagenet. It's not like tell me any object. You see in this picture. 
It's an image notice like here are the thousand classes that we asked you 
to look for, tell us which one of those thousand classes you find just tell 
me one thing for these object: detection, datasets! It's here's! A list of 
object classes that we want you to tell us about.</p>

<p>You know, find every 
single one of them at any time in the picture, along with where they are so 
in this case, why isn't there tree or jump labeled? That's because for this 
particular data set, they weren't one of the classes that the annotators 
were asked to find and therefore not part of this particular problem. Okay, 
so that's kind of the specification of the object detection problem, so let 
me describe stage 1 and stage. 1. Is actually going to be surprisingly 
straightforward and we're going to start at the top and work down we're 
going to start out by classifying the largest object in each image, so 
we're going to try and say the person actually, this one is wrong. Dog is 
not the largest object. Sofa is the largest object, all right, so here's an 
example of a misclassified one bird, correct person, correct okay, that'll 
be the first thing we try to do. That's not going to require anything new, 
so it'll just be a bit of a warm-up for us. The second thing will be to 
tell us the location of the largest object at each image again here. This 
is actually incorrect. It should have labeled the sofa, but you can see 
where it's coming from and then finally, we will try and do </p>

<h3>8. <a href="https://youtu.be/cRjPVN3oo4s?t=39m30s">00:39:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Time to start writing about your work in this course</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Both at the same time, which is to label what it is and where it is for the 
largest thing - and this is going to be obviously straightforward - 
actually so to be kind of good warm-up to get us going again. But what I'm 
going to do is I'm going to use it as an opportunity to show you some 
useful coding techniques and a couple of little faster. I Andy details 
before we then get on to multi-label classification and then object 
specification. So, let's start here, the logbook that we're using is Pascal 
notebook and it's all of the notebooks are in the DL too. One thing you'll 
see in some of my notebooks is torch, Dakota dot set device. You may have 
even seen it in the last part. Just in case you're wondering why that's 
there, I have four GPUs on the university server that I use, and so I can 
put a number from North to three in here to pick one. This is how I prefer 
to use multiple GPUs, rather than run a model on multiple GPUs, which 
doesn't always beat it up that much and it's kind of awkward. I generally 
like to have different GPUs running different things, so I, in this case I 
was running something in this on device one and doing something else, 
another booking device to. Obviously, if you see this in a notebook left 
behind, that was a mistake. If you don't have more than one GPU you're 
going to get an error done, so you just change it to zero or delete that 
line entire. So there's a number of standard object.</p>

<p>Detection data sets 
just like imagenet kind of a standard object. Classification data set and 
kind of the the old classic kind of image net equivalent, if you like, is 
Pascal vo, see </p>

<h3>9. <a href="https://youtu.be/cRjPVN3oo4s?t=41m30s">00:41:30</a></h3>

<ul style="list-style-type: square;">

<li><b> What we‚Äôll study in Part 2</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>




<h3>10. <a href="https://youtu.be/cRjPVN3oo4s?t=40m40s">00:40:40</a></h3>

<ul style="list-style-type: square;">

<li><b> Artistic style (or neural style) transfer</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Visual object class and it's something like that: yeah, the actual main 
website for it is like I don't know it's running on somebody's coffee, 
warmer or something it goes down all the time. Every time he makes coffee. 
I don't know so, some folks with mÈrida. It's very kind of thin, so you 
might find it easier to grab from the mirror you'll see when you downloaded 
that there's a 2007 dataset of 2012 data set that there basically were like 
academic competitions in those different years. Just like the Internet data 
set, we tend to use, is like actually the image net 2012 competition 
producer now we'll be using the 2007 version in this particular notebook 
feel free to use of 2012 and stared. It's a bit bigger. You might get 
better results. A lot of people, in fact, most people now in research 
papers, actually combine the two you do have to be careful because there's 
some leakage between the validation sets between the two. So if you do 
decide to do that, make sure you do some reading about the data set to make 
sure you know how to combine them correctly. The first thing you'll notice 
in terms of coding here is this. We haven't used this before I'm going to 
be using this all the time now. This is part of the Python 3 standard 
library called path Lib, and it's super handy. It's basically gives you an 
object-oriented access to directory or a file, so you can see if I go path, 
dot, something it there's lots of things I can do.</p>

<p>One of them is iterative 
directory. However, path iterate directory returns that hopefully you've 
come across generators by now, because we do quite a lot of stuff that use 
them behind the scenes without talking about them too much. But basically, 
a generator is something in in Python 3, which you can iterate over right. 
So basically, you go for oh in that grid, Oh for instance, right, ok or of 
course you could do the same thing as a list. Comprehension right or you 
can just stick the word list around it to turn a generator into the list. 
Ok, so anytime, you see me put mist around something, that's normally 
because it pretend a generator. It's not particularly interests, ding. The 
reason that things generally return generators is that, like what, if the 
directory had 10 million items in you, don't necessarily 1 to 10 million 
long list, so we were the for loop, just grep 1. Do the thing thrown over 
wait a second throw it away and lets you do things lazily you'll see that 
the things that's returning aren't actually strings, but they're some kind 
of object. If you're using Windows it'll be a Windows path of Linux would 
be a POSIX path. Most of the time you can use them as if they're strings so 
most like. If you pass it, you know any of the OS path dot. Whatever 
functions in Python, it'll just work at some external libraries.</p>

<p>It won't 
work. So that's fine! If you grab one of these - let's say, let's say, o 
equals: let's just grab one of these, so in general you can change data 
types in Python, just by naming the data type that you want and treating it 
like a function and that will cast it. I had so many time you try to use 
one of these paths, Lib objects and you pass it to something which says 
like I was expecting a stream. This is not a string. That's okay, so you'll 
see there's quite a lot of convenient things. You can do one kind of fun 
thing is the slash operator is not divided by, but its path, slash, so like 
they've overwritten, the slash operator in Python, so that it works, so you 
can say path, slash whatever and that gets you you'll see like see how 
That's not inside a stream right, so this is actually applying, not the 
division operator, but the overridden slash operator, which means get the 
child thing in that path. That makes sense and you'll see. If you run that 
it doesn't return a string, it returns, a pathway, object. Okay and so part 
one of the things the path the object can do is it has an open method 
right. So this it's it's kind of it's actually pretty cool once you start 
getting. The hang of it and you'll also find that, like the open method 
takes all the kind of arguments you're familiar with, it's a right for 
binary your encoding order.</p>

<p>So in this case, I want to load up these these 
JSON files, which contain not the images but the bounding boxes and the 
classes of the objects and so in Python. The easiest way to do that is with 
the JSON library or there's some faster API, equivalent versions. But this 
is pretty small, so you won't need them and you go json dot load and you 
pass it and open file object, and so the easy way to do that since we're 
using path live, is just go path open. So these JSON files that we're going 
to look inside in a moment if you haven't used it before JSON, is 
JavaScript object, notation it's kind of the most standard way to pass 
around hierarchical structure. Don't yet know, obviously not just with 
JavaScript you'll see. I've got some JSON files in here. They actually did 
not come from the mirror. I mentioned the the original pascal annotations 
were an xml format, but cool kids club uses email anymore. We have to use 
JSON, so somebody's converted them all to JSON and so you'll find. The 
second link here has all the JSON files. So if you just pop them in the 
same location that I've put them here, everything will so these annotation 
files jaison's basically contain a dictionary. Once you open up the JSON, 
it becomes a Python dictionary and they've got a few different things in 
the first is we can look at images, it's got a list of all of the images 
how big they are and the unique ID for each one.</p>

<p>One thing you'll notice 
here is taken the word images and put it inside a constant court images 
that may seem kind of weird, but if you're using you can a notebook or any 
kind of IDE or whatever this down means, I can tap complete all of my 
Strings and I won't accidentally type it slightly wrong, so it's just a 
handy trick: okay, so here's the contents first few things and the images 
more interestingly here are some of the annotations right. So you'll see 
basically an annotation contains a bounding box and the bounding box tells 
you the column and row if the top left and it's height and width, and that 
it tells you that that particular bounding box is for this particular 
image. So you'd have to join that up to over here to find it sexually, Oh 
to top jpg, okay and it's of category ID 7 um it also some of them at least 
has a polygon segmentation, not just a bounding box. We're not going to be 
using that some of them have an ignore flag, so we'll ignore the ignore 
flags, and some of them have something telling you it's a crowd of that 
object, not just one of them right. So that's that's what these annotations 
look like. So then you saw here there's a category ID so then the 
categories for examples they're, basically each ID. He has a name here we 
go okay. So what I did then was turned the his categories list into a 
dictionary from ID to name.</p>

<p>I created a dictionary from ID to name of the 
image file names and I created a list all of the image IDs just to make 
life easier. So you know generally like when you're working with a new data 
set, at least when I work with a new dataset. I try to make it look the way 
I would want it to if I designed that data set so kind of do a quick 
manipulation, and so, like the the steps you see here and you'll, see an 
h-class. Basically, like the sequence of steps, I talk as I started. 
Working with, is this bead onus it except like without the thousands of 
screw-ups that I did. I find like the the one thing people most comment on 
when they see me working in real time. Having seen my classes is like wow, 
you actually don't know what you're doing it's like 99. Some of the things 
I do don't work more percentage of the things that do work end up here so 
like this is like I mentioned that, because machine learning and 
particularly deep learning, is kind of incredibly frustrating, because you 
know in theory you just to find the Correct must function and flexible 
enough architecture and you press train and you don't all right, but if 
that was actually all a talk, then like nothing would take any time. The 
problem is that all the </p>

<h3>11. <a href="https://youtu.be/cRjPVN3oo4s?t=52m10s">00:52:10</a></h3>

<ul style="list-style-type: square;">

<li><b> Neural style notebook</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Steps along the way until what works it doesn't work. You know like it. It 
goes straight to infinity or crashes, with an incorrect answer size or 
whatever, and I will endeavour to show you some kind of debugging 
techniques as we go, but it's one of the hardest things to teach because, 
like I don't know, maybe I just have quite a Fewget it out yet, but it's 
like the main thing it requires is tenacity. I find like the biggest 
difference between the people. I've worked with who are super effective and 
the ones who don't seem to go very far has never been about intellect it's 
always been about. You know, sticking with it, basically never never giving 
up. So it's particularly important with this kind of deep learning stuff, 
because you don't get that continuous reward cycle like with normal 
programming. You've got like 12 things to do until you've got your Flash 
endpoints staged up. You know in at each stage. It's like. Okay, we have 
successfully processing the JSON, and now we successfully you know, I've 
got the callback from that promise and now I successfully created the 
authentication system. Like you know, it's this constant sequence of like 
stuff that works where else generally with training the model. It's a 
constant stream of life, it doesn't work, it doesn't work, it does.</p>

<p>Okay. 
So let's see a look at the images so you'll find inside the GOC dev kit, 
there's 20 toy 2007 and 2012 directories and in there there's a bunch of 
stuff. That's mainly these XML files, the one we care about the JPEG 
images, and so again here you've got path, tips, slash operator and inside 
there's a few examples of the images okay. So what I wanted to do was 
</p>

<h3>12. <a href="https://youtu.be/cRjPVN3oo4s?t=54m15s">00:54:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Mendeley Desktop, an app to track research papers</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>To create a dictionary where the key was the image ID and the value was a 
list of all of its annotations. So basically, what I wanted to do was go 
through each of the annotations that doesn't say to ignore it and append 
it. The bounding box and the class to the appropriate dictionary item where 
that dictionary item is a list, but the annoying thing is, of course, is 
that if that dictionary item doesn't exist yet then there's no list to the 
pen too, so one super handy trick in Python Is that there's a class called 
collections default depth, which is just like a dictionary, but if you try 
and access a key that doesn't exist, it magically makes itself exist and it 
sets itself equal to the return value this function now. This could be the 
name of some function that you've defined or it can be a lambda function. A 
lambda function simply means it's a function that you define in place, 
we'll be seeing lots of them. So here's an example of a function. All the 
arguments to the function are listed on the left, so there's no arguments 
to the function and lambda functions. A special you don't have to write 
return as there a return is assumed. So, in this case, this is a lambda 
function that takes no arguments and returns an empty list, so in other 
words, every time I try and access something in train annotations that 
doesn't exist now does exist it as an empty list, which means I can go into 
It okay, one comment on variable: naming is </p>

<h3>13. <a href="https://youtu.be/cRjPVN3oo4s?t=56m15s">00:56:15</a></h3>

<ul style="list-style-type: square;">

<li><b> <a href="http://arxiv-sanity.com/">arXiv-Sanity.com</a></b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>When I read through these notebooks I'll generally try and like speak out, 
the English words that the variable name is a limit for a reasonable 
question would be well. Why didn't I write the full name of the variable in 
English rather than using a short demonic. It's a personal preference. I 
have based on a number of programming communities where the basic kind of 
thesis is that the more that you can see in a single kind of I grab of the 
screen, the more you can like understand intuitively. That won't go every 
time. You have to your eye has to jump around it's kind of like a context, 
change that reduces your understanding, it's a style of programming. I 
found super helpful and so generally speaking, I try to. I particularly try 
to reduce the vertical height, so things don't scroll off the screen, but I 
also try to reduce the size of things so that there's a mnemonic there, 
which, if you know it's training annotations, it doesn't take long view to 
see the patient's. You know throughout the whole thing yet, so I'm not 
saying you have to do it this way. I'm just saying there's some very light 
programming communities, some of which have been around for 50 or 60 years 
which refused this approach, and it's interesting to compare like. I guess 
my philosophy is somewhere between math and Java. You know like in math, 
everything is a single character.</p>

<p>The same single character can be used in 
the same paper for five different things and depending on whether it's in 
italics or bold, faced with capitals, another fire in Java, you know 
variable names, sometimes require a few pages. Well. So for me, I 
personally like names, which are you know short enough to not take too much 
of my. You know perception to see it once, but long enough to have a 
mnemonic. Also. However, a lot of the time the variable will be describing 
a mathematical object as it exists in the paper and there isn't really an 
English name for it, and so, in those cases I will use the same like often 
single letter that the paper uses right and So if you see something called 
</p>

<h3>14. <a href="https://youtu.be/cRjPVN3oo4s?t=59m">00:59:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Jeremy on <a href="http://twitter.com/">twitter.com</a> and <a href="http://reddit.com/r/MachineLearning/">reddit.com/r/MachineLearning/</a></b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Delta or a or something, and it's like something inside an equation from a 
paper, I generally try to use the same thing just to explain that yeah and 
by no means do you have to do the same thing. I will say, however, if you 
contribute to first day I I'm not particularly fastidious about coding 
style or whatever, but if you write things more like the way I do than the 
wage are, but people do okay. So by the end of this, we now have a 
dictionary from file names to at Apple and so here's an example of looking 
up that dictionary and we get back a bounding box and a-plus you'll see 
when I create the bounding box. I've done a couple of things: the first 
years I've switched the X & amp Y, coordinates and the reason for this I 
think we mentioned this briefly in the last course. The kind of computer 
vision world when you say like. Oh, my screen is 640 by 480. That's width 
by height, or else the math world. When you say my array is 640 by 480. 
It's rows by colors, so you'll see that a lot of things like pil, pillow 
image library in Python tend to do things in this kind of width by height 
or columns by rows way. Numpy is the opposite way around. So I again, my 
view is: don't put up with it's kind of incredibly annoying inconsistency 
fix it right, so I've decided fastai is you know the lump I pytorch way is 
the right way, so I'm always rows by cons so you'll see here. I sketched my 
rows of columns.</p>

<p>I've also decided that we're going to do things by 
describing the top left, XY coordinate and the bottom right XY coordinate 
the bounding box rather than the XY and the eye width. Okay, so you'll see 
here, I was converting the the </p>

<h3>15. <a href="https://youtu.be/cRjPVN3oo4s?t=1h1m15s">01:01:15</a></h3>

<ul style="list-style-type: square;">

<li><b>  Neural style notebook (continued)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Height and width to the top, so you know again, it's kind of like I often 
find dealing with junior programmers and particular junior data scientists 
that they kind of get given datasets that are in shitty formats or happy 
api's, and they just act as if everything has To be that way, but your life 
would be much easier if you take a couple of moments to make things 
consistent, make them the way you want to be okay, so earlier on, I took 
all of our classes and created a categories list, and so, if we Look up 
category number 7, which is what this year's veteran on the 7 is car. Let's 
have a look at another example. Image number 17 has two bounding boxes. One 
of them is of type 15, one some type 13, that is a person and a horse. So 
this would be much easier to understand if we can see a picture of these 
things. So let's create some pictures so having just turned our height 
width stuff into top left bottom right stuff. We're now going to create a 
method to do the exact opposite, because anytime, I want to call some 
library that expects the opposite, I'm going to need to pass it in the 
opposite. So here is something that converts a bounding box to a hiking, 
with B bhw. The bounding box, okay, so it's again reversing the order and 
credit and giving us the height width. So we can now open an image in order 
to display it and where we going to get to is we're going to get it to show 
that sets that car? We just sorts it out right, so one thing that I often 
get asked on the forums or through github is like well.</p>

<p>How do I find out 
about this open image thing? Where did it come from? What does it mean who 
uses it? And so I wanted to just to take a moment, because what other 
things are going to be doing a lot? And although a lot of you aren't 
professional coders, you have backgrounds in statistics or you know, 
meteorology your physics or whatever, and I apologize for those of you that 
are professional coders. You know this already. You need because we're gon 
na be a lot doing about a stuck with the fastai library and other 
libraries. You need to go to navigate very quickly through them, okay, and 
so let me give you a quick overview of how to navigate through code and for 
those of you that haven't used an editor properly before this is going to 
blow your months right. </p>

<h3>16. <a href="https://youtu.be/cRjPVN3oo4s?t=1h4m5s">01:04:05</a></h3>

<ul style="list-style-type: square;">

<li><b> Broadcasting, APL as ‚ÄúA Programming Language‚Äù, and Jsoftware</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>For those of you that have you're going to be like, so for the demo, I'm 
going to show you in Visual Studio code personally, my view is that on 
pretty much every platform, unless you're prepared to put in the decades of 
your life, to learn beer more In apps, well, Visual Studio code is probably 
the best editor out there. It's free it's open source. There are other 
perfectly good ones as well. Okay, also, if you download a recent version 
of anaconda, it will offer to install Visual Studio code for you. It 
integrates with anaconda sets it up with your Python interpreter and comes 
with the Python extensions and everything. So it's it's a it's a good 
choice, if you're not sure if you've got some other editor, you, like you, 
know, search for the right keywords in the health. So if I fire up Visual 
Studio code, the first thing to do, of course, is do a git loan of the 
faster I library to your laptop you'll, find in the root of the repo, as 
well as the environment, yml file that sets up a condor environment. 52, 
you one of the students has been kind enough to create an environment, CPU, 
yml file, and perhaps one of you that knows how to do. This can add some 
notes to the wiki, but basically you can use that to create a local CPU, 
only fastai installation and the reason you might want to do. That is so 
that, as you navigate the code, you know you'll be able to navigate into 
pytorch you'll, see all the status is there anyway.</p>

<p>So I open up visual 
studio code and it's as simple as saying open, folder right and then you 
can just point it out the faster I get hub, folder that you just 
downloaded, and so the next thing you need to do is to set up visual studio 
Code to say, I want to use the fastai, Condor environment place. So the way 
you do, that is with the select interpreter, command and there's a really 
nice idea, which is kind of like the best of both worlds, between a 
command-line interface and a GUI which is you hit. This is the only command 
in each mode. Ctrl shift P, you hit ctrl shift P, and then you start typing 
what you want to do and watch what happens. Joseph P, I want to changed my 
interpreter in okay and it appears if you're, not sure you can kind of try 
a few different things right. So here we are Python, select interpreter and 
you can see generally, you can type stuff in it'll. Give you a list of 
things if it can and so here's a list of all of the environments 
interpreters. I have set up and here's my fastai environment, okay, so 
that's basically the only setup that you have to do. The only other thing 
you might want to do is to know there's an integrated terminal, and so, if 
you hit ctrl backtick, it brings up the terminal and you can the first time 
you do it it'll ask you: what terminal do you want if you're in Windows 
It'll be like PowerShell or command prompt or if you're on Linux, you've 
got more shells installed and asked so </p>

<h3>17. <a href="https://youtu.be/cRjPVN3oo4s?t=1h7m15s">01:07:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Broadcasting with Keras</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>As you can see, I've got it set up to use. Okay and you'll see it 
automatically goes to the directory. Alright, so the main thing we want to 
do right now, let's find out what I couldn't understand is so the only 
thing you need to know to do that is control teeth. If you hit ctrl T, you 
can now type the name of a class function. Pretty much anything and you 
find out about it so open image you can see it appears and it's kind of 
cool if there's something that's got like camelcase capitalized or 
something that underscore you can just type the first few letters of each 
pitch. So I could be like open image. For example, look I do that and it's 
found the function. It's also found some other things that match. Oh there, 
it is okay, so that's kind of a good way. You can see exactly where it's 
come from when you can find out exactly what it is and then the next thing 
I guess would be like. Well, what's it used for so? If it's used inside 
fastai, you could say find references which is shift. O smoke set up should 
say, shift open image shift f12 and it brings up something saying: oh, it's 
used twice in this codebase and I can go and I can have a look at each of 
those examples. Okay, if it's used in multiple different files, it'll tell 
you the different files that it's used in it. Another thing, that's really 
handy, then, is, as you look at the code, you'll find that certain bits of 
the code call other parts of the code.</p>

<p>So, for example, if you're inside 
files data set and you're like oh, this is calling something called open 
image. What is that? Well, you can wave your pointer over it and it'll give 
you the doc string or you can hit f12, and it jumps straight to its 
definition. Right. So like often it's easy to get a bit lost in, like 
things call things cool things and if you have to manually go to each bit, 
it's if you're ready for us this way. It's always one button way right, 
ctrl T to go to something that you're specific. You know the name of or f12 
to jump to the name, the definition of something that you're clicking on 
and when you're done, you probably want to go back where you came from so 
alt left takes you back to where you were okay, so whatever you use Bm 
Emacs Adam whatever they all have this functionality. As long as you have 
an appropriate extension installed, if you use pycharm, you can get that 
for free. That doesn't need any exchange. It's Python! You know whatever 
you're using. You want to know how to do this stuff. Finally, I mentioned 
there's a nice thing, called sin mode, ctrl, K, Z, which basically gets rid 
of everything else, so you can focus, but it does keep this nice little 
thing on the right hand, side, which kind of shows you whele, okay, so 
that's something that you Should practice if you haven't played around with 
it before during the week, because we're increasingly going to be, you 
know, digging deeper and deeper into faster iron, pipe or fibers? As I say, 
if you're already a professional coders know all this stuff - apologies for 
telling you stuff - you know okay, so we're going to well.</p>

<p>Actually, since 
we did that, let's just talk about open image, you'll see that we're using 
cv 2 cv 2 is the library is actually the opencv library. You might wonder 
why we're using open CV - and I want to explain some of the units of fastai 
to you, because some of them are kind of interesting and might be helpful 
to the torch vision like the standard kind of pytorch vision. Library 
actually uses apply torch tensors for all of its. You know: data 
augmentation and stuff, like that, a lot of people use pillow Pio a 
standard of Python, imaging library I found I did like a lot of testing of 
all. Of these I found open CV was about 5 to 10 times faster than to watch 
vision, so early on actually teamed up with one of the students from an 
earlier class to do the planetlab satellite competition back 
</p>

<h3>18. <a href="https://youtu.be/cRjPVN3oo4s?t=1h12m">01:12:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Recreate input with a VGG model</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>When that was on and we used to watch vision and because it was so slow, we 
could only get like 25 % GPU utilization, because we were doing a lot of 
data augmentation. And so then I use the profiler to find out what's going 
on and realized. It was all in in to watch, vision, pillow or PIL is quite 
a bit faster, but it's not as fast as open CV. It also is not nearly as 
threadsafe, so I actually talked to the guy who developed the the thing 
that python has. This thing called the global interpreter, lock this before 
the GI L, which basically means that true fred's can't do do pythonic 
things at the same time. Yes, but it makes python a really shitty language, 
actually the modern programming, but they're stuck with it. So I spoke to 
the guy on Twitter who actually made it so that open CV releases the GIM, 
so one of the reasons the faster your library is so amazingly fast is 
because we don't use multiple processors, like every other library does for 
our data organization. We actually do multiple threats and the reason we 
can do multiple threads is because we use it and see bit. Unfortunately, 
OpenCV is like a really shitty API, it's kind of inscrutable a lot of 
stuff. It does this point documented as they're poorly documented. It's 
documented but like in really obtuse kind of ways. So that's why I try to 
make it so like no one using fastai needs to know that it's using a CD you 
know like.</p>

<p>If you want to open an image, do you really need to know that 
you have to pass these flags to open to actually make it work? Do you 
actually need to know that if the reading fails, it doesn't show an 
exception? It just silently returns. Now you know it's these kinds of 
things that we try to do to actually make it work. Lastly right, but as you 
start to dig into it, you'll find yourself in these places and you're kind 
of want to know. You want to know what - and I mentioned this in particular 
to say: don't start using you know height or your data orientation, don't 
start bringing in pillow you'll find suddenly things slow down horribly or 
the body threatening won't work anymore or whatever. I try to stick to 
using OpenCV for your processing, okay, so so we've got our image, we're 
just going to use it to to demonstrate the pascal library, and so the next 
thing I wanted to show you in terms of like important coding, stuff we're 
going to Be using throughout this course is is using matplotlib a lot 
better, so matplotlib is so named because it was a rich, a clone of 
matlab's flooding. Later, unfortunately, MATLAB matlab's plotting library 
is awful, but at the time it was what everybody knew. So at some point the 
matplotlib folks realized - or they probably always view that the MATLAB 
plotting library is lawful, so they added a second API to it, which was an 
object-oriented API. Unfortunately, because nobody who originally learned 
that plot, let let the OO API, they then taught the next generation of 
people, the old MATLAB style, API and now there's basically no examples or 
tutorials online.</p>

<p>I'm aware of that use the much much better easier to 
understand simpler ago. So one of the things are going to try and show you, 
because plotting is so important in deep learning is how to use this API 
and I've discovered some simple little tricks. One simple little trick is 
plot. Subplots is just a super handy wrapper. I'm going to use it lots 
right and what it does is it returns two things. One of the things you 
probably won't care about. The other thing is an axes, object and basically 
anywhere where you used to say, PLT dot, something you now say: ax dot 
something, and it will now do that plotting to that particular sub bike. So 
a lot of the time you'll use this or I'll use this during this course to 
kind of plot, multiple plots that we can compare next to each other, but 
even in this case, I'm I'm creating a single plot, alright, but it's just 
it's just nice to Only know one thing rather than lots of things so, 
regardless of whether you doing one plot and lots of plots, I always start 
now with with this, that I was right, and the nice thing is that this way I 
can pass in an access object. If I want to plot it into a figure I've 
already created or if it hasn't been passed, you know I can create so this 
is also a nice way to make your matplotlib functions like really versatile 
and you're kind of see this used throughout this course.</p>

<p>So now, rather 
than plot that I am show it's a yesterday on show okay and then, rather 
than kind of weird stateful setting things in in the old-style API, you can 
now use ooohs. You know, get access that returns an object except visible, 
that's a property! It's all pretty normal, straightforward stuff. So once 
you start getting the hang of a small number of these oo, matplotlib things 
hopefully you'll find life a little easier. So I'm going to show you a few 
right now. Actually, so let me show you a cool example. What I think is a 
cool example, so one thing that kind of drives me crazy, with people 
putting text on images, whether it be subtitles on TV or people doing stuff 
with computer vision. Is that it's like white text on a black background or 
black text on a black background? You can't read it, and so a really simple 
thing that I like to do. Every time I draw on an image is to either make my 
text in boxes white with a little black border, or vice versa, and so 
here's a like cool little thing you can do in matplotlib is you can take a 
matplotlib plotting object and you can go Set path, effects and say add a 
black stroke around it, and you can see that then, when you draw that, like 
it doesn't matter that here, it's white on a white background right or here 
at some black background, it's equal and like it's just.</p>

<p>I know it's a 
simple little thing, but it kind of just makes life so much better when you 
can actually see your bounding boxes and actually read the text, so you can 
see, rather than just saying add a rectangle. I get the object that it 
creates and then pass that object to draw outline now everything I do that 
again, this nice path effect runner, you can see. Matplotlib is perfectly 
convenient way of drawing stuff alright. So when I want to draw a rectangle 
matplotlib calls that a patch and then you can pass it all different kinds 
of patches. So here's again, you know, rather than having to remember all 
that every time please take another function. Alright, now you can use that 
function. Every time you know you don't have to put it in a library 
somewhere. I always put lots of functions inside my notebook. If I use it 
in like three notebooks, then I know it's useful enough that I'll stick it 
in a separate library. You can draw text and notice. All of these take an 
axis object right, so this is always going to be added to whatever thing I 
want to add it to right, so I can add text and outline around it. So, 
having done all that, I can now take my show image, which and notice here 
the show image. If you didn't pass it an axis, it returns, the axis it 
created right so show image returns returns the axis that image is on. I 
then turn my bounding box into height width for this particular images, 
bounding box. I can then draw the rectangle.</p>

<p>I can then draw the text in 
the topple in the top left corner. So remember, the bounding box x and y 
are the first two coordinates right. So the column to the top left - this 
is the remember. The top all contains two things: the bounding box and then 
the class. So this is the class and then to get the text of it. I just pass 
it into my categories list and there we go. Okay, so now that I've kind of 
got all that set up, I can use that for all of my object, detection stuff 
from here all right. What I really want to do, though, is to kind of 
package all that up, so here it is packaging it all. It up so here's 
something that draws an image with some annotations right, so it shows the 
image that goes through each annotation turns it into height and width 
draws the rectangle Roza test. Okay, if you haven't seen this before each 
annotation, remember, contains a bounding box and a class so rather than 
going for o in a and n and going o 0 or 1, I can D structure it. Okay, this 
is a D structuring assignment. So if you put something on there, something 
on the left, then that's going to put the two parts of a top-off or a list 
into those two things to bandy. So for the bounding box and the class in 
the annotations go ahead and do that and so then I can then say: ok draw a 
image of particular index by grabbing the image ID opening it up and then 
calling that draw. And so let's test it out and there it is okay.</p>

<p>So you 
know that kind of seems like quite a few steps, but to me, when you're 
working with a new data set like getting to the point that you can rapidly 
explore it, it pays </p>

<h3>19. <a href="https://youtu.be/cRjPVN3oo4s?t=1h22m45s">01:22:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Optimize the loss function with a deterministic approach</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Off right, you'll see, as we start building our model, we're going to keep 
using these functions now to kind of see how things go alright, so step, 
one from our presentation is to do a classifier, okay, and so I think it's 
always good like for me. I didn't really have much experience before I 
started preparing this course a few months ago in doing this kind of 
object, detection stuff, so I was like alright, I want I want to get this 
feeling of, even though it's deep learning of continual progress all right 
so, Like what could I make work all right? Well, why don't? I find the 
biggest object in each image and classifier. I know how to do that all 
right, so it's like this is one of the biggest problems I find today with 
the younger students. If they figure out the whole big solution, they want 
generally, which involves a whole lot of new speculative ideas. I tried 
before, and they spend six months doing it and then the day before the 
presentation, none of it works and this roof right. Where else like I've 
talked about my approach to Kabul, competitions before I was just like half 
an hour, if you go at the end of that, half an hour, submit something right 
and try and make it a little bit better than yesterday's. So I kind of 
tried to do the same thing in preparing this lesson right, which is try to 
create something that's bit better than lasting okay.</p>

<p>So the first thing 
was like the easiest thing I could come up with was my largest item 
classifier. So the first thing I needed to do was to go through each of 
those each of the bounding boxes in an image and get the largest one right. 
So I actually didn't write that first. I actually wrote this first right. 
So normally I like pretend that somebody else has created the exact API. I 
want and then go back and write right. So I kind of I wrote this phone 
first and it's like okay. I need something which takes all of the bounding 
boxes for a particular image and finds the largest and well that's pretty 
straightforward. I can just sort the bounding boxes and here again we've 
got a lambda function. So again, if you haven't used lambda functions 
before this is something you should study during the week right, they're 
used all over the place to quickly define a function or like a once-off 
function, and in this case the pythons dead. The Python built-in sorted 
function lets you pass in a function to say how do you decide whether 
something's earlier or later, in the sort order? And so in this case I took 
the product of the last two items of my bounding box list. Ie the bottom 
right hand corner the two items of my bounding box: vest a the top left 
corner. So bottom right couple left is the size, the two sizes, and if you 
take the product of those two things you get the size of any boss and so 
then that's the function do that in descending order I mean often um often 
you can take something.</p>

<p>It's gon na be a few lines of code and turn it into 
one line of code, and sometimes you can take that too far. But for me I 
like to do that. You know where I reasonably can, because again it means 
like, rather than having to understand a whole big chain of things. My 
brain can just say like. I can just look at that at once and say: okay, 
there. It is, and also I find it over time. My brain kind of builds up this 
little library of idioms. You know and like more and more things, I can 
look at a single line and know what's going on okay, so this bill is a 
dictionary and it's a dictionary, because this is a dictionary 
comprehension. A dictionary comprehension is just like a list. 
Comprehension, I'm gon na use it a lot in this part of the course except it 
goes inside curly brackets and it's got a key colon value all right. So 
here the key is going to be the image ID and the value is the largest 
angles. Okay. So now that we've got that we can look at an example and 
here's an example of the largest bounding box for this image. Okay, so 
obviously there's a lot of objects here, there's three bicycles and three 
people: okay, but here's the largest bus - and I feel like this - ought to 
go without saying. But it definitely needs to be said because so many 
people don't do it. You need to look at every stage when you've got any 
kind of processing pipeline if, if you're as bad at coding, as I am 
everything you do, will be wrong the first time you do it right, but like 
there's lots of people that are as bad as Be according and yet lots of 
people write lines minds of code, assuming they're all correct and then at 
the very end, they've got a mistake and they don't know where it came from 
right. So particularly when you're working with images, write or text like 
things that humans can look at and understand, keep looking at it right so 
here I have it yep.</p>

<p>That looks like the biggest thing, and that certainly 
looks like this. So, let's move on here's another nice thing in path: Lib 
make directory okay method, so I'm going to create a path called CSB, which 
is a path to my large objects. Csv file. Why am I going to create a CSV 
file? Pure laziness? Right? We have an image classifier dot from CSV, but I 
could go through a whole lot of work to create a custom data set and blah 
blah blah to use this particular format. I have but why you know it's so 
easy to create the CSB check it inside a temporary folder and then use 
something they already have right. So this is kind of a something I've seen 
a lot of times on the forum is people will say like how do I convert this 
weird structure into a way that first day I can accept it, and then 
normally somebody on the forum will say like print It to a CSV file, so 
that's a good, simple tip and the easiest way to create a CSV file is to 
create a pandas data frame all right. So here's my pandas data frame - I 
can just give it a dictionary with the name of a column and the list of 
things in that column. So there's the file name, there's the category and 
then you'll see here. Why do I have this? I've already named the columns in 
the dictionary. Why is it here? Because the order of columns matters all 
right and a dictionary does not have an order? Okay, so this says the file 
name comes first in the category list, all right, so that's a good trick to 
creating a CSV. So now it's just dogs and cats.</p>

<p>Right I have a CSV file. It 
contains a bunch of file names and for each one it contains the plus of 
that object. So this is the same two lines of code 15,000 times. What we 
will do, though, is to like take a look at this. The one thing that's 
different is crop type, so you might remember, the default strategy for 
creating whatsis is here to 24, a to 24 by to 24 image in fastai is to 
first of all resize it so the largest side. Sorry, the smallest side is to 
24 and then to take a random crop, assuming it's rectangular, a random 
square run during training and then during validation. We take the center 
crop unless we use data augmentation, in which case we do a few ran across 
for bounding boxes. We don't want to do that because, unlike an image net, 
where the thing we care about is pretty much in the middle and it's pretty 
big a lot of the stuff in object. Detection is quite small and close to the 
edge, so we could crop it out, and that would be bad. So when you create 
your transforms, you can choose crop type, equals crop type, got no and no 
means don't crop and therefore to make it square. Instead, it squishes it 
so you'll see this guy now looks kind of a bit strangely wide right, and 
that's because he's been squished like this. Okay and generally speaking, a 
lot of computer vision models work a little bit better if you crop rather 
than squish, but they still work pretty well. If you squish right - and in 
this case we definitely don't want to crop.</p>

<p>So this is perfectly fine 
right, so we you know if you had like very long or very or images that you 
know such that if a human looked at the squashed version, if you like that, 
looks really weird, then that difficult, but in this case we're just Like 
so the computer whoa okay, so I'm going to kind of quite often just dig a 
little bit into some more depths of fastai and pipe torch. In this case, I 
want to just look at data loaders a little bit more, so you already know 
that, let's just make sure this is all run so </p>

<h3>20. <a href="https://youtu.be/cRjPVN3oo4s?t=1h33m25s">01:33:25</a></h3>

<ul style="list-style-type: square;">

<li><b> Visualize the iterations through a short video</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>You already know that inside a model data object when there's lots of model 
data subclasses like image, classifier data. We have a bunch of things 
which include training, data, loader and the training data set all right 
and we'll talk much more about this. So, but the main thing to know about 
training about a data loader is that it's an iterator that each time you 
grab the next iteration of stuff from it, you get a mini batch. Okay and 
the mini that you get is of whatever size you asked for, and by default, 
the batch size is 64. Okay, you can pass below um, however, so in Python, 
the way you grab the next thing from an iterator is with next right. You 
can't just do that right and why can't you just do that? The reason you 
can't do that is because you need to say, like start a new epoch now right 
in general, like this, isn't just in pi choice but for any Python, 
iterator, you're kind of need to say start at the beginning of the 
sequence. Please all right, and so the way you do that - and this is a 
general Python concept. Is you write it up and it says please grab an 
iterator out of this object right specifically, as we will learn later, it 
means this class has to have to find an underscore underscore header 
underscore underscore method, which returns some different object, which 
then has an underscore Underscore next underscore underscore yes right! So 
that's how I do that right, and so, if you want to grab just a single 
batch, this is how you do it X, comma y, equals next in a data load that Y 
X, comma Y, because our our lab data loader is our data.</p>

<p>Sets behind the 
daily loaders always have an X. You know the independent in the Y, the 
dependent variable, so here we can grab a mini batch of X's and Y's, and 
now I'm going to pass that to that show image command we had earlier, but 
we can't send that straight to show image. For example, here it is for one 
thing: it's not an umpire right, it's not on the CPU and its shape is all 
wrong. It's not to 24 by 2. 24. 3, it's 3 by 2. 3. 4. 30. 24. Furthermore, 
these are not numbers between 0 & amp. 1, why not, because remember all of 
the standard, imagenet pre-trained models expect our data to have been 
normalized to have a 0 mean and a 1 standard deviation. So if you look 
inside see, let's use Visual Studio code for this, that's what we've been 
doing. So, if you look inside transform the strong model, so ctrl T 
transforms from model TFM egg, alright, which in turn calls transforms. 12 
ashle transports model calls transport stats, and here you can see 
normalize and it normalizes with some set of image statistics and the set 
of image statistics they're, basically hard-coded. This is the image snap 
statistics: this is statistics, user insertion models right so there's a 
whole bunch of stuff. That's been done to </p>

<h3>21. <a href="https://youtu.be/cRjPVN3oo4s?t=1hm7m30s">01:37:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Recreate a style</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>The input to get it ready to be passed to a pre train model, so we have a 
function called denim for denormalize. It doesn't only do normalize. It 
also fixes up the dimension order and all that stuff right and they look. 
The denormalization depends on the transport. Okay and the data set knows 
what transform was used to create it. So that's why you have to go model 
data dot and then some data set dot d norm and that's a function that it's 
stored for you. That will undo that all right and then you can pass that a 
mini batch, but you have to turn it into non bio, first, okay. So this is 
like all the stuff that you need to be able to do to kind of grab, batches 
and unlock them, and so, after you've done all that, you can show the image 
and we've got that catalyst. So that's looking good. So, in the end, we've 
just got this to end of four lines of code. We've got our transforms: we've 
got our model data come learn about pre-trained we're using your resna 34 
here, I'm gon na add accuracy as a metric fix some optimization function to 
an LR find and that looks kind of weird, not particularly helpful. 
Normally, we would expect to see a uptick on the right. The reason we don't 
see it is because we intentionally remove the first few points in the last 
few points. The reason is that often the last few points shoots so high up 
towards infinity that you basically can't see anything so the vast majority 
of the time. Removing the last few points is a good idea.</p>

<p>However, when 
you've got very few mini batches. Sometimes it's not a good idea, and so a 
lot of people asked us on the forum. Here's how you fix it all right, just 
say: skip by default. It skips 10 at the start. So in this case we just say 
5 by default. It's gives 5 at the end, we'll just say 1, and so now we can 
see that the shape properly um. If your data sets really tiny, he made it 
he's a smaller batch size like if you only have like three or four batches 
worth this. One is not going to see that in this case, so it's it's fine. 
We just have to plot a little bit more okay, so we pick a learning rate. We 
say fit after 1 Apoc just training the last player. It's a descent. Let's 
unfreeze a couple of players: do another epoch, 2 % and freeze the whole 
thing not really improving. Why are we stuck at 80 % kind of makes sense 
right, like unlike imagenet or dogs versus cats, where each image has one 
major thing they were kicked because they have one major thing and the one 
major thing is what they're asked to look for a lot Of the Pascal data set 
has lots of little things, and so a largest classifier is not necessarily 
going to do great. But of course, we really need to be able to see the 
results to kind of see like whether it makes sense so we're going to write 
something that creates this, and in this case I'm kind of like I. After 
working with this a while, I know what the 20 Pascal classes are, so I know 
there's a person and a bicycle class.</p>

<p>I know there's a dog and I so for 
class. So I know this is wrong. It should be so forever. That's correct! 
Yes! Yes, chair, that's wrong. I think the tables bigger motorbikes 
correct, because there's no cactus there should be a bus person's correct. 
That's correct, Kasper if plants great cars correct. So that's looking 
pretty good all right so um when you see a piece of code like this, if 
you're not familiar with all the steps to get there, it can be a little 
overwhelming alright and I feel the same way when I see a few lines of Code 
in something I'm not familiar with, I feel like a 1 as well, but it turns 
out there's two ways to make. It super super simple to understand the code 
or there's one high level, where the high level way is run. Each line of 
code step yeah, step printout the inputs print out the efforts most of the 
time that'll be enough. If there's a line of code where you don't 
understand how the outputs relate to the inputs go and have a look for the 
sauce. So now all you need to know is what are the two ways you can step 
through the lines of code, one at a time um. The way I use paths the most 
often is to take the contents of the loop copy it create a cell. Above it 
paste it out, dent it right, I equals naught and then put them all in 
separate cells and then run each one one at a time printing out the inputs. 
Now I mean - I know that's obvious, but the number of times I actually see 
people do that when they asked me for help is basically zero, because if 
they had done that they wouldn't be asking for help another method.</p>

<p>That's 
super handy and there's particular situations where a super super super 
handy is to use the Python debugger, who here is used a debugger before so 
after two thirds so for the other half of here this would be life-changing. 
Actually, a guy. I know this morning is actually a deep learning researcher 
wrote on Twitter and his his message on Twitter was how come nobody told me 
about the Python debugger before my life has changed and like this guy's, 
an expert but because, like nobody, teaches basic software engineering 
skills In academic courses, you know nobody thought to say to him: hey 
Mark, you know what there's something that shows you everything your code 
does one stair at a time. So I replied on Twitter and I said good news mark 
not only that every single language in existence in every single operating 
system also has a </p>

<h3>22. <a href="https://youtu.be/cRjPVN3oo4s?t=1h44m5s">01:44:05</a></h3>

<ul style="list-style-type: square;">

<li><b> Transfer a style</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Debugger and if you google, for language named debugger, it will tell you 
Harry's right so there's a metal piece of information point in Python. The 
standard debugger is called PDB, ok and there's two main ways to use it. 
The first is to go into your code and the reason I'm mentioning this now is 
because during the next few weeks, if you're anything like me, 99 % at the 
time you'll be in a situation where your codes not working right and very 
often it all have Been on the fourteenth mini-batch inside the forward 
method of your custom, module that it's like. What do you do right and the 
answer? Is you go inside your module and you wrap that right and if you 
know it was only happening on the 14th iteration, you type. If I equals 13 
right, so you can set a conditional breakpoint, that's put a breakpoint 
PDB. Is the Python debugger fastai imports it for you? If you get the 
message that PDB spots there, then you can just say import PD Lee, ok! So, 
let's try that and justly it's not the most user-friendly experience it 
just pops up a boss right, but the first cool thing to notice is holders. 
Should the debugger even works in a notebook all right, so that's pretty 
nifty. You can also work in the terminal plus, and so what can you do? You 
can type a trip right and there are plenty of tutorials here, and the main 
thing to know is this is one of these situations where you definitely want 
to know the one letter mnemonics right, so you could type next, but you 
definitely want to talk right.</p>

<p>You could type continue you're. Definitely 
less. I've listed the main ones you need. So what I can do now that I'm 
sitting here is like it shows me the line, I'm Kara it's about to run. 
Okay, so one thing I might want to do is to print out something and I can 
write any Python expression and hit them up and find it okay. So that's 
that's a useful thing to do a might want to find out like more about like 
well. Where am I in the code? More generally, I just want to see this line, 
but what's the before it and after it, okay, so I want a whole forest 
right, and so you can see I'm about to run that line. These are the lines 
above it in the blower, okay um, so I might be like okay, let's run this 
line and see what happens so go to. The next line is ten okay and you can 
see now it's about to run the next one. One handy tip you don't even have 
to type n if you just hit enter it repeats. The last thing you did so 
that's okay, so I now should have a thing called beep right. Unfortunately, 
single letters are often used for debugger commands. So if I just type B, 
it'll run the big man rather than print B. For me, that's so to force it to 
print use, P print, okay, so there's bird all right! Fine! Let's do next 
again right at this point. If I hit next it'll draw the text, but I don't 
want to just draw the text, I want to know how it's going to draw the text, 
so I don't put no next over it. I want to ask step into it.</p>

<p>So if I now hit 
s to step into it, I'm now inside draw test, and I now hit n. I concede 
your text and so forth. Okay and then I'm like okay. I know everything I 
want to know about this. I will continue until I hit the next breakpoint, 
so C will continue. What if I was zipping along this happens quite often 
that, like let's step into Dean on here, I am inside Dean on and what will 
often happen is if you're debugging, something in your pytorch module and 
it's hidden exception and you're. Trying to debug you'll find yourself like 
six layers deep inside pytorch, but you want to actually see backup what's 
happening when you called it from right. So in this case I'm inside this 
property, but I actually want to know what was going on up. The call stack 
I just hit you and that doesn't actually run in a thing. It just changes 
the context of the debugger to show me what called it, and now I can type. 
You know things to find out about that: environment, okay and then, if I'm 
gon na go down again, it's deep okay, so, like I'm, not gon na show you 
everything about the debugger, but I just showed you all of those commands 
right. Yes, there's a Oh something that we found helpful as we've been 
doing. This is using from ipython court a debugger imports a trace, and 
then you get a all prettily colored. It's usually excellent tip. Let's 
learn about some of our students here. Is it tell us? I know you were doing 
an interesting project. Can you tell us about it? Okay, hello.</p>

<p>Everyone I 
mean is a here with my uh, my collaborator Britt and we're using this kind 
of stuff to try to build a Google Translate for animal communication yeah. 
So that involves playing around a lot with like unsupervised, machine, 
neural translation and doing it on top of audio. Where do you get data for 
that from ah that's sort of the hard problem, so there you have to go and 
like we're talking to a number of researchers to try to collect and collate 
large data sets, but if we can't get it that way, we're thinking About 
building a living library of the audio of the species of Earth that 
involves going out and like collecting a hundred thousand hours of like 
gelada monkey vocalization, so all right, that's great here! Okay, so let's 
get rid of that set trace um the other place that the debugger comes in 
particularly handy is, as I say, if you've got an exception all right, 
particularly if it's deep inside pipes watch. So if I like, when I times 
100 here, obviously that's gon na in exception, I've got rid of the set 
trace. So if I run this now, okay something's wrong. Now, in this case, 
it's easy to see what's wrong right, but like often it's not so what do I 
do? Percent debug pops open the debugger at the point, the exception that 
okay, so now I can check like okay, creds Len crits, 64. 5 times 100. I've 
got a print that size and 100, oh, no one, okay and you can go down the 
list. Okay, so I do all of my development, both with the library end of the 
lessons in G but a notebook.</p>

<p>I do it all interactively and I use you know - 
percent debug. You know all the time, along with this idea of, like copying 
stuff out of a function of putting in a desert of cells, running it step by 
step. There are similar things you can do inside, for example, Visual 
Studio code, there's actually Jupiter extension, which lets you select any 
line of code inside Visual Studio code and it'll and say run in Jupiter, 
and it will run it in Jupiter and create a little window showing You, the 
output, there's neat little stuff like that. Personally, I think Jupiter 
notebook is better and perhaps by the time you watch this on the video you 
know the lab or me the main thing give it a lab selection in the next 
version of Jupiter notebook pretty similar Wow. I just broke it totally: 
okay! Well, we know exactly how to fix it, so we were worried about that. 
Another time - hey debug it this evening, okay, so to kind of do the next 
stage we want to create the bounding box. Okay and now creating the 
bounding box around the largest object may seem like something you haven't 
done before, but actually it's totally something you've done before. Okay, 
and the reason is something you've done before is we know that we can 
create a regression rather than a classification. Here all right, in other 
words a classification year on there, is just one that has a sigmoid or 
soft mapped out port and that we use across entropy or binary cross entropy 
loss function like that's. Basically, what makes it if we don't have the 
softmax it boys at the end, and we use means Guidera as a loss function. 
It's now our regression model right, and so we can now use it to predict a 
continuous number rather than the category.</p>

<p>We also know that we can have 
multiple outputs like in the planet, competition. We did a multiple object 
classification. What if we combine the two ideas and to a multiple column 
regression? So in this case, we've got four numbers top left out and why 
bottom-right X & amp Y yeah and we could create a neural net with four 
activations. We could have no softmax or sigmoid and use a mean squared 
error loss function, and this is kind of like where you're thinking about 
it like differentiable programming, it's not like how do I create a 
bounding box model? It's like all right. What do I need? I need four 
numbers. Therefore, I need a neural network with four activations. Okay, 
that's traffic. What I need to know, the other half I need to know is a 
loss function, in other words, what's a function that, when it is lower, 
means that the four numbers are better, because if I can do those two 
things, I'm going okay. Well, if the X is close to the first activation and 
the wires close to the second so forth, then I'm done so that's it. I just 
need to create a model with four activations with a mean squared error loss 
function, and that should be it right like we don't need anything new. So, 
let's try it so again: we'll use a CSV right and if you remember from part 
one to do a multiple label classification, your multiple labels have to be 
spaced, separated, okay and then your file name is comma separated. So 
I'll. Take my largest item.</p>

<p>Dictionary create a bunch of bounding boxes for 
each one, separated by a space. No use. You know this comprehension I'll 
then create a data frame like I did before I'll turn that into a CSV and 
now I've got something. That's got the file name and the four bounding box 
corners. I will then pass that to from CSV again I will use crop type 
equals crop type dot. No real next week, we'll look at transform type dot, 
coordinate for now. Just realize that when we're doing scaling and data 
augmentation that needs to happen to the bounding boxes, not just images 
image, classifier data dot, CSV gets us to a situation where we can now 
grab one mini batch of data. We can do normalize it. We can turn the 
bounding box back into a height width so that we can show it, and here it 
is okay, remember we're not doing classification, so I don't know what kind 
of thing this is it's just a thing, but there is the thing okay. So I now 
to create a comic debt based on President 34, but I don't want to add the 
standard, a set of fully connected layers that create a classifier. I want 
to just add a single linear layer with four outputs. So fastai has this 
concept of a custom head. If you say my model has a custom head, the head 
being the thing that's added to the top of the model, then it's not going 
to create any of that fully connected Network for you, it's not going to 
add the adaptive average pooling for you, but instead It'll add whatever 
model you asked for so in this case I've created a tiny model.</p>

<p>It's a model 
that flattens out the previous layer. So remember, I'm normally would have 
a seven by seven by I think 512 previous layer in risen at 34. So it has 
flattens that out into a single vector of length, 2508 fat and then I just 
add a linear layer that goes from 2508, eight to four there's my four yeah. 
So, like that's the simplest possible kind of final layer, you could add. I 
stick that on top of my pre-trained risen at 34 model, so this is exactly 
the same as usual, except I've just got this custom here, all right, 
optimize it with atom user criteria, I'm actually not going to use MSC, I'm 
going to use l1 loss. So I can't remember recover this last week. We can 
revise it next week if we did it, but l1 loss means rather than adding up 
the squared errors, add up the absolute values of years. So it's like it's. 
It's normally. Actually what you want, adding up. The squared errors really 
penalize -- is bad misses by too much so l1 loss is generally better to 
work with okay I'll come back to this next week, but basically you can see 
what we do now is we do our ela find find our learning rate learn For a 
while freeze to learn a bit more freeze, three learn a bit more and you can 
see this validation loss, which remember, is the absolute value mean of 
absolute value with pixels were off by gets, lower and lower, and then, 
when we're done, we can print Out the bounding boxes, and lo and behold, 
it's done a damn good job.</p>

<p>Okay, so well revise this a bit more next week, 
but, like you can see this idea of like if I said to you before this class, 
do you know how to create a bounding box model? You might have said no 
nobody's taught me that all right, but the question actually is: can you 
create a model with for continuous outputs? Yes, can you create a loss 
function that is lower if those poor outputs are near to four other 
numbers? Yes, then you're done. Okay, now you'll see if I scroll a bit 
further down, it starts looking a bit crappy anytime. We've got more than 
one object, and that's not surprising right because, like how the hell do 
you decide which birds? So it's just said I'll just pick the middle, which 
cow I'll pick the middle. How much of this is actually potted plant right 
this one? It could probably improve, but you know it's got close to the 
car, but it's pretty weird right, but nonetheless you know for the ones 
that are reasonably clear. I would say it's done a pretty good job. Okay, 
all right! So that's time for this week I think J. You know it's been a 
kind of gentle introduction for the first lesson, if you're, a professional 
coder, there's, probably like not heaps of new stuff here for you, and so 
you know. In that case, I would suggest like practicing learning. You know 
about bounding boxes and stuff if you answer experienced with things like 
debuggers and that flat live api and stuff like that, there's gon na be a 
lot for you to practice because we're going to be really assuming you know 
well from next week.</p>

<p>Okay, thanks: everybody see you next Monday, [ 
Applause, ] </p>






  </body>
</html>
