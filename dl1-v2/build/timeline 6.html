<p><strong>Lesson 6</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=10s">00:00:10</a> Review of articles and works<br>
"Optimization for Deep Learning Highlights in 2017" by Sebastian Ruder,<br>
“Implementation of AdamW/SGDW paper in Fastai”,<br>
“Improving the way we work with learning rate”,<br>
“The Cyclical Learning Rate technique”</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=2m10s">00:02:10</a> Review of last week “Deep Dive into Collaborative Filtering” with MovieLens, analyzing our model, ‘movie bias’, ‘@property’, ‘self.models.model’, ‘learn.models’, ‘CollabFilterModel’, ‘get_layer_groups(self)’, ‘lesson5-movielens.ipynb’</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=12m10s">00:12:10</a> Jeremy: “I try to use Numpy for everything, except when I need to run it on GPU, or derivatives”,<br>
Question: “Bring the model from GPU to CPU into production ?”, move the model to CPU with ‘m.cpu()’, ‘load_model(m, p)’, back to GPU with ‘m.cuda()’, ‘zip()’ function in Python</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=16m10s">00:16:10</a> Sort the movies, John Travolta Scientology worst movie of all time “Battlefield Earth”, ‘key=itemgetter()jj’, ‘key=lambda’</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=18m30s">00:18:30</a> Embedding interpration, using ‘PCA’ from ‘sklearn.decomposition’ for Linear Algebra</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=24m15s">00:24:15</a> Looking at the “Rossmann Retail / Store” Kaggle competition with the ‘Entity Embeddings of Categorical Variables’ paper.</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=41m2s">00:41:02</a> “Rossmann” Data Cleaning / Feature Engineering, using a Test set properly, Create Features (check the Machine Learning “ML1” course for details), ‘apply_cats’ instead of ‘train_cats’, ‘pred_test = m.predict(True)’, result on Kaggle Public Leaderboard vs Private Leaderboard with a poor Validation Set. Example: Statoil/Iceberg challenge/competition.</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=47m10s">00:47:10</a> A mistake made by Rossmann 3rd winner, more on the Rossmann model.</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=53m20s">00:53:20</a> “How to write something that is different than Fastai library”</p>
</li>
<li>
<p>PAUSE</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=59m55s">00:59:55</a> More into SGD with ‘lesson6-sgd.ipynb’ notebook, a Linear Regression problem with continuous outputs. ‘a*x+b’ &amp;  mean squared error (MSE) loss function with ‘y_hat’</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=1h2m55s">01:02:55</a> Gradient Descent implemented in PyTorch, ‘loss.backward()’, ‘.grad.data.zero_()’ in ‘optim.sgd’ class</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=1h7m5s">01:07:05</a> Gradient Descent with Numpy</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=1h9m15s">01:09:15</a> RNNs with ‘lesson6-rnn.ipynb’ notebook with Nietzsche, Swiftkey post on smartphone keyboard powered by Neural Networks</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=1h12m5s">01:12:05</a> a Basic NN with single hidden layer (rectangle, arrow, circle, triangle), by Jeremy,<br>
Image CNN with single dense hidden layer.</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=1h23m25s">01:23:25</a> Three char model, question on ‘in1, in2, in3’ dimensions</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=1h36m5s">01:36:05</a> Test model with ‘get_next(inp)’,<br>
Let’s create our first RNN, why use the same weight matrices ?</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=1h48m45s">01:48:45</a> RNN with PyTorch, question: “What the hidden state represents ?”</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=1h57m55s">01:57:55</a> Multi-output model</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=2h5m55s">02:05:55</a> Question on ‘sequence length vs batch size’</p>
</li>
<li>
<p><a href="https://youtu.be/sHcLkfRrgoQ?t=2h9m15s">02:09:15</a> The Identity Matrix (init!), a paper from Geoffrey Hinton “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units”<br>
</p>
</li>
</ul>


