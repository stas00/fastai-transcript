<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 7: Resnets from scratch</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://course.fast.ai/">fast.ai: Deep Learning Part 1 (v2) (2018)</a></p>
  <h1><a href="http://course.fast.ai/lessons/lesson7.html">Lesson 7: Resnets from scratch</a></h1>
  <h2>Outline</h2>
<p>We finish off our recurrent neural network from scratch implementation from last week, and introduce the GRU and LSTM cells to allow training of long sequences with RNNs.</p>

<p>Then we complete this part of the course with a return to computer vision, where we implement the powerful resnet architecture and batch normalization layer from scratch. Congratulations on completing the course! Be sure to let us know on the forums about what projects you have built, and what you’re planning next…</p>

  <h2>Video Timelines and Transcript</h2>


<h3>1. <a href="https://youtu.be/H3g26EVADgY?t=3m4s">00:03:04</a></h3>

<ul style="list-style-type: square;">

<li><b>  Review of last week lesson on RNNs,</b></li>

<li><b>Part 1, what to expect in Part 2 (start date: 19/03/2018)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Okay, so last class of part one. I guess the theme of part one is 
classification and regression with deep wading and specifically it's about 
identifying and learning the best practices for classification and 
regression, and we started out with the kind of here are three lines of 
code to do: image, classification and gradually, we've Been for the first 
four lessons within kind of going through NLP, structured data, 
collaborative filtering and kind of understanding, some of the key pieces 
and, most importantly, understanding. You know how to actually make these 
things work well in practice, and then the last three lessons are then kind 
of going back over all of those topics, in kind of reverse order to 
understand more detail about what was going on and understanding what the 
code looks Like behind the scenes and wanting to kind of write them from 
scratch, part two of the course we'll move from a focus on classification 
and regression, which is kind of predicting a thing like a number or or at 
most a small number of things. Like a small number of labels and we'll 
focus more on generative, modeling, generative modeling means predicting 
kind of lots of things, for example, creating a sentence such as in Ural, 
translation or image, captioning or question-answering, while creating an 
image such as in style, transfer, super-resolution segmentation and So 
forth and then in part, two it'll move away from being just here are some 
best practices.</p>

<p>You know established best practices, either through people 
that are written papers or through research. That last day is done, and it 
kind of got convinced that these are best practices to some stuff, which 
would be a little bit more speculative. You know some stuff, which is maybe 
recent papers that haven't been fully tested yet and sometimes in part. 
Two, like pickles, will come out in the middle of the course and will 
change direction with the course and study that paper, because it's just 
you know interesting, and so, if you're interested in kind of learning a 
bit more about how to read a paper and how To implement it from scratch and 
so forth, then that's another good reason to do part two. It still doesn't 
assume any particular math background, but it does beyond kind of high 
school, but but it does assume that you're prepared to spend time, like you 
know, digging through the notation and understanding it and converting it 
to code and so forth. All right so we're we're up to is is our intent at 
the moment, and I think one of the issues I find most with teaching iron 
ends is trying to ensure that people understand they're, not in any way 
different or unusual or magical they're they're. Just a standard fully 
connected Network, and so let's go back to the standard, fully connected 
Network which looks like this right, so to remind you, the arrows represent 
one or more layer operations. Generally speaking, a linear, followed by a 
nonlinear function.</p>

<p>In this case, their matrix modifications, followed by 
real new raw or fan and the arrows of the same color represent the same, 
exactly the same weight matrix being used, and so one thing which was just 
slightly different from previous fully connected networks, we've seen, is 
that we Have an input coming in at the not just at the first layer, but 
also for the second layer and also at the third layer, and we tried a 
couple of approaches. One was concatenating the inputs and one was adding 
the airport's okay. But there was nothing at all conceptually different 
about this, so that code looked like this. We had a model where we 
basically defined the the three arrows colors. We had as three different 
weight: matrices, okay and by using the linear. We got actually both the 
weight matrix and the bias vector wrapped up for free for us, and then we 
went through and we did each of our embeddings put it through our first 
linear layer, and then we did each of our. We call them hiddens. Being the 
orange orange areas and in order to avoid the fact that there's no orange 
arrow coming into the first one, we decided to kind of invent an empty 
matrix and that way every one of these rows about the same right. And so 
then we did exactly the same thing except we used to loop just to refactor 
the cutter okay. So it's just. It was just a code refactoring.</p>

<p>There was no 
change of anything conceptually and since we were doing a refactoring, we 
took advantage of that to increase the number of characters to eight, 
because I was too lazy to type 8 when the alias, but I'm quite happy to 
change the loop in that stage. Yeah, so this now looked through this exact 
same thing, but we had eight of these rather than three. So then we 
refactored that again by taking advantage of an end errand in which 
basically puts that loop together for us and keeps track of the this.h as 
it goes along for us and so by. Using that we were able to replace the loop 
with a single call, and so again that's just a refactoring doing exactly 
the same thing. Okay, so then we looked at something which was mainly 
designed to save some training time, which was previously we had if we had 
a big piece of text right. So we've got like a movie review, but we were 
basically splitting it up into eight character, segments and we'd grab like 
segment number one and use that to predict the next character right. But in 
order to make sure that we kind of used all of the data we didn't just put 
it up like that, we actually said like okay, here's our whole thing, let's 
grab the first will be to grab this section. The second will be to grab 
that section in that section, then that section and each time would predict 
predicting the next one character, a lot okay, and so you know I was bit 
concerned that that seems pretty wasteful because, like as we calculate 
this section, nearly all Of it overlaps with the previous section, okay, so 
instead what we did was we said all right.</p>

<p>Well, what if we actually did 
split it into non-overlapping pieces right - and we said all right - let's 
grab this section here and use it to predict every one of the characters, 
one along right and then let's grab this section here and use it to predict 
every one Of the characters went along so after we look at the first 
character in we try to predict the second character and then now, if we 
look at the second character, we try to predict the third character and so 
okay, and so that's where you've got to and Then what if you perceptive 
folks, asked a really interesting question or expressed their concern, 
which was hey after we got through the first? The first point here after we 
got through the first point here, we kind of withdrew away our H, 
activations and started a new one which meant that when it was trying to 
use character, one to predict character, it's got nothing to go on. You 
know it hasn't built, it's only built, it's only done one linear layer, and 
so that seems like a problem which indeed it is okay. </p>

<h3>2. <a href="https://youtu.be/H3g26EVADgY?t=8m48s">00:08:48</a></h3>

<ul style="list-style-type: square;">

<li><b> Building the RNN model with ‘self.init_hidden(bs)’ and ‘self.h’, the “back prop through time (BPTT)” approach</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>So we're going to do the obvious thing, which is: let's not throw away H. 
Okay, so let's not throw away that that matrix at all so in code. The big 
problem is here, but every time we call forward so in other words, every 
time we do a new mini-batch we're creating our our hidden state right, 
which remember, is the orange circles. Okay, we're resetting it back to a 
bunch of zeros, and so, as we go to the next non-overlapping section, we're 
saying forget everything that's come before, but in fact the whole point is 
we know exactly where we are we're at the end of the previous section and 
About to start the new next contiguous section, so let's not throw it away 
so instead the idea would be to cut this out right, move it up to here: 
okay, store it away in self and then kind of keep updating it right now. So 
we're going to do that and there's going to be some minor details to get 
right. So, let's start by looking at the model, so here's the model, it's 
it's nearly identical and okay, here's the model, it's nearly identical, 
but I've got as expected. One more line in my constructor, where I call 
something called init hidden and as expected in it hidden sets self dot H 
to be a bunch of zeros. Okay, so that's entirely unsurprising and then, as 
you can see, our R and n now takes in self garage and it as before, spits 
out our new hidden, activations and so now the trick is to now store that 
away inside self dot, H and so here's wrinkle Number one, if you think 
about it, if I was to simply do it like like that right and now I train 
this on a document.</p>

<p>That's I don't know a million words million characters. 
Long then, the size of this unrolled are a 10 is has a million circles 
here, and so that's fine going forwards right there. When I finally get to 
the end - and I say, here's my character and actually remember we're doing 
multi output now so multi output looks like this right or if we were to 
draw the unrolled version of multi output, we would have a triangle coming 
off at every Point: okay, so the problem is that then, when we do back 
propagation, we're calculating, you know how much does the error at 
character? One impact the final answer: how much does the error character 
to impact the final answer and so forth, and so we need to go back through 
and say like how do we have to update our weights based on all of those you 
know errors, and so, if There are our million characters. My unrolled R and 
N is a million layers long. I have a 1 million layer fully connected 
Network. All right and like I didn't - have to write the million layers 
because I have for loop and the for loops hidden away behind that. You know 
the self dot, R and n, but it's still there right, we so so this is 
actually a 1 million layer fully connected Network, and so the problem with 
that is it's going to be very memory intensive, because in order to do the 
chain rule, I Have to be able to multiply at every step, like you know, 
after a few times she acts right and so, like I've got.</p>

<p>That means I have 
to remember that those values you the value of every set of layers, so I'm 
gon na have to remember all those million layers and I'm going to do - have 
to have to do a million multiplications and I'm going to have to do that. 
Every batch okay, so that would be bad so to avoid that we basically say 
all right well from time to time. I want you to forget your history, okay, 
so we can still remember the state right, which is to remember, like what's 
the values in our hidden matrix right, but we can remember the state 
without remembering everything about how we got there. So there's a little 
function called repackage variable, which literally is just this right. It 
just simply says: grab the tensor out of it right because remember, the 
tensor itself doesn't have any concept of history right and create a new 
variable out of that, and so this variables going to have the same value, 
but no no history of operations and therefore, When it tries to back 
propagate it all it'll stop there. So, basically, what we're going to do, 
then, is we're going to call this in our forward. So that means it's going 
to do add characters it's going to back propagate through eight layers. 
It's going to keep track of the actual values in our hidden state, but it's 
going to throw away at the end of those eight, it's its history of 
operations. So this is this approach.</p>

<p>It's called back prop through time, 
and you know when you read about it online people make it sound like like a 
different algorithm or some big insight or something, but it's it's not at 
all right. It's just saying: hey after our for loop, you know just throw 
away your your history operations and start afresh, so we're keeping our 
hidden state but we're not keeping our hidden States history. Okay, so 
that's that's! Wrinkle number one! That's what this repackage bar is doing, 
and so what do you see? Bp, BP TT, that's referring to that crop through 
time and you might remember. We saw that in our original errand in Lesson 
we had a variable called BP t t equals 70, and so when we set that they're 
actually saying how many layers backprop through another good reason not to 
back crop through too many layers is, if you have any Kind of gradient 
instability like gradient explosion or gradients, banishing you know too 
many more of the more layers you have, the harder, the network s to Train 
so smaller and less resilient on the other hand, and longer value for VP TT 
means that you're able to explicitly Capture a longer kind of memory, more 
state, okay, so that's a that's something that you get to tune when you 
create your area, all right, wrinkle number, two is: how are we going to 
put the data into this right like it's all, very well? The way I described 
it just now where we said you know we could do this, and we can first of 
all look at this section.</p>

<p>Then this section in this section, but we're 
going to do a mini batch at a time right. We want to do a bunch at a time, 
so, in other words, we want to say: let's do it like this, so mini-batch 
number one would say: let's look at this section and predict that section 
and at the same time, in parallel, let's look at this totally Different 
section and predict this and at the same time, in parallel, let's look at 
this totally different section and predict this right, and so then, because 
remember in our in our hidden state, we have a vector of hidden state for 
everything in our mini batch right. So it's going to keep track of at the 
end of this is going to be a you know, a vector here, a vector here, a 
vector here, and then we can move across to the next one and say okay, so 
this part of the mini batch use. This to predict that and use this to 
predict that and use this to predict that right, so you can see that we're 
moving, that we've got like a number of totally separate bits of our text 
that we're moving through in parallel right. So hopefully this is going to 
ring a few bells for you, because what happened was </p>

<h3>3. <a href="https://youtu.be/H3g26EVADgY?t=17m50s">00:17:50</a></h3>

<ul style="list-style-type: square;">

<li><b> Creating mini-batches, “split in 64 equal size chunks” not “split in chunks of size 64”, questions on data augmentation and choosing a BPTT size, PyTorch QRNN</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Was back when we started looking at torch texture, the first time we 
started talking about how it creates these mini batches? And I said what 
happened: was we took our whole big long document consisting of like you 
know the entire works of nature or all of the IMDB reviews, concatenated 
together or whatever, and a lot of a lot of you? Not surprisingly, because 
this really said this is really weird at first, a lot of you didn't quite 
hear what I said correctly. What I said was we split this into 64, equal 
sized chunks and a lot of your brains when Jeremy just said, we split this 
into chunks of size 64, but that's not what Theresa Jeremy said. We split 
it into 64: equal sized chunks right. So if this whole thing was length, 64 
million right, which would be a reasonable sized corpus, not an unusual 
size corpus, then each of our 64 chunks would have been of length 1 million 
right, and so then what we did was. We talked the first chunk of 1 million 
and we put it here and then we took the second chunk of 1 million and we 
put it here - the third chunk of 1 million. We put it here and so forth to 
create 64 chunks and then H. Mini-Batch consisted of us going - let's split 
this down here and here and here and each of these is of size, BP te T, 
which I think we had something like 70 right, and so what happened was we 
said. Alright, let's look at our first mini batch.</p>

<p>Is all of these right, 
so we do all of those at once and predict everything accrue off set by one 
and then at the end of that first mini batch. We went to the second chunk 
right and used each one of these to predict the next one. Offset by one ok, 
so that's that's why we did that slightly weird thing right is that we 
wanted to have a bunch of things we can look through in parallel, each of 
which, like hopefully, a far enough away from each other. You know that we 
don't have to worry about the fact that you know the truth. Is this 
starting? The start of this million characters was actually in the middle 
of a sentence, but you know who cares right because it's you know that only 
happens once every million characters honey. I was wondering if you could 
talk a little bit more about augmentation for this kind of data set and how 
to data augmentation of this kind of data said yeah. No, I can't because I 
don't. I really know a good way. It's one of the things I'm going to be 
studying between now and part two. There have been some recent 
developments, particularly something we talked about. The machine learning 
course - and I think, we've refinished in here, which was somebody for a 
recent careful competition, won it by doing data augmentation by randomly 
inserting parts of different rows basic, something like that may be useful 
here and I've seen it.</p>

<p>I've seen some papers that do something like that, 
but yeah I haven't seen any kind of recent ish state-of-the-art new NLP 
papers that that are doing this kind of data orientation. So it's something 
we're planning to work on, so it's generally how the issues be PTT. So 
there's a couple of things to think about when you pick your be PTT. The 
first is that you'll note that the the matrix size for a mini batch has a B 
PTT, the the TT by batch size. So one issue is your GPU Ram needs to be 
able to fit that by your embedding matrix racks. Every one of these is 
going to have B of length embedding length plus all of the hidden state. So 
one thing is to you know: if you get a cruder out of memory error, you need 
to reduce one of those if you're, finding your training is very unstable, 
like your loss is shooting off to LAN. Suddenly, then, you could try to 
decreasing your B PTT because you've got less layers to gradient explode 
through it's too slow. You could try decreasing your B PTT because it's 
going to kind of do one of those steps at a time like that for loop can't 
be paralyzed. Well, I say that there's a recent thing called QR, an N which 
is will hopefully talk about in part two which kind of does paralyze it, 
but the versions we're looking at, don't paralyze it. So there would be the 
main issues. I think before look at performance.</p>

<p>Look at memory and look at 
stability and try and find a number: that's you know as high as you can 
make it, but all of those things work for you, okay, so trying to get. 
Although that chunking and lining up and anything to work is more code than 
I want to write so for this section we're going to go back and use torched 
s together, okay, so when you're using AP is like fastai </p>

<h3>4. <a href="https://youtu.be/H3g26EVADgY?t=23m41s">00:23:41</a></h3>

<ul style="list-style-type: square;">

<li><b> Using the data formats for your API, changing your data format vs creating a new dataset class, ‘data.Field()’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>And torch text, which in these case these two API, is a desire to, or at 
least from the first day I site, designed to work together, you often have 
a choice which is like okay. This API has a number of methods that expect 
the data in this kind of format, and you can either change your data to fit 
that format or you can write your own data set subclass to handle the 
format that your data is already in. I've noticed on the forum. A lot of 
you are spending a lot of time. Writing your own dataset classes, whereas I 
am way lazier than you and I spend my time instead changing my data to fit 
the data set classes. I have like. I that's fine and if you realize, like 
oh there's, a kind of a format of data that me and other people are likely 
to be seen quite often, and it's not in the first day our library, then by 
all means, write the data set subclass it submitted As a PR and then 
everybody can benefit, you know, but </p>

<h3>5. <a href="https://youtu.be/H3g26EVADgY?t=24m45s">00:24:45</a></h3>

<ul style="list-style-type: square;">

<li><b> How to create Nietzsche training/validation data</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>In this case, I just kind of thought I want to have some Nietzsche data fed 
into torch text. I'm just going to put it in the format that watch text 
kind of already support, so torch text already has, or at least the first 
day I wrap her around Twitter text already has something where you can have 
a training path and a validation path, and you Know one or more text files 
in each path containing a bunch of stuff, that's concatenated together for 
your language model. So, in this case, all I did was I made a copy of my 
nature file copied it into training, made another copy, stuck it into the 
validation and then in one of the you know in the training set. I did I 
deleted the last twenty percent of rows and in the validation set I deleted 
all, except for the last one Center for us, and I was done right, so I 
found this that in this case I found that easier than writing a custom 
dataset class. The other benefit of doing it. That way was that I felt like 
it was more realistic to have a validation set that wasn't a random 
shuffled set of rows of text. That was like a totally separate part of the 
corpus because I feel like in practice, you're very often going to be 
saying. Like oh, I've got, I don't know these books or these authors I'm 
learning from, and then I want to apply it to these different books and 
these different authors, you know, so I felt like for getting a more 
realistic validation of my nietzsche model.</p>

<p>I should use like a whole 
separate piece of the text, so in this case it's the last. You know 20 % of 
the rows if the corpus, so I haven't, created this for you right 
intentionally, because you know this is the kind of stuff I want to do. 
Practicing is making sure that you're familiar enough comfortable enough 
with with batch or whatever, that you can create these and that you 
understand what they need to look like and so forth. So in this case you 
can see I've now got. You know a train and a validation here, and then I 
could yeah okay, so you can see. I've literally just got one file in it 
because it's a fire when you're doing a language model, ie predicting the 
next character or predicting the next word. You don't really need separate 
files. It's fine if you do have separate files, but they just get capped 
native together anyway. Alright, so that's my source data, and so here is 
you know the same lines of code that we've seen before and let's go over 
them again. So it's a couple of lessons ago right so in torch text we 
create this thing called a field and the field initially is just a 
description of how to go about pre-processing the test. Okay, now you in 
this case, I'm gon na, say: hey lowercase it. You know cuz, I don't mean 
now, I think about it. There's no particular reason to have done this lower 
case upper case would work fine too, and then how do I talk maser, and so 
you might remember last time we used a tokenization function, which kind of 
largely spit on white space and try to do some flavor Things with 
punctuation right and that gave us a word model in this case.</p>

<p>I want to 
character model, so I actually want every character put into a separate 
token. So I could just use the function list in Python because list in 
Python does that? Okay, so this is where you can kind of see like 
understanding how libraries like torch text and fast ar e are designed to 
be extended, can make your life a lot easier right. So when you realize 
that very often both of these libraries kind of expect you to pass a 
function, that does something and then you realize like. Oh, I can write 
any function. I like all right okay, so this is now going to mean that each 
mini batch is going to contain a list of characters, and so here's where we 
get to define all our different parameters and so to make it the same as 
previous sections of this notebook. I'm going to use the same batch size, 
the same number of characters then they're going to rename it to their PT 
t. Since we know what that means, the number of the size of the embedding 
and the size of our hidden state. Okay, remembering that size of our hidden 
state simply means going all the way back to the start, right and hidden 
simply means the size of the state that's created by each of those orange 
arrows. So it's the size of each of those circles, yeah, okay. So having 
done that, we can then create a little dictionary saying: what's our 
training, validation and test set in this case, I don't have a separate 
test set, so I just use the same thing and then I can say all right.</p>

<p>I want 
a language model data subclass with model data, I'm going to grab it from 
text files, and this is my path, and this is my field which I defined 
earlier, and these are my files - and these are my hyper parameters - min 
fracks not going to do Anything actually in this case, because there's not, 
I don't think, there's going to be any character that appears less than 
three times: that's probably redundant. Okay, so at the end of that it 
says, there's going to be 963 batches to go through, and so, if you think 
about it, that should be equal to the number of tokens divided by the batch 
size divided by B PTT because that's like the size of Each of those 
rectangles you'll find that in practice, it's not exactly that and the 
reason it's not exactly that is that the authors of torch text did 
something pretty smart, which I think we've briefly mentioned this before 
they said. Okay, we can't shuffle the data like with images we'd like to 
shuffle the order, so every time we see them in a different order, so 
there's a bit more random listed. We can't shuffle because we need to be 
contiguous, but what we could do is randomize. The length of you know 
basically, randomize be PTT a little bit each time, and so that's what 
pytorch? Does it's not always going to give us exactly 8 characters? Long 5 
% of the time it'll actually cut it enough, and then it's going to add on a 
small little standard deviation. You know to make it slightly bigger or 
smaller than for white okay.</p>

<p>So it's going to be slightly different to 
eight on average. Yes, just to make sure is it going to be constant per 
Vinny watch yeah yeah, exactly that's right, so a mini batch you know has 
to kind of it needs to do a matrix multiplication and the mini batch size 
has to remain constant because we've got this Age, weight matrix that has 
to you know, has to line up in size with the size of the mini batch yeah. 
But the number you know the sequence length can can change their problem. 
Okay, so that's why we have 963, that's so the length of a data loader is 
how many mini batches in this case. It's so do it approximate okay number 
of tokens is how many unique things are in the vocabulary and remember 
after we run this line. Text now does not just contain in a description of 
what we want, but it also contains an extra attribute called vocab right, 
which contains stuff like a list of all of the unique items in the 
vocabulary and a reverse mapping from each item to its number. Okay. So 
that text object is now an important thing to keep out all right. So let's 
now try this. So we do. We started out by looking at the class, so the 
class is exactly the same as the class we've had before. The only key 
difference is to call in at hidden which calls sets out. So H is not a 
variable anymore. It's now an attribute itself. That H is a variable 
containing a bunch of zeros.</p>

<p>Now I've been shown that that size remains 
constant H time, but unfortunately, when I said that I lied to you and the 
way that I lied to you is that the very last mini batch will be shorter. 
Okay, the very last mini batch is actually going to have less than 60 well, 
it might be exactly the right size if it so happens that this data set is 
exactly divisible by B PTT times patch size, but it probably isn't so. The 
last batch will probably has a little bit less okay, and so that's why I do 
a little check here. That says, let's check that the batch size inside 
self, dot, H, right and so self dot H is going to be the height. Sorry, the 
height is going to be the number of activations and the width is going to 
be the mini batch size. Okay, check that that's equal to the actual 
sequence length. Sorry, the actual batch size length that we've received 
okay and, if they're not the same, then set it back to 0's again, okay, so 
this is just a minor little wrinkle that basically, at the end of each 
epoch, it's going to do like a little mini mini Batch right, and so then, 
as soon as it starts the next epoch, it's going to see that they're not the 
same again and it'll reinitialize it to the correct full batch size. Okay, 
so that's why you know if you're wondering there's an inert hidden, not 
just in the constructor but also inside forward it's to handle this kind of 
end of each epoch, start of each epoch.</p>

<p>Difference: okay, not an important 
point by any means, but potentially confusing. When you see it, okay, so 
the last wrinkle, the last </p>

<h3>6. <a href="https://youtu.be/H3g26EVADgY?t=35m43s">00:35:43</a></h3>

<ul style="list-style-type: square;">

<li><b> Dealing with PyTorch not accepting a “Rank 3 Tensor”, only Rank 2 or 4, ‘F.log_softmax()’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Wrinkle is something which i think is something that slightly sucks about 
pytorch, and maybe somebody can be nice enough to try and fix it with a PR 
if anybody feels like it, which is that the loss functions such as softmax, 
I'm not happy receiving a rank. 3 tensor remember a rank. 3 tensor is just 
another way of saying dimension three right: okay, there's no particular 
reason they ought to not be happy receiving a rank. 3 tensor, you know, 
like somebody, could write some code to say, hey a wreck. Three tensor is, 
probably you know a sequence length by batch size by you know, results 
thing, and so you should just do it for each of the two initial axis, but 
no one's done that, and so it expects it to be a rank. Two tensor, funnily 
enough. It can handle write to or rec for, they're, not right through yeah, 
so we've got so. We've got a rank. Two tensor containing you know for each 
time period. I can't remember which way around the the y axes are, but 
whatever for each time period for each batch, we've got our predictions. 
Okay and then we've got our our actuals for each time period. For each 
batch, we've got our predictions and we've got our actuals. Okay, and so we 
just want to check whether they're the same and so at an ideal world, our 
lost function, a loss function, would check.</p>

<p>You know, item 1, 1, then item 
1, 2 and then item 1 3. But since that hasn't been written, we just have to 
flatten them both out okay, and we can literally just flatten them out, put 
rose, rose, and so that's why here I have to use dot view. Okay and so dot 
view says, the number of columns will be equal to the size of the vocab, 
because remember we're going to end up with a prediction. You know a 
probability for each letter and then the number of rows is. However, big is 
necessary which will be equal to batch size times B, PTT, okay, and then 
you may be wondering where I do that. That's so that's where the 
predictions you may be wondering where I do that for the target, and the 
answer is torch text knows that the target needs to look like that. So 
torch text has already done that for us. Okay, so torch text automatically 
changes the target to be flattened out, as you might actually remember. If 
you go back to lesson 4, when we actually looked at a mini batch, that's 
bad out of torch text. We did. We noticed actually that it was flattened - 
and I said, we'll learn about why later and so later is now all right. 
Okay, so they're the three wrinkles get rid of the history ooh. I guess for 
wrinkles recreate the hidden state if the batch size changes flatten out 
and then you just torch text to create mini batches that line up nicely. So 
once we do those things we can then create our model, create our optimizer 
with that models, parameters and fit it.</p>

<p>One thing to be careful of here is 
that softmax now, as of pytorch 0.3, requires that we pass in a number here 
saying which access do we want to do the softmax over? So at this point 
this is a three-dimensional tensor right, and so we want to do the softmax 
over the final axis right. So when i say which axis do we do the softmax 
over? Remember we divide by there we go e to the X. I divided by the sum of 
e to the X I so it's saying, which axis do we sum over so which access we 
want to sum to one, and so in this case clearly we want to do it over the 
last axis, because the last axis is The one that contains the probability 
per letter of the alphabet - and we want all of those probabilities to sum 
to one okay, so therefore, to run this notebook you're going to need 
pytorch 0.3, which just came out this week. Ok! So if you're doing this on 
the milk you're, fine, I'm sure you've got at least a 0.3 or later ok, 
where else the students here, if you just go Conda and update it, will 
automatically update you to 0.3. The really great news is that 0.3, 
although it does not yet officially support windows, it does in practice. I 
successfully installed 0.3 from Condor yesterday by typing. Condor install 
torch, pytorch in Windows are then attempted to use the entirety of lesson 
1 and every single part worked. So I actually ran it on this very laptop. 
So, for those who are interested in doing deep, burning on their laptop can 
definitely recommend the new surface book.</p>

<p>The new surface book 15-inch has 
a gtx, 1066 gig GPU in it, and i was getting and it was running about 3 
times slower than my 1080 TI, which i think means it's about the same speed 
as an AW sp2, the instance and, as you can see, It's also a nice 
convertible tablet that you can write on and it's thin and light, and so 
it's like I've never seen a such a good, deep learning boss. Also, I 
successfully installed Linux on eart and all of the faster I staff worked 
on Linux as well. So really good option if you're interested in a laptop 
that can run deep learning, stuff, . Alright. So that's that's going to be 
aware of with this team equals minus 1. So then we can go ahead and 
construct this and we can call fit and yeah we're basically going to get 
pretty similar results to what we got the ball. Alright. So then we can go 
a bit further without RNN by just kind of unpacking it a bit more, and so 
this is now again exactly the same thing gives exactly the same answers, 
but I have removed the cold air in it. So I've got rid of this self to iron 
in okay, and so this is just something I won't spend time on it, but you 
can check it out so instead, I've now to find iron in as R and n cell and 
I've copied and pasted the code. Above don't run it, this is just for your 
reference from pytorch.</p>

<p>This is this: is the color, the definition of 
eridan, so in pytorch, and I want you to see that you can now read pytorch 
source code and understand it not only that you'll recognize it as being 
something we've done before it's a matrix modification of The weights by 
the inputs plus biases, so f, dot, linear, simply does a matrix product, 
followed by an addition right and interestingly you'll see they do not 
concatenate the the input bit and the hidden bit. They sum them together, 
which is our first approach and I'm, as I said, you can do either neither 
one's right or wrong. But it's interesting to see this is the definition 
yeah </p>

<h3>7. <a href="https://youtu.be/H3g26EVADgY?t=44m5s">00:44:05</a></h3>

<ul style="list-style-type: square;">

<li><b> Question on ‘F.tanh()’, tanh activation function,</b></li>

<li><b>replacing the ‘RNNCell’ by ‘GRUCell’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Yes, you know can give some insight about what are they using that 
particular activation function: firm, yeah yeah. I think you might have 
briefly covered this last week, but very happy to do it again. If I did 
basically fan that's positive 1 and negative 1 then looks like that. So in 
other words, it's a sigmoid function, double the height minus one. 
Naturally they're they're equal. So it's it's! It's a nice function in that 
it's forcing it to be. You know no smaller than minus one, no bigger than 
plus one and since we're multiplying by this white matrix again and again 
and again and again, we might worry that our Lu, because it's unbounded 
might have more of a gradient explosion problem. That's basically the 
theory. Having said that, you can actually ask pytorch for an RNN cell, 
which uses a different non-linearity, so you can see by default it uses. 
Then you can ask for a value as well, but yeah most people seem to pretty 
much. Everybody still seems to use them as far as I can tell so. You can 
basically see here. This is all the same, except now I've got an errand in 
cell, which means now I need to put my for loop back alright, and you can 
see every time I call my my little linear function. I just obtained the 
result onto my list. Okay and at the end, the result is that all stacked up 
together, okay, so like just trying to show you how nothing inside 
pytorches is mysterious right, you should find you get. Basically the fact 
you I found.</p>

<p>I got exactly the same answer from this as the previous one. 
Okay, in practice, you would never write it like this, but what you may 
well find in practice is that somebody will come up with like a new kind of 
eridan cell or a different way of kind of keeping track of things over time 
or a different way Of doing regularization, and so inside posterize code, 
you will find that we we do this exactly this. Basically, we have this by 
hand, because we use some regularization approaches that are supported by 
pytorch, all right. So then another thing I'm not going to spend much time 
on, but I'll. Mention briefly, is that nobody really uses this hour an 
insult in practice and the reason we don't use that eridan so in practice 
is even though the fan is here, you do tend to find gradient. Explosions 
are still a problem, and so we have to use pretty low learning rates to get 
these to train and pretty small values for B PTT to get them to Train. So 
</p>

<h3>8. <a href="https://youtu.be/H3g26EVADgY?t=47m15s">00:47:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Intro to GRU cell (RNNCell has gradient explosion problem - i.e. you need to use low learning rate and small BPTT)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>What we do instead is we replace the eridan cell with something like this. 
This is called a GI: u cell and a GI? U so here it is, has a picture of it 
and there's the equations for it. So basically I'll show you both quickly 
but we'll talk about it much more in part, two we've got our input. Okay 
and our input normally goes straight in gets multiplied by a weight matrix 
to create our new activations. That's not what happens and then we've cost. 
We also, we add it to the existing activations. That's not what happens 
here in this case. Our input goes into this H, tilde temporary thing, and 
it doesn't just get added to our activations, their previous activations, 
that our previous activations get multiplied by this value. R and R stands 
for reset: it's a reset gate. Okay, and how do we calculate that this? This 
value goes between Norton one right in our reset gate? Well, the answer is: 
it's simply equal to a matrix product between some weight matrix and the 
concatenation of our previous hidden state and our new input. In other 
words, this is a little one, hidden layer, neural net and in particular 
it's a one, hidden layer, neural net, because we're then put it through the 
sigmoid function when you seasick, but one of the things I hate about 
mathematical notation is symbols are overloaded. A lot right, sometimes, 
when you see Sigma, that means standard deviation when you see it next to a 
parenthesis like this, it means the sigmoid function.</p>

<p>Okay, so in other 
words that okay, which looks like that okay. So this is like a little mini 
neural net, with no hidden layers or to think of it. Another way, it's like 
a little logistic regression. Okay - and this is - I mentioned this 
briefly, because it's going to come up a lot in part two, and so it's a 
good thing to like start learning about it's. This idea that, like in the 
very learning itself, you can have like little mini neural nets inside your 
neural nets right and so this little mini neural net is going to be used to 
decide how much of my hidden state am. I going to remember right, and so it 
might learn that Oh in this particular situation forget everything you 
know. For example, oh there's a full-stop, you know hey when you see a 
full-stop, you should throw away nearly all of your hidden state. That is 
probably something it would learn and that that's very easy for it to learn 
using this little mini neuron there, okay, and so that goes through to 
create my new hidden state, along with the input and then there's a second 
thing. That happens, which is there's this gate here called Z and what Z 
says is all right. You've got your some amount of your previous hidden 
state, plus your new input right and it's going to go through to create 
your new state and I'm going to.</p>

<p>Let you decide to what degree do you use 
this new input version of your hidden state and to what degree where you 
just leave the hidden state the same as before? So this thing here is 
called the update gate right, and so it's got two choices. It can make the 
first-years to throw away some hidden state when deciding how much to 
incorporate that versus my new input and how much to update my hidden state 
versus just leave it exactly the same. And the equation, hopefully, is 
going to look pretty similar to you, which is check this out here. Remember 
how I said you want to start to recognize some some common ways of looking 
at things. Well, here I have a 1 something by a thing and a something 
without the 1 by a thing which remember is a linear interpolation right, 
so, in other words, the value of Z is going to decide to what degree do I 
have keep the previous hidden state And to what degree do I use the new 
hidden state right? So that's why they draw it here, as this kind of like 
it's not actually a switch, but like you can put it in any position. You 
can be like. Oh, it's here or it's here or it's here to decide how much 
that'll do ok, so so they're, basically the equations. It's a it's a little 
mini neural net, with its own weight matrix to decide how much to update 
little mini neural net with its own weight.</p>

<p>Matrix to decide how much to 
reset and then that's used to do an interpolation between the two hidden 
states, so that's called giu gated recurrent Network there's the definition 
from the pytorch source code. They have some slight optimizations here 
that, if you're interested in, we can talk about them on the forum, but 
it's exactly the same for we just saw, and so if you go and NDI you that it 
uses this same code, but it replaces the iron in so With this cell, okay 
and as a result, rather than having something where we needed, where we 
were getting a 1.54 we're nowgetting down to one point 400 and we can keep 
training even more get right down to one point: three: six: okay, so in 
practice a Giu or very nearly equivalently, we'll see in a moment in lsdm 
in practice what pretty much everybody always uses. So the art he and HT 
are ultimately scalars after they go through the sigmoid, but there are 
Clyde element-wise. Is that correct, mm-hmm yeah, although of course one 
for each mini batch? But yes, the scaler yeah, okay, great thanks and on 
the excellent colas </p>

<h3>9. <a href="https://youtu.be/H3g26EVADgY?t=53m40s">00:53:40</a></h3>

<ul style="list-style-type: square;">

<li><b> Long Short Term Memory (LSTM), ‘LayerOptimizer()’, Cosine Annealing ‘CosAnneal()’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Blog Chris Ellis blog there's an understanding, LS TM networks post, which 
you can read all about this in much more detail if you're, interested and 
also the other one I was dealing from here - is a wild ml, also have a good 
blog post on this there's. Somebody wants to be helpful, feel free to put 
them in the lesson: wiki. Okay, so then putting it all together. I'm now 
going to replace my GI, you with Nellis TM, I'm not going to bother showing 
you the soul for this. It's very similar to GI! U but the LS TM has one 
more piece of state in it called the sell state, not just the hidden state. 
So if you do use an LS, TM you're now inside you're in it hidden, have to 
return a couple of matrices, they're exactly the same size as the hidden 
state, but you just have to return the tupple okay, the details, don't 
matter too much, but we can Talk about it during the week, if you're 
interested, you know, when you pass in you still pass in self dot, H still 
returns. A new value of H is to repackage it in the usual way. So this code 
is identical to the code before one thing. I've done, though, is I've had a 
drop out inside my air and in which you can do with the height watch, R and 
n function. So that's going to do drop out after a time step and I've 
doubled the size of my hidden layer.</p>

<p>Since I've now added point five drop 
out, and so my hope was that this would make it be able to learn more but 
be more resilient as it does so so then I wanted to show you how to take 
advantage of a little bit more fastai Magic without using the layer class, 
and so I'm going to show you how to use Cobra and specifically we're going 
to do s GDR without without using the learner class. Ok. So to do that, we 
create our model again, just a standard, pytorch model. Ok and this time, 
rather than going remember the usual pipe torch approach is: opt equals up 
to M, naught atom and you pass in the parameters and a learning rate. I'm 
not going to do that. I'm going to use the fastai layer, optimizer class, 
which takes my opt-in class constructor from pytorch. It takes my model, it 
takes my learning rate and optionally takes weight. Decay, ok, and so this 
class is tiny. It doesn't do very much at all. The key reason it exists is 
to do differential learning rates and differential weight decay right, but 
the reason we need to use it is that all of the mechanics inside fastai 
assumes that you have one of these right. So if you want to use like 
callbacks or SPDR or whatever in code, where you're not using the learner 
class, then you need to use rather than saying you know: opt equals off to 
m dot, atom and here's my parameters, you instead say lay optimizer, okay, 
so That gives us a layer, optimizer object and, if you're interested 
basically behind the scenes, you can now grab a dot, opt property which 
actually gives you the optimizer.</p>

<p>But you don't have to worry about that 
itself, but that's basically what happens behind the scenes? The key thing 
we can now do is that we can now when we call fit, we can pass in that 
optimizer and we can also pass in some callbacks and specifically we're 
going to use the cosine annealing callback, okay, and so the cosine 
annealing callback requires a Layer optimizer object right, and so what 
this is going to do is it's going to do cosine annealing by changing the 
learning right inside this object? Okay, so the details aren't terribly 
important. We can talk about them on the forum. It's really the concept I 
wanted to get across here right, which is that now that we've done this, we 
can say all right create a cosine and kneeling callback which is going to 
update the learning rates in this layer. Optimizer, the length of an epoch 
is equal to this here right. How many mini batches are there in an epoch? 
Well, it's whatever the length of this data. Loader is okay, so because 
it's going to be it's going to be doing the cosine annealing, it needs to 
know how often to reset okay and then you can pass in the cycle more in the 
usual way, and then we can even save our model automatically. Like you 
remember how there was that cycle saved name parameter that we can pass to 
learn not fit. This is what it does behind the scenes behind the scenes. It 
sets an on cycle, end callback, and so here I have to find that callback as 
being something that saves my model.</p>

<p>Okay, so there's quite a lot of cool 
stuff that you can do with callbacks. Callbacks are basically things where 
you can define like at the start of training or at the start of an epoch or 
at the side of a batch or at the end of training or at the end of an epoch 
or at the end of a batch. Please call this club okay and so we've written 
some for you, including SGD R, which is the cosine annealing callback and 
then so how I recently wrote a new callback to implement the new approach 
to decoupled weight decay. We use callbacks to draw those little graphs of 
the loss over time, so there's lots of cool stuff. You can do with 
callbacks. So in this case, by passing in that callback, we're getting as 
JDR and that's able to get us down to one point three one here and then we 
can train a little bit more and eventually get down to one point: two five, 
and so we can now Test that out - and so if we passed in a few characters 
of text we get not surprisingly and a after four. Thus, let's do then 400, 
and now we have our own Nietzsche, so Nietzsche tends to start his sections 
with a number and a dot. So 2, 9 3, perhaps that every life, the values of 
blood have intercourse when it senses there is unscrupulous his very rights 
and still impulse love. Ok, so I mean it's slightly less clear than 
Nietzsche normally, but it gets the tone right.</p>

<p>Ok - and it's actually 
quite interesting like if to play around with training these character, 
based language models, to like run this at different levels of loss, to get 
a sense of like what does it look like, like you really notice that this is 
like 1.25 and, like That's slightly worse, like 1.3, this looks like total 
junk. You know, there's like punctuation in random places, and you know 
nothing makes sense and, like you start to realize that the difference 
between you know, Nietzsche and random. Junk is not that far in kind of 
language model terms, and so, if you train this for a little bit longer, 
you'll suddenly find like. Oh it's it's it's making more and more sense. 
Okay. So if you are playing around with NLP stuff, particularly generative 
stuff like this and you're like there is also like kind of okay, but not 
great, don't be disheartened, because that means you're, actually very, 
very nearly there. You know the the difference between like something which 
is starting to create something which almost vaguely looks English, if you 
squint and something that's, actually a very good generation. It's it's not 
it's not far in most motion tests. Okay, great! So, let's take a 
</p>

<h3>10. <a href="https://youtu.be/H3g26EVADgY?t=1h1m47s">01:01:47</a></h3>

<ul style="list-style-type: square;">

<li><b> Pause</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Five-Minute break we'll come back at 7:45 and we're going to go back to 
computer vision. </p>

<h3>11. <a href="https://youtu.be/H3g26EVADgY?t=1h1m57s">01:01:57</a></h3>

<ul style="list-style-type: square;">

<li><b> Back to Computer Vision with CIFAR 10 and ‘lesson7-cifar10.ipynb’ notebook, Why study research on CIFAR 10 vs ImageNet vs MNIST ?</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Okay, so now become full circle back to vision, so now we're looking at 
less than seven so far ten that book, you might have heard of so far ten. 
It's a really well-known data set in academia and it's partly it's well 
known. It's actually pretty old by you know: computer vision standards well 
before image net was around. There was sci-fi 10. You might wonder why 
we're going to be looking at such an old data set, and actually I think, 
small data sets - are much more interesting than image net because, like 
most of the time, you're likely to be working with stuff, with a small 
number of thousands of Images, rather than one and a half million images, 
some of you will work at one and a half million images, but most of you 
won't right so learning how to use these kind of data sets, I think, is 
much more interesting, often also a lot of the Stuff we're looking at like 
in medical, imaging we're looking at like the specific area where there's a 
lung, nodule you're, probably looking at like 32 by 32 pixels at most as 
being the area where that lung nodule actually exists right and so sci-fi 
10 is small. Both in terms of it doesn't have many images, and the images 
are very small, and so therefore, I think this is like it's been in. A lot 
of ways is much more challenging. Then something like image net and in some 
ways it's much more interesting right and also, most importantly, you can 
run stuff much more quickly on earth.</p>

<p>So it's much better to test out your 
algorithms with something you can run quickly and they're still 
challenging. And so I hear a lot of researchers complain about like how 
they can't afford to study all the different versions that their algorithm 
properly, because it's too expensive and they're doing that on imagenet. 
So, like it's literally a week of you know expensive CP GPU work for every 
study they do and like. I don't understand why you would do that kind of 
study on imagenet doesn't make sense yeah, and so this has been a 
particularly you know. There's been a particular a lot of kind of debate 
about this this week, because I'm really interesting researcher named Ali 
raha me nips this week gave a talk. A really great talk about kind of the 
need for rigor in experiments in deep learning, and you know he felt like 
there's a lack of rigor and I've talked to him about it quite a bit since 
that time and I'm not sure we yet quite understand each Other as to where 
we're coming from, but but we have very similar kinds of concerns, which is 
basically people aren't doing carefully tuned carefully thought about 
experiments, but instead they kind of throw lots of GPUs or lots of data 
and consider that a day. And so this idea of like saying like well, you 
know it's my data statement. It's my algorithm meant to be good.</p>

<p>That's 
moral imagers at small data sets well, if so, let's study on sci-fi 10 
revin, studying it on imagenet and then do more studies of different 
versions of the algorithm and learning different bits on and off, 
understand which parts are actually important and so forth. People also 
complain a lot about amnesty, which we've talked about looked at before and 
I would say the same thing about em this right, which is like, if you're, 
actually trying to understand rich parts of your algorithm, make a 
difference and why using m. Mr? That kind of study is a very good idea and 
all these people who complain about em NIST, I think they're, just showing 
off they're saying like oh, I work at Google and I have you know a part of 
TP use and I have $ 100,000 a week Of time just being on it no worries, but 
I don't know, I think, that's all it is. You know it's just signalling, 
rather than actually academically rigorous, okay. So I'm so far ten you can 
download from here, and this person is very kindly made it available in 
image form. If you, google, for size 510 you'll find us a much less 
convenient form. So please use this one. It's already in the exact form you 
need once you download it, you can use it in the usual way. So here's a 
list of the classes that are there now you'll see here. I've created this 
thing called that's normally.</p>

<p>When we've been using pre trained models, we 
have been seeing transforms from model and that's actually created the 
necessary transforms to convert our data set into a normalized data set 
based on the means and standard deviations of each channel in the original 
model. That was trained in our case, though this time we got a trainer 
model from scratch, so we have no such thing, so we actually need to tell 
it the mean and standard deviation of our data to normalize it. Okay - and 
so in this case I haven't included the code here to do it. You should try 
and try this yourself to confirm that you can do this and understand where 
it comes from, but this is just the mean channel and the standard deviation 
per channel of all of the images. Alright, so we're going to try and create 
a model from scratch, and so the first thing we need is some 
transformations, so for so far 10 people generally do data augmentation of 
simply flipping randomly horizontally. So here's how we can create a 
specific list of augmentations to use and then they also tend to add a 
little bit of padding black padding around the edge and then randomly pick 
a 32 by 32 spot from within that pattern image. So if you add the pad 
parameter to any of the fast, a a transform creators, it'll it'll do that 
for you, okay, and so in this case, I'm just going to add 4 pixels around 
each size, and so now that I've got my transforms. I can go ahead and 
create my image classifier data from paths in the usual way.</p>

<p>Okay, I'm 
going to use a batch size of 256 because these are pretty small. So it's 
going to, let me do a little bit more at a time. So here's what the data 
looks like so, for example, here's a boat and just to show you how tough 
this is. What's that? Okay, it is it's a not chicken prog-rock. So I guess 
it's this big thing. Whatever the thing is called there's your frog, okay, 
so so these are the kinds of things that we want to look at. So I'm going 
to </p>

<h3>12. <a href="https://youtu.be/H3g26EVADgY?t=1h8m54s">01:08:54</a></h3>

<ul style="list-style-type: square;">

<li><b> Looking at a Fully Connected Model, based on a notebook from student ‘Kerem Turgutlu’, then a CNN model (with Excel demo)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Start out so our student, Karen, we saw one of his posts earlier in this 
course. He he made this really cool log book which shows how different 
optimizers works. There we go. So Karen made this really cool notebook. I 
think it was maybe last week in which he showed how to create various 
different optimizers from scratch, so this is kind of like the Excel thing 
I had, but this is the Python version of momentum and Adam and Nesterov and 
Atta grad all written from scratch. It is very cool one of the nice things 
he did was. He showed a tiny little general-purpose, fully connected 
Network generator, so we're going to start with his so so he called that 
simple net. So are we so here's a simple class which has a list of fully 
connected layers? Okay, whenever you create a list of layers in pytorch, 
you have to wrap it in an end module list just to tailpipe torch, to like 
register these as attributes. And so then we just go ahead and flatten the 
data that comes in because it's fully connected layers and then go through 
each layer and call that linear layer do the value to it and at the end, do 
a soft mess. Okay, so there's a really simple approach, and so we can now 
take that model. And now I'm going to show you how to step up one level of 
the API higher rather than calling the fit function.</p>

<p>We're going to create 
a learn object, but we're going to create a learn, object from a custom 
model. And so we can do that by say: we want a convolutional learner, we 
want to create it from a model and from some data and the model is this 
one. So this is just a general height watch model and this is a model data 
object of the usual kind and that will return a loaner. So this is a bit 
easier than what we just saw with the RNN. We don't have to fiddle around 
with lair, optimizers and cosine and kneeling and callbacks, and whatever 
this is now a loaner that we can do all the usual stuff with that we can do 
it with any model that we created. Okay. So if we just go, learn that'll go 
ahead and print it out; okay, so you can see. We've got three thousand and 
seventy two features coming in, because you've got 32 by 32 pixels by three 
channels: okay, and then, we've got 40 features coming out of the first 
layer, that's going to go into the second layer. Ten features coming out 
because we've got the ten so far ten categories. Okay, you can call dot 
summary to see that a little bit more detail, we can do LR find we can plot 
that and we can then go fetch and we can use cycle length and so forth. 
Okay, so with a simple how many hidden layers do we have one hidden layer 
right, one getting layer, one output, layer, one hidden layer model with, 
and here we can see the number of parameters we have is that over 120,000 
okay, we get a 47 percent accuracy.</p>

<p>So not great all right, so let's kind 
of try and improve it right, and so the goal here is we're going to try and 
eventually replicate the basic kind of architecture of a resonator okay. So 
that's where we're going to try and get to hear it specially built up to a 
resonant, so the first step is to replace our fully connected model with a 
convolutional model. Okay, so, to remind you, so to remind you, a fully 
connected layer is simply doing a dot product right. So if we had like all 
of these data points and all of these weights right, then we basically do a 
sum product of all of those together right. In other words, it's a matrix 
multiply right. Then that's a fully connected layer, okay, and so we need 
the white matrix is going to take, contain an item for every every element 
of the input for every element of the output. Okay. So that's why we have 
here a pretty big weight matrix and so that's why we had, despite the fact 
that we have such a crappy accuracy. We have a lot of parameters because in 
this very first layer we've got 3072 coming in and for T coming out. So 
that gives us three thousand times forty parameters, and so we end up not 
using them very efficiently, because we're basically saying every single 
pixel in the input has a different weight. And, of course, what we really 
want to do is kind of find groups of three by three pixels that have 
particular patterns to them.</p>

<p>Okay, and remember, we call that a convolution 
okay, so a convolution looks like so. We have like little 3x3 section of 
our image and a corresponding 3x3 set of filters right or our filter with a 
three by three kernel, and we just do a sum product of just that three by 
three by that three by three okay and then we do That for every single part 
of our image right, and so when we do that across the whole image, that's 
called a convolution and remember in this case we actually had multiple 
filters right, so the result of that convolution actually had multiple. It 
was a tensor with an additional third dimension to it effectively. So let's 
take exactly the same code that we had before, but we're going to replace n 
n dot linear with NN com2 D. Okay. Now what I want to do in this case, 
though, is each time I have a layer I want to the next layer, smaller and 
so the way I did that in my Excel example, was I used max Pauling right, so 
max Pauling took every 2x2 section and Replaced it with this maximum value 
right nowadays, we don't use that kind of max bowling much at all. Instead, 
nowadays, what we tend to do is do what's called a stride to convolution. 
Let's drag to convolution, rather than saying, let's go through every 
single 3x3. It says. Let's go through every second 3x3 so, rather than 
moving this three by three one to the right, we move it two to the right 
and then when we get to the end of the row rather than moving one row down, 
we move two rows down.</p>

<p>Okay, so that's called a stride to convolution, and 
so it's tried to convolution has the same kind of effect as a max pooling, 
which is you end up having the resolution in each dimension, so we can ask 
for that by saying stroud equals to okay. We can say we wanted to be three 
by three by saying kernel, size and then the first term parameters are 
exactly the same as nn, but linear they're, the number of features coming 
in and the number of features coming out. Okay, so we create a multiple 
list of those layers and then at the very end of that. So in this case I'm 
going to say, okay, I've got three channels coming in the first one layer 
will come out with 20, then a at 40 and then 80. So if we look at the 
summary we're going to start with a 32 by 32, we're going to spit out of 15 
by 15 and then a 7 by 7 and then a 3 by 3 right. And so what do we do now 
to get that down? To a prediction of one of 10 classes, what we do is we do 
something called adaptive max pooling, and this is what is pretty standard 
now for state-of-the-art algorithms. Is that the very last layer we do a 
max pool, but rather than doing like a 2 go to next Paul, we say like it 
doesn't. Have you to bow to could have been 3x3, which is like replace 
every three by three pixels with its maximum could have been four by four 
adaptive backs. Paul is where you say: I'm not going to tell you how big an 
area to pull, but instead I'm going to tell you how big a resolution to 
create right.</p>

<p>So, if I said, for example, I think my input here is like 28 
by 28 right. If I said, do a 14 by 14 adaptive max Paul, that would be the 
same as a 2 by 2 max Paul, because in other that's saying, please create a 
14 by 14 output. If I said, do a 2 by 2 adaptive max Paul right, then that 
would be the same as saying do a 14 by 14 max Paul, and so what we pretty 
much always do in modern cnn's. Is we make our per northmet layer a 1 by 1 
adaptive Max Paul so in other words, find the single largest cell and use 
that as our new activation right, and so once we've got that we've now got 
a 1 by 1 tensor right, we're actually 1 By 1 by number of features tensor, 
so we can then, on top of that, go view X, dot view X, dot, size, comma, 
minus 1, and actually there are no other dimensions to this. Basically 
right. So this is going to return a matrix of mini-batch by number of 
features, and so then we can feed that into a linear layer with, however 
many classes, we need right, so you can see here. The last thing I pass in 
is how many classes am I trying to predict and that's what's going to be 
used to create that last layer, so it goes through every convolutional 
layer? Does a convolution does arel you? Does an adaptive max pool this dot 
view just gets rid of those trailing unit, backsies one comma, one axis, 
which is not necessary. That allows us to fit that into our final linear 
layer that spits out something of size C, which here is ten, so you can now 
see how it works.</p>

<p>It goes 32 to 15 to 7 by 7 to 3 by 3. The adaptive next 
pull makes it 80 by 1 by 1 right and then our dot view makes it just a mini 
batch size by 80 and then finally, a linear layer which makes it from 80 to 
10, which is what we wanted. Okay, so that's our like. Most basic, you 
would call this a fully convolutional network, so a fully convolutional 
network is something where every layer is convolutional, except for the 
very last. So again, we can now go Li dot, find and now, in this case, when 
I did ll find it went through the entire data set and we're still getting 
better, and so, in other words, even a the default final learning rate 
rises, 10, and even at that Point it was still like pretty much getting 
better, so you can always override the final learning rate by saying n del 
R equals that and that will get it just to get it to try morphine's, okay, 
and so here is the learning rate finder, and so I Picked 10 to the minus 1 
trained that for a while and that's looking pretty good. So I try to put 
the cycle length of 1 and it's starting to flatten out at about 60 % right. 
So you can see here the number of elements. The number of parameters I have 
here are 500 7000 28,000 about 30,000 right. So I have about a quarter of 
the number of routers that my accuracy has gone up from 47 % to 60 % right 
and the time per epoch.</p>

<p>Here is under 30 seconds, and here also so the time 
period box about the same, and that's not surprising because when you use 
small, simple architectures most of the time is the memory transfer. The 
actual time during the compute is is trivial. Okay, so I'm going to 
refactor </p>

<h3>13. <a href="https://youtu.be/H3g26EVADgY?t=1h21m54s">01:21:54</a></h3>

<ul style="list-style-type: square;">

<li><b> Refactored the model with new class ‘ConvLayer()’ and ‘padding’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>This slightly because I want to try and put less stuff inside my forward 
and so calling RAL you every time you know it doesn't seem ideal. So I'm 
going to create a new class called conf lair. Okay and the conf lair class 
is going to contain a convolution with a kernel size of three and a stride 
of two one thing I'm going to do now is I'm going to add padding. Did you 
notice here? The first layer went from 32 by 32 to 15 by 15, not 16 by 16, 
and the reason for that is that, at the very edge of your convolution right 
here see how this first convolution like there isn't a convolution where 
the middle is. The top left. Point right because there's like nothing 
outside it, where else, if we had put a row of zeros at the top and a row 
of zeros at the edge of each column, we now could go all the way to the 
edge. Alright, so pad equals 1 adds that little layer of zeros around the 
edge for us, ok, and so this way we're going to make sure that we go 32 by 
32 to 16 by 16 to 8 by 8. It doesn't matter too much when you've got these 
bigger layers, but by the time you get down to like say, 4 by 4. You really 
don't want to throw away a whole piece right, so padding becomes important 
so by refactoring it to put this with its defaults. Here and then in the 
forward, I put the value in here as well. It makes by confident you know a 
little bit smaller and you know more to the point. It's going to be easier 
for me to make sure that everything is correct in the future.</p>

<p>By always 
using this common player class, ok, so now you know not only how to create 
your own neural network model, but how to create your own neural network 
layer. So here now I can use conf layer right - and this is such a cool 
thing about pytorch is a layer definition and a neural network definition 
are literally identical. Okay, they both have a constructor and a forward, 
and so anytime you've got the lair. You can use it as a neural net anytime. 
You have a neural net, you can use it as a lair okay. So this is now the 
exact same thing as we had before. One difference is. I now have padding 
okay and another thing just to show you. You can do things differently back 
here, my max pool I did as as an object likely. I use the class n n dot 
adaptive max pool, and I stuck it in this attribute and then I called it, 
but this actually doesn't have any state. There's no weights inside max 
pooling, so I can actually do it with a little bit less code by calling it 
as a function right. So everything that you can do as a class. You can also 
do as a function. It's inside this capital F, which is n n dot functional 
okay. So this should be a tiny bit better, because this time I've got the 
padding. I didn't trade it for as long to actually check. So, let's skip 
over that all right. So one issue here is that, in the end this is having 
I, when I tried to add more layers, I had travel training it. Okay and the 
reason I was having trouble training it is it was you know.</p>

<p>If I used 
larger learning rates, it would go off to NI N and if I use smaller 
learning rates that are kind of takes forever and doesn't really have a 
chance to explore properly. So it wasn't resilient. So to make my 
</p>

<h3>14. <a href="https://youtu.be/H3g26EVADgY?t=1h25m40s">01:25:40</a></h3>

<ul style="list-style-type: square;">

<li><b> Using Batch Normalization (BatchNorm) to make the model more resilient, ‘BnLayer()’ and ‘ConvBnNet()’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Model more resilient, I'm going to use something called batch normalization 
which literally everybody calls bachelor and bachelorettes a couple of 
years old now and it's been pretty transformative since it came along 
because it suddenly makes it really easy to train deeper networks. Alright, 
so the network I'm going to create is going to have more layers right. I've 
got one two three four five convolutional layers plus a fully connected 
layer right so like back in the old days. That would be considered a pretty 
deep network and we considered pretty hard to train nowadays. Super simple 
thanks to vaginal now to use batch norm. You can just write in end on that 
to learn about it, we're going to write it from scratch. Okay, so the basic 
idea of batch norm is that we've got some vector of activations anytime I 
draw a vector of activations. Obviously I mean you can repeat it for the 
mini batch. So I pretend it's a mini batch with one. So we've got some 
veteran activations and it's coming into some layer right, so so probably 
some convolutional matrix multiplication and then something comes out. The 
other side. So imagine this. This is just a matrix multiply, which was like 
I don't know, say it was a identity matrix right then, every time I'd 
multiply it by that across lots and lots of layers.</p>

<p>My activations are not 
getting bigger, they're, not getting smaller they're, not changing at all. 
Okay! That's all fine right, but imagine if it was actually like 2 2, 2 
right, and so if every one of my weight, matrices or filters was like that, 
then my activations are doubling each time right and so suddenly I've got 
as exponential growth and that in deep Models, that's going to be a 
disaster right because my gradients are exploding at an exponential rate, 
and so the challenge you have is that it's, it's very unlikely. Unless you 
try carefully to deal with it, that your matrices, your weight, matrices on 
average, are not going to cause your activations to keep getting smaller 
and smaller, or keep getting bigger and bigger right. You have to kind of 
carefully control things to make sure that they stay. You know at a 
reasonable size. You want to, you, know, keep them at a reasonable scale. 
So we start things off with zero, mean standard deviation, one by 
normalizing. The inputs really like to do is to normalize every layer, not 
just the inputs, all right and so, okay, fine, let's do that right. So here 
I've created a BN layer which is exactly like my Kampf layer. It's got my 
common 2d with my stride. My padding right, I do my condom, I value right 
and then I calculate the mean of each channel or of each filter and the 
standard deviation of each channel or each filter. And then I subtract the 
means and divide by the standard deviations right.</p>

<p>So now I don't actually 
need to normalize my input at all, because it's actually going to do it 
automatically right, it's normalizing it per channel or and for later 
layers its normalizing it per filter. So it turns out that's not enough 
right, because SGD is bloody-minded right, and so, if sgt decided that it 
what's the weight matrix to be. You know like so where that matrix is 
something which is going to. You know, increase the values overall 
repeatedly then trying to divide it by the subtract, domains and divide by 
the standard deviations just means the next mini-batch. It's going to try 
and do it again and they were try and do it again, it'll try and do it 
again. So it turns out that this actually doesn't help like it literally 
does nothing, because SGD is just going to go ahead and undo it. The next 
mini batch. So what we do is we create a new multiplier for each channel 
and a new added value for each Channel literally just and we just start 
them out as the addition and addition is just a bunch of zeros so for the 
first layer, three zeros and the Multiplier for the first layer is just 
three ones. Okay, so number of filters for the first layer is just three, 
and so we then, like basically undo exactly what we just did or potentially 
we undo them right. So by saying this is an addenda parameter that tells 
pytorch you're allowed to learn these as weights right so initially it 
says: okay, so check the means divided by the standard deviations 
multiplied by one add on zero.</p>

<p>Okay, that's fine! Nothing much happened 
there, but what it turns out is that now, rather than like, if it wants to 
kind of scale the layer up, it doesn't have to scale up every single value 
in the matrix. It can just scale up this single trio of numbers self dot M. 
If it wants to shift it all Apple down, a bit doesn't have to shift the 
entire weight matrix. They can just shift this trio of numbers self dot a 
so I will say this, I'm at this talk I mentioned at nips Alley. Rahim ease 
talk about rigor. He actually pointed to this paper this batch norm paper 
as being a particularly useful, particularly interesting paper, where a lot 
of people, don't necessarily we quite quite know why it was right, and so, 
if you're thinking like okay subtracting out the means and then adding some 
learned Weights of exactly the same rank and size sounds like a weird thing 
to do. There are a lot of people that feel the same way right. So at the 
moment, I think the best is I can say like intuitively is what's going on 
here. Is that we're normalizing the data and then we're saying you can then 
shift it and scale it using far fewer parameters than would have been 
necessary. If I was asking you to actually shift and scale the entire set 
of convolutional filters right, that's the kind of basic intuition, more 
importantly, in practice, what this does is it adds.</p>

<p>Is it basically allows 
us to increase our learning rates and it increases the resilience of 
training and allows us to add more layers? So once I added a PN layer 
rather than a common flower, I found I was able to add more layers to my 
model and it still trained effectively. Generally, are we worried about 
anything that maybe we are divided by something very small or anything like 
that? Once we do this, probably I think in the pie chart version, it would 
probably be divided by itself, dudes plus Epsilon or something yeah. This 
worked fine for me, but yeah. That is definitely something to think about. 
If you were trying to make this more reliable, I mentioned poplar, so the 
self dot m and self dot a getting it's getting updated through back 
propagation as well yeah. So, by putting like saying it's an N n dot 
parameter, that's how we flag to pytorch! To learn it through that probe 
exactly right. The other interesting thing it turns out the batch norm 
does. Is it regularizes? In other words, you can often decrease or remove, 
drop out or decrease or remove weight? Okay, when you use batch normal and 
the reason why is, if you think about it, each mini batch is going to have 
a different mean and a different standard deviation to the previous mini 
batch. So these things keep changing and because they keep changing it's 
kind of changing the meaning of the filters in this subtle way, and so it's 
adding a regularization effect, because it's noise that when you add noise 
of any kind, it regularizes your model, all right.</p>

<p>I'm actually cheating a 
little bit here in the real version of batch norm. You don't just use this 
batches mean and standard deviation, but instead you take an exponentially 
weighted moving average standard deviation and - and so if you wanted to 
exercise to try a during the week, that would be a good thing to try. But I 
will point out something very important here, which is, if self-training, 
when we are doing our training loop. This will be true when it's being 
applied to the training set and it will be false when it's being applied to 
the validation set, and this is really important, because, when you're 
going through the validation set, you do not want to be changing the 
meaning of the Model, okay, so this is this really important idea is that 
there are some types of layer that are actually sensitive to what the mode 
of the of the network is, whether it's in training mode or as plight, which 
calls it evaluation mode, or we might say a Test mode right and actually we 
actually had a bug, a </p>

<h3>15. <a href="https://youtu.be/H3g26EVADgY?t=1h36m2s">01:36:02</a></h3>

<ul style="list-style-type: square;">

<li><b> Previous bug in ‘Mini net’ in ‘lesson5-movielens.ipynb’, and many questions on BatchNorm, Lesson 7 Cifar10, AI/DL researchers vs practioners, ‘Yann Lecun’ &amp; ‘Ali Rahimi talk at NIPS 2017’ rigor/rigueur/theory/experiment.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Couple of weeks ago, when we did our mini net for movie lens the 
collaborative filtering, we actually had F dot dropout in our forward pass 
without protecting it with a F self training, F, dot dropout, as a result 
of which we were actually doing dropout in the Validation piece, as well as 
the training piece, which obviously isn't what you want: okay, so I've 
actually gone back and fixed this by changing it to using n n dot dropout 
and n n dot. Dropout has already been written for us to check whether it's 
being used in training mode or not that or alternatively, I could have 
added a if self dot training before I use the dropout yeah. Okay. So it's 
important to think about that. You know any and the main, the main true or 
pretty much the only two built-in two pytorch, where this happens is 
dropout and, and so interestingly, this is also a key difference in fastai, 
which no other library does is that these means and standard deviations Get 
updated in training mode in every other library as soon as you basically 
say, I'm training, regardless even of whether that layer is set to 
trainable or not, and it turns out that with a pre trained network. That's 
a terrible idea! If you have a pre trained network for specific values of 
those means and standard deviations in batch norm, if you change them, it 
changes the meaning of those pre trained layers right and so in fastai, 
always by default. It won't touch those means and standard deviations.</p>

<p>If 
your layer is frozen, okay as soon as you, I'm freezing it'll start 
updating them unless you've set won't, be and freeze. True, if you set 
learned up being freeze true, it says: never touch these met, means and 
standard deviations, and you know I've found in practice that that often 
seems to work a lot better for pre-trained models, particularly if you're 
working with data that's quite similar to what The pre trained model was 
trained with you know. As you look like, I did a lot of work. Did you say 
sorry, like quite a lot of code here well you're doing more work than you 
would normally do essentially you're calculating all these aggregates as 
you go through each each each layer? Yes, wouldn't this mean you're 
training like your epoch time like now, this is like super fast like if you 
think about what a cone has to do. A cones has to go through every 3x3. You 
know with a stride and do this multiplication and then addition like that 
is a lot more work than simply calculating the per channel mean. So this is 
so and that's a little bit of time, but it's it's less time intensive than 
the convolution. Would it be like right after, like decomposition, yeah, 
we'll talk about that in a moment? So at the moment we have it after the 
rally and in the original batch norm paper. I will that's where they put 
it. So this this idea of something called an ablation study and an ablation 
study is something where you basically try kind of turning on and off 
different pieces of your model to see like which bits make which impacts 
and one of the things that wasn't done in the Original batch norm paper was 
any kind of really effective, ablation study and one of the things 
therefore, that was missing was this question, which you just asked, which 
is like: where do you put the vaginal before the early year after the 
earlier whatever? And so since that time, you know that oversight has 
caused a lot of problems because it turned out the original paper didn't 
actually put it in the best spot, and so then other people since then have 
now figured that out and they're like every time.</p>

<p>I show people code where 
it's actually in the spot, that turns out to be better people, always say 
your bedrooms in the wrong spot, and I have to go back and say no, I know 
that's what the paper said. What they're doing now. That's what I thought, 
and so it's kind of causes confusion. So there's there's been a lot of 
question about that. So a little bit of a higher-level question, so we 
started out with cipher data. Yes, it's the basic reasoning that you use a 
smaller data set to quickly train a new model, and then you take it the 
same model and you're using much much much bigger data set to get a higher 
accuracy level. Is that the basic? Maybe so, if you want to you know, if 
you had a large data set or if you were like interested in the question of 
like how good is this technique on a large data set, then, yes, what you 
just said would be what I would do. I would do lots of testing on a small 
data set, which I had already discovered, had the same kinds of properties 
as my larger data set, and therefore my conclusions would likely carry 
forward and then our test them at the end. Having said that, personally, I 
matched be more interested in actually studying small datasets for their 
own sake, because I find most people I speak to in the real world. Don't 
have a million images they have.</p>

<p>You know somewhere between about two 
thousand, and twenty thousand images seems to be much more common, so I'm 
very you know very interested in having fewer rows, because I think it's 
more valuable in factors I'm also pretty interested in small images, not 
just for the rest. You mentioned, which is it allows me to test things out 
more quickly, but also, as I mentioned before. Often a small part of an 
image actually turns out to be what you're interested in that's. Certainly 
true in in medicine, I have two questions. The first is on what you 
mentioned in terms of small datasets, particular middle medical. Imaging 
you've, you've heard of, I guess, is it vicarious to start up in the 
specialization and one-shot learning, so your opinions on that, and then 
this second being this is related to. I guess Ali's talk at nips, so it 
was, I don't say its controversial, but like young laocon, there was like 
a really, I guess, controversial thread attacking you in terms of what 
you're talking about as a baseline of theory, just not keeping up with 
practice, and so I mean I guess I was siding with the on where's all he 
actually, he tweeted at me quite a bit trying to defend like he wasn't 
attacking yawn at all, but in fact he was you know trying to support him, 
but I just kind of feel like A lot of theory, as as you go, is just sort of 
at it. They even it's hard to keep up whether then you know no archive from 
on Draco party to keep up.</p>

<p>But if the theory isn't keeping up, but industry 
is the one that's actually sitting in the standard, then doesn't that mean 
that you know people who are actual practitioners are the ones like young 
lacunae are publishing the theory that are keeping up to date or is like 
Academic research institutions are actually behind, so I don't have any 
comments on the vicarious papers because I haven't read them, I'm not aware 
of any of them have as actually showing you know better results than the 
papers, but I think they've come a long way in the Last twelve, so that 
might be wrong. Yeah yeah, I viewed the discussion between yarn lacunae and 
a lyric. Jimmy is very interesting because they're, both smart people who 
have interesting things to say. Unfortunately, a lot of people talk, Ally's 
talk as meaning something which he says it didn't mean and when I listened 
to his talk, I'm not sure he didn't actually made it at the time, but he 
clearly doesn't mean it now, which is he's. He's now said many times he 
didn't. He was not talking about theory. He was not saying, we need more 
theory at all. Actually, he thinks we need more experiments, and so 
specifically he's he's also now saying he wished. He hadn't used the word 
rigour which I also wish, because rigour is it's kind of meaningless and 
everybody can kind of say when he says rigor. He means the specific thing I 
study you know.</p>

<p>So a lots of people have kind of taken his talk as being 
like. Oh yes, this proves that nobody else should work in neural networks 
unless they are experts at the one thing, I'm an expert in so yeah. So I'm 
going to catch up with him and talk about more about this in January and 
hopefully we'll pick up some more stuff out together. But basically what 
would we can clearly agree on, and I think you and also agrees on is 
careful. Experiments are important. Just doing things on massive amounts of 
data using massive amounts of TP use or GP users, not interesting of 
itself, and we should instead try to design experiments that give us the 
maximum amount of insight into what's going on so Jeremy. Is it a good 
statement to say something like so drop out and bash norm are very 
different? Things drop out is the realization technique. Bash norm has 
maybe some realization effect, but it's actually just about convergence of 
the optimization method. Yeah, yeah and - and I would further say like I 
can't see any reason not to use pattern, or there are versions of batch 
norm that, in certain situations, turned out not to work so well. That 
people have figured out ways around that for nearly every one of those 
situations now, so I would always seek to find a way to use batch norm. It 
may be a little harder in our own ends, at least, but even there there are 
ways of doing batch norm in our attenders as well.</p>

<p>So you know, try, try 
and always use batch norm on every layer if you can and the question that 
somebody asked is, does it mean I have to? I can stop normalize in my data 
yeah yeah. It does, although do it anyway, because it's not at all hard to 
do it, and at least that way the people using your data - I don't know they 
kind of know how you've normalized it, and particularly with these issues 
around a lot of libraries. In my opinion, at least warm not my opinion, my 
experiments don't deal with batch norm correctly for pre-trained models. 
Just remember that when somebody starts retraining, those averages and 
stuff are going to change for your data set, and so if your new data set 
has very different input averages, it could really cause a lot of problems 
so so yeah. I went through a period where I actually stopped normalizing my 
data and you know things kind of worked, but it's probably not worth it. 
Okay, so so the rest of this is identical right. All I've done is I've 
changed, conf layer to BN layer, but I've done one more thing, which is I'm 
kind of trying to get closer and closer to modern approaches which I've 
added a single convolutional layer at the start, with a bigger kernel, size 
and a stride Of one, why have I done that? So the basic idea is that I want 
my first layer to kind of have a richer input right so before my first 
layer had an input of just three, because there's just three channels 
right. But if I start with my image right and and I kind of take a bigger 
area - few different color - I kind of take a bigger area right and I do a 
convolution using that bigger area.</p>

<p>In this case, I'm doing five by five 
right. Then that kind of allows me to try and find more interesting, richer 
features in that 5x5 area, and so then I spin out a bigger output. This 
case, I spit out a filter size good about ten five by five filters, and so 
the idea is like pretty much. Every state of the art convolutional 
architecture now starts out with a single con flare, with like a five by 
five or seven by seven or sometimes even like 11, by 11 convolution, with 
like quite a few filters, you know something like you know. Thirty two 
filters coming out and it's just just a way of kind of trying to and like 
because I use the straight of one and the padding of kernel size minus one 
over two. It means that my outputs going to be exactly the same size as my 
input, but just got more filters now. This is just a good way of trying to 
create a richer starting point for my sequence of convolutional layers. 
Okay, so that's the basic theory of why I've added this single convolution, 
which I just do once at the start and then I just go through all my layers 
and then I do my adaptive max pooling and my final classifier okay. So it's 
a minor tweak, but it helps right and so you'll see. Now I kind of can go 
for a Moodle. I have 60 % and after a couple is 45 %. Now, after a couple, 
that's 57 % and after a few more I'm out for 68 % okay, so you can see 
it's. You know the the batch norm and you know tiny bit the conveyor at the 
start, it's helping now, what's more, you can see.</p>

<p>This is still increasing 
right, so that's looking pretty encouraging okay! So, given that 
</p>

<h3>16. <a href="https://youtu.be/H3g26EVADgY?t=1h50m51s">01:50:51</a></h3>

<ul style="list-style-type: square;">

<li><b> ‘Deep BatchNorm’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>This is looking pretty good. An obvious thing to try might be to see is to 
try increasing the depth of the model, and now I can't just add more of 
most dried to layers because remember how it half the size of the image 
each time, I'm basically down to two by two At the end right, so I can't 
add much more so what I did instead was. I said: okay, here's my original 
layers, so you must trade. Two layers for everyone also create a straight 
one layer. So astrayed one layer doesn't change the size, and so now I'm 
saying zip my stride, two layers and my stride, one layers together and so 
first of all do the straight too and then do the straight one. So this is 
now actually twice as deep okay. So this is so. This is now twice as deep, 
but I end up with the exact same. You know two by two that I had before, 
and so, if I try this, you know here after one two three four epochs is at 
sixty-five percent. After one two three epochs, I'm still at 65 %, it 
hasn't helped right, and so the reason it hasn't helped is I'm now too 
deep, even for batch norm, two handlers. Now my depth is now one two three 
four five times two is ten: eleven kampf 112. Okay, so 12 layers deep, it's 
possible to train a standard, confident 12 layers deep, but it starts to 
get difficult to do it properly right and it certainly doesn't seem to be 
really helping much if at all, so that's where I'm instead going to 
</p>

<h3>17. <a href="https://youtu.be/H3g26EVADgY?t=1h52m43s">01:52:43</a></h3>

<ul style="list-style-type: square;">

<li><b> Replace the model with ResNet, class ‘ResnetLayer()’, using ‘boosting’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Replace this with a ResNet all right, so a rest net is our final stage. 
What a resin it does is, I'm going to replace our BN layer right, I'm going 
to inherit from BN layer and replace our forward with that and that's it. 
Everything else is going to be identical, except now I'm going to do like 
way. Lots of layers I'm going to make it four times deeper right and it's 
going to Train beautifully just because of that. So why does that help so 
much so this is called a ResNet block and, as you can see, I'm saying 
that's not what I meant to do. I'm saying my predictions equals my input 
plus some function. You know, in this case a convolution of my input. 
Alright, that's that's! That's what I've written here and so I'm now going 
to shuffle that around a little bit and I'm going to say I'm going to say f 
of X, equals y minus X. Ok, so there's the same thing shuffled around 
right! That's my prediction! Within the previous layer right, and so what 
this is then doing, is it's trying to fit a function to the difference 
between these two right, and so the difference is actually the residual. So 
if this is what I'm trying to calculate my actual Y value - and this is the 
thing that I've most recently calculated, then the difference between the 
two is basically the error in terms of what I've calculated so far, and so 
this is therefore saying that okay, Try to find a set of convolutional 
weights that attempts to fill in the the amount I was off by so.</p>

<p>In other 
words, if we let's clear this out, if we have some inputs coming in right 
and then we have this function, which is basically trying to predict the 
error, it's like how much are we off by right, and then we add that on so 
we basically Add on this additional, like prediction of how much will be 
wrong by and then we add on another prediction of how much were we wrong by 
that time and add on another prediction of how much we wrong by that time, 
then that each time we're kind of Zooming in getting closer and closer to 
our correct answer and each time we're saying like okay, we've got to a 
certain point, but we still got an error. You've still got a residual, so 
let's try and create a model that just predicts that residual and add that 
on to our previous model and then let's build another model that predicts 
the residual and add that on to our previous model. And if we keep doing 
that again and again, we should get closer and closer to our answer, and 
this is based on a theory called boosting which people that have done some 
machine learning will certainly come across right. And so basically, the 
trick here is that by specifying that, as being the thing that we're trying 
to calculate, then we kind of get boosting for free right. It's like 
because we couldn't just juggle that around to show that actually it's just 
calculating a model on the wrist Jill. So that's kind of amazing and you 
know it totally works.</p>

<p>As you can see here, I've now got my standard batch 
norm. Layer, okay, which is something which is going to reduce my size by 
two, because it's got the stride too, and then I've got a resident layers, 
dried one and another resident layer, astride one right and sorry. I think 
I said that was four of these is actually three of these, so this is now 
three times deeper. I zipped through all of those and so I've now got a 
function of a function of a function, so three layers per group and then my 
con at the start and my linear at the end. So this is now three times 
bigger than my original and if I fit it you can see, it's just keeps going 
up and up and up and up, I keep fitting it more he's going up and up and 
it's still going up when I kind of got Bored, okay, so the rest net has 
been a really important development and it's allowed us to create these 
really deep networks right now. The full risk net does not quite look the 
way. I've described it here. The full res net doesn't just have one 
convolution right, but it actually has two convolutions right. So the way 
people normally draw resident blocks is they normally say. You've got some 
input coming in to the layer. It goes through one convolution to 
convolutions and then gets added back to the original input right. That's 
the full version of a ResNet block.</p>

<p>In my case, I've just done 
</p>

<h3>18. <a href="https://youtu.be/H3g26EVADgY?t=1h58m38s">01:58:38</a></h3>

<ul style="list-style-type: square;">

<li><b> ‘Bottleneck’ layer with ‘BnLayer()’, ‘ResNet 2’ with ‘Resnet2()’, Skipping Connections.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>One convolution, okay and then you'll see also in every block right one of 
them. It's actually the first one. Does he it's actually the first one here 
is not a resident block, but a standard convolution with a stride of two 
right. This is called a bottleneck layer right and the idea is. This is not 
a ResNet block. So from time to time, we actually change the geometry 
right, we're doing this trade to in resident. We don't actually use just a 
standard, convolutional layer, there's actually a different form of 
bottleneck, block that I'm not going to picture in this course. I'm going 
to teach you in part, two okay, but, as you can see, even this somewhat 
simplified version of a resin. It still works pretty well, and so we can 
make it a little bit bigger all right, and so here I've just increased all 
of my sizes. I have still got my three and also I've had it drop out right. 
So at this point I'm gon na say this is other than the minor simplification 
of ResNet. You know a reasonable approximation of a good starting point for 
a modern architecture - okay and so now, I've added in my point to drop 
out. I've increased the size here and if I train this, you know I can treat 
it for a while. It's going pretty well, I can then add in gta. At the end, 
eventually, I get 85 %, and you know this is at a point now, where, like 
literally, I wrote this whole notebook in like three hours right.</p>

<p>We can 
like create this thing in three hours, and this is like an accuracy that in 
kind of 2012-2013, was considered pretty much data the art for say, pocket 
right, so this is actually no. This is actually pretty damn good to get. 
You know nowadays, the most recent results are like 97 %. You know there 
are there's plenty of room. We can still improve, but they're all based on 
these techniques like there isn't really anything. You know when we start 
looking in in part to it like how to get this right up to state of the art. 
You'll see it's basically better approaches to data augmentation, better 
approaches to regularization some tweaks on ResNet, but it's all basically 
the circuit. Okay, so so is the residual training on the residual method. 
Is that only looks like it's a generic thing that can be applied non image 
problems? Oh great question, yeah? Yes, it is, but it's like being ignored 
everywhere else in NLP, something called the transformer architecture 
recently appeared and you know was shown to be the state of the art for 
translation and it's got like a simple resonance structure. You know first 
time, I've ever seen it in NLP. I haven't really seen anybody else take 
advantage of it yeah this general approach. We call these skip connections, 
this idea of like skipping over a layer and kind of doing an identity. It's 
yeah: it's been appearing a lot in computer vision and nobody else much 
seems to be using it.</p>

<p>Even though there's nothing computer vision specific 
about it. So I think it's a big opportunity. Okay, so final stage, I 
</p>

<h3>19. <a href="https://youtu.be/H3g26EVADgY?t=2h2m1s">02:02:01</a></h3>

<ul style="list-style-type: square;">

<li><b> ‘lesson7-CAM.ipynb’ notebook, an intro to Part #2 using ‘Dogs v Cats’.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Want to show you is how to use an extra feature of pytorch, to do something 
cool and it's going to be a kind of a segue into part. Two. It's going to 
be our first little hint as to what else we can build on these neural nets, 
and so - and it's also going to take us all the way back to lesson 1, which 
is we're going to do. Dogs and cats. Ok, so going all the way back to dogs 
and cats we're going to create a resin at 34; ok, so these different 
ResNet, 3450 101 they're they're, basically just different numbers of 
different sized blocks. It's like how many of these kind of pieces do you 
have before it bottleneck block and then how many of these sets of super 
blocks? Do you have right? That's all these different numbers mean. So if 
you look at the torch vision source code, you can actually see the 
definition of these different resonates. You'll see they're all just 
different parameters: right, ok, so we're going to use rest at 34, and so 
we're going to do this a little bit more by hand okay. So if this is my 
architecture, this is just the name of a function then I can call it to get 
that model right and then true, look at the definition is, do I want the 
pre-trained? So, in other words, is it going to load in the pre-trained 
imagenet weights? Okay, so M now contains a model, and so I can take a look 
at it like so.</p>

<p>Okay - and so you can see here what's going on right - is 
that inside here I've got my initial two deconvolution, and here is that 
kernel size of seven by seven, okay and interestingly, in this case, it 
actually starts out with a seven by seven strobe. Okay, there's the padding 
that we talked about to make sure that we don't lose the edges all right. 
There's our batch naught. Okay, there's our Lu! You get the idea right, 
Kong and then so here you can now see there's a layer that contains a bunch 
of blocks all right. So here's a block which contains a cons fetch norm 
rally you con Bethnal. You can't see it printed, but after this is where it 
does the addition all right so there's like a whole ResNet block and then 
another resident block and then another ResNet block okay and then you can 
see. Also, sometimes you see one where there's a stripe right. So here's 
actually one of these bottleneck layers. Okay, so you can kind of see how 
this is. This is structure so, in our case sorry, I skipped over this a 
little bit, but the approach that we ended up using for real you was to put 
it before our before our Bashan on which see what they do here. We've got 
fetch norm railing. You cons that no, I'm really okay, okay, so you can see 
the order that they're using it here. Okay and you'll find like there's two 
different versions of ResNet. In fact, there's three different versions of 
ResNet floating around the one which actually turns out to be the best. 
It's called the pre-act ResNet, which has a different ordering again, but 
you can look it up.</p>

<p>It's basically a different order of where the value and 
where the batch norm yeah okay, so we're going to start with a standard 
resident 34 and normally what we do is we need to now turn this into 
something that can predict dogs versus class right. So currently, the final 
layer has a thousand features, because imagenet has a thousand features 
right, so we need to get rid of this. So when you use confer owner from 
pre-trained in fastai, it actually deletes this layer for you, and it also 
deletes this layer and something that, as far as I know, is unique to 
fastai. Is we replace this see? This is an average pooling layer of size, 
seven by seven right. So this is the basically the adaptive pooling layer, 
but whoever wrote this didn't know about adaptive pooling, so they manually 
said. Oh, I know it's meant to be seven by seven, so in fastai we replaced 
this with adaptive pooling, but we actually do both adaptive average 
pooling and adaptive max pooling and we then concatenate them two together 
which it's it is something we invented, but at the Same time we invented 
it. Somebody wrote a paper about it. So it's you know, we don't get any 
credit, but I think we're the only library that provides it, and certainly 
only one that does it by default. We're going to for the purpose of this 
exercise, though, we're going to do a simple version where we delete the 
last two layers so we'll grab.</p>

<p>All the children of the model will delete 
the last two layers and then instead we're going to add a convolution which 
just has two outputs right. I'll show you why, in a moment right then we're 
going to do our average pooling and then we're going to do soft mess? Okay, 
so that's a model which is going to have you'll see that there is no. This 
one has a fully connected layer at the end, this one does not have a fully 
connected layer yet, but if you think about it, this convolutional layer is 
going to be two filters only right and it's going to be two by seven by 
seven, and so Once we then do the average pooling it's going to end up 
being just two numbers that it produces. So this is a different way of 
producing just two numbers. I'm not going to say it's better, it's going to 
say it's different, okay, but there's a reason. We do it I'll show you the 
reason. We can now train this model in the usual way right, so we can say, 
transform stock model, image, classifier data from paths, and then we can 
use that Kampf learner from model data we just learnt about. I'm now going 
to freeze every single layer, except for that one - and this is the fourth 
last layer, so we'll say, freeze to minus four right, and so this is just 
training the last layer. Okay, so we get 99.1 percent accuracy so that you 
know this approaches.</p>

<p>Working fine and here's what we can do, though we can 
now do something. </p>

<h3>20. <a href="https://youtu.be/H3g26EVADgY?t=2h8m55s">02:08:55</a></h3>

<ul style="list-style-type: square;">

<li><b> Class Activation Maps (CAM) of ‘Dogs v Cats’.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Called fast comm last activated Maps fast activation is what we're going to 
do. Is we're going to try to look at this particular cat and we're going to 
use a technique called class activation Maps where we take our model and we 
ask you which parts of this image turned out to be important and when we do 
this, it's going to Feed out this is got the picture, it's going to create 
alright, and so, as you can see here, it's found the cat. So how did it do 
that? Well, the way it did that will kind of work backwards is to produce 
this matrix now you'll see in this matrix, there's some pretty big numbers 
around about here which correspond to our cat. So what is this matrix? This 
matrix is simply or to the value of this feature. Matrix times this py 
vector the py vector, is simply equal to the predictions which in this case 
said I am 100 %, confident it's a cat right, so this is just equal to the 
value of. If I just call the model passing in our cat, this is our cat. 
That's an X, then we got our predictions right. So it's just the value of 
our predictions, so py is just the value of our predictions. What about 
feet? What's that equal to feet is equal to the values in this layer right, 
in other words, the value that comes out of the final in facts come out of 
this ladder coming out of the final convolutional layer right. So it's 
actually the seven by seven by two, and so you can see here, let's see 
feet, the shape of features is two filters by seven by seven right.</p>

<p>So the 
idea is, if we multiply that vector by that tensor right, then it's going 
to end up grabbing all of the first channel, because that's a 1 and none of 
the second channel, because that's a 0 and so therefore it's going to 
return the value of The last convolutional layer for the for the section 
which lines up with being a cat, but if you think about it, this the first 
section lines up with being a cat. The second section lines up with being a 
dog. So if we multiply that tensor by that tensor, we end up with this 
matrix and this matrix is which parts most like a cat or to put it another 
way in our model. The only thing that happened after the convolutional 
layer was an average pooling layer. So the average pooling layer talked 
that 7x7 grid and said average. How much each part is cat Lake that 
answered my final value. My final prediction was the average cattiness 
there's the whole thing right, and so because it had to be able to average 
out these things. To get the average cattiness, that means I could then 
just take this matrix and resize it to be the same size as my original cat 
and just overlay it on top. You get this heat map right, so the way you can 
use this technique at home is to basically calculate this matrix right on 
some. Like really you've got some really big picture.</p>

<p>You can calculate 
this matrix on a quick, small, little cognate and then zoom into the bit 
that has the highest value and then rerun it just on that part that so it's 
like. Oh, this is the area that seems to be the most like a hat or most 
like a dog that zoom in to that bit right. So I skipped over that pretty 
quickly because we ran out of time and so we'll be learning more about 
these kind of approaches in part two, and we can talk about it more on the 
forum and hopefully you get the idea. The one thing that totally skipped 
over was: how do we actually ask for that particular layer, okay and I'll? 
Let you read about this during the week, but basically there's a thing 
called a hook, so we said we called save features, which is this little 
class that we wrote that goes register forward hook and basically a forward 
hook is a special pytorch thing that every Time it calculates a layer, it 
runs this function, it's like a callback. Basically, it's like a callback 
that happens every time. It calculates a layer, and so in this case it just 
saved the value of the particular layer that I was interested in okay, and 
so that way I was able to go inside here and grab those features out. Look 
after I was done okay, so I call save features that gives me my pork and 
then later on. I can just grab the value that I saved okay, so I skipped 
over that pretty quickly. But if you look in the pipe or docks they have 
some more information and help about that.</p>

<p>Yes, Jeremy, can you 
</p>

<h3>21. <a href="https://youtu.be/H3g26EVADgY?t=2h14m27s">02:14:27</a></h3>

<ul style="list-style-type: square;">

<li><b> Questions to Jeremy: “Your journey into Deep Learning” and “How to keep up with important research for practioners”,</b></li>

<li><b>“If you intend to come to Part 2, you are expected to master all the techniques in Part 1”, Jeremy’s advice to master Part 1 and help new students in the incoming MOOC version to be released in January 2018.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Spend five minutes talking about your journey into deep learning yeah and 
finally, how can we keep up with important research that is important to 
practice, sure yeah, so it's gon na that's good! I think I'll close more on 
the latter bit, which is like what now okay. So for those of you are 
interested, you should aim to come back for part two. If you're aiming to 
come back for part two, how many people would like to come back to part 
two? Okay, that's not bad! I think almost everybody. So if you want to come 
back to part two be aware of this by that time, you're expected to have 
mastered all of the techniques we've learnt in part one and there's plenty 
of time between now and then okay, even if you haven't done much or any Ml 
before, but it does assume that you're going to be working, you know at the 
same level of intensity for now until then that you have been with 
practicing right so practicing. So, generally speaking, the people who did 
well in part two last year had watched each of the videos about three times 
right and some of the people. Actually, I knew had actually discovered. 
They learnt some of them off by heart by mistake. So they're like watching 
the video, is again is helpful and make sure you get to the point that you 
can recreate the notebooks without watching the videos, all right, and so 
other men make more interesting, obviously try and recreate them notebooks 
using different data sets.</p>

<p>You know and definitely then just keep up with 
the forum and you'll see people keep on posting more stuff about recent 
papers and recent advances and over the next couple of months, you'll find 
increasingly less and less of it seems weird mysterious and more and more 
of It makes perfect sense, and so it's a bit of a case with just thing. 
Staying tenacious, you know, there's always going to be stuff that you 
don't understand yet and but you'll be surprised. If you go back to lesson 
one and two now you'll be like. Oh, that's all trivial right, so you know 
that's kind of hopefully a bit of your learning journey and yeah. I think 
the main thing I've noticed is that people who succeed are the ones who 
just keep keep working at it. You know so not coming back here. Every 
Monday you're not going to have that forcing function. I've noticed the 
forum suddenly gets busy at 5:00 p.m. on a Monday. You know it's like, Oh 
course, is about to start, and suddenly these questions start coming in. So 
now that you don't have that forcing function, you know try and use some 
other technique to. You know give yourself that little kick. Maybe you can 
tell your partner at home. You know I'm going to try and produce something 
every Saturday for the next four weeks or I'm going to try and finish 
reading this paper or something you know anyway.</p>

<p>So I hope to see you all 
back in March and even I, regardless whether I do or don't it's been a 
really great pleasure to get to know you all and I hope to keep seeing on 
the forum thanks very much. [ Applause, ] </p>




  </body>
</html>
